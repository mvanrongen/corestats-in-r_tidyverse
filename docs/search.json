[{},{"path":"index.html","id":"overview","chapter":"1 Overview","heading":"1 Overview","text":"sessions intended enable perform core data analysis techniques appropriately confidently using R.6 lecture-practicals6 lecture-practicalsOngoing formative assessment exercisesOngoing formative assessment exercisesNo formal assessmentNo formal assessmentNo mathematical derivationsNo mathematical derivationsNo pen paper calculationsNo pen paper calculationsThey ‚Äúmindlessly use stats program‚Äù course!","code":""},{"path":"index.html","id":"core-aims","chapter":"1 Overview","heading":"1.1 Core aims","text":"know presented arbitrary dataset e.g.Know data analysis techniques availableKnow ones allowableBe able carry understand results","code":""},{"path":"index.html","id":"core-topics","chapter":"1 Overview","heading":"1.2 Core topics","text":"Simple hypothesis testingCategorical predictor variablesContinuous predictorsTwo predictor variablesMultiple predictor variablesPower analysis","code":""},{"path":"index.html","id":"practicals","chapter":"1 Overview","heading":"1.3 Practicals","text":"practical document divided various sections. section explanatory text help understand going ‚Äôre trying achieve.\nmay list commands relevant section displayed boxes like :Conditional operatorsTo set filtering conditions, use following relational operators:> greater >= greater equal < less <= less equal == equal != different %% contained combine conditions, use following logical operators:& | ","code":""},{"path":"index.html","id":"index-datasets","chapter":"1 Overview","heading":"1.4 Datasets","text":"course uses various data sets. easiest way accessing creating R-project RStudio. download data folder right-clicking link Save ‚Ä¶. Next unzip file copy working directory. data accessible via <working-directory-name>/data/tidy.","code":""},{},{"path":"cs1-intro.html","id":"cs1-intro","chapter":"2 Introduction","heading":"2 Introduction","text":"","code":""},{"path":"cs1-intro.html","id":"objectives","chapter":"2 Introduction","heading":"2.1 Objectives","text":"Aim: carry basic one two sample statistical tests.end section practical participants able achieve following listed tests:Understand purpose test isPerform test RInterpret test outputUnderstand assumptions/conditions test appropriateCheck assumptionsThe tests covered practical :One-sample tests\nOne sample t-test\nOne-sample Wilcoxon signed-rank test\nOne sample t-testOne-sample Wilcoxon signed-rank testTwo-sample tests\nStudent‚Äôs t-test\nMann-Whitney U test\nPaired two-sample t-test\nWilcoxon signed-rank test\nStudent‚Äôs t-testMann-Whitney U testPaired two-sample t-testWilcoxon signed-rank test","code":""},{"path":"cs1-intro.html","id":"background","chapter":"2 Introduction","heading":"2.2 Background","text":"practical focus underlying mathematical theory tests although demonstrators happy answer questions.\ntest section explaining purpose, section explaining perform test R, section explaining results output screen, section covering assumptions required perform test.","code":""},{},{"path":"introduction.html","id":"introduction","chapter":"3 Introduction","heading":"3 Introduction","text":"","code":""},{"path":"introduction.html","id":"cs1-one-sample-tests","chapter":"3 Introduction","heading":"3.1 One-sample tests","text":"","code":""},{"path":"introduction.html","id":"objectives-one-sample","chapter":"3 Introduction","heading":"3.1.1 Objectives (one-sample)","text":"QuestionsWhen perform one-sample test?one-sample tests assumptions?interpret present results tests?ObjectivesSet hypothesis single sample continuous dataBe able summarise visualise data RUnderstand assess underlying assumptions testsPerform one-sample t-test Wilcoxon signed-rank test RKnow test appropriate whenBe able interpret report results","code":""},{"path":"introduction.html","id":"purpose-and-aim","chapter":"3 Introduction","heading":"3.1.2 Purpose and aim","text":"tests used single sample continuous data. used find sample came parent distribution given mean (median). essentially boils finding sample mean (median) ‚Äúclose enough‚Äù hypothesised parent population mean (median).\n, figure , use tests see probability sample ten points comes distribution plotted .e.¬†population mean 20 mm.","code":""},{"path":"introduction.html","id":"choosing-a-test","chapter":"3 Introduction","heading":"3.1.3 Choosing a test","text":"two tests going look situation; one-sample t-test, one-sample Wilcoxon signed rank test. tests work sort data ‚Äôre considering , different assumptions.data normally distributed, one-sample t-test appropriate. data aren‚Äôt normally distributed, distribution symmetric, sample size small one-sample Wilcoxon signed rank test appropriate.statistical test consider five tasks. come back , pay extra close attention.Setting hypothesisSummarise visualisation dataAssessment assumptionsImplementation statistical testInterpreting output presentation resultsWe won‚Äôt always carry exactly order, always consider five tasks every test.","code":""},{"path":"introduction.html","id":"cs1-two-sample","chapter":"3 Introduction","heading":"3.2 Two-sample tests","text":"","code":""},{"path":"introduction.html","id":"objectives-two-sample","chapter":"3 Introduction","heading":"3.2.1 Objectives (two-sample)","text":"QuestionsWhen perform two-sample test?two-sample tests assumptions?interpret present results tests?ObjectivesSet hypothesis two-sample continuous dataDetermine correct data format perform two-sample test RSummarise visualise dataCheck underlying assumptions (normality, homogeneity variance)able choose appropriate two-sample test run RBe able interpret report results","code":""},{"path":"introduction.html","id":"purpose-and-aim-1","chapter":"3 Introduction","heading":"3.2.2 Purpose and aim","text":"tests used two samples continuous data trying find samples came parent distribution . essentially boils finding difference means two samples.","code":""},{"path":"introduction.html","id":"two-sample-choosing-a-test","chapter":"3 Introduction","heading":"3.2.3 Choosing a test","text":"five key tests can used deal two samples. Choosing test use depends upon key assumptions satisfied sample data effectively boils answering four questions samples:samples normally distributed? (Yes/)big samples? (<30 data points >30 data points)samples paired? (Yes/)samples variance? (Yes/)two sets tests consider depending answers questions 1 2. data normally distributed big samples need look parametric tests. data normally distributed sample size small, need look non-parametric tests (see Figure 3.1. Questions 3 4 help pick specific test use, summarised Figure 3.2.\nFigure 3.1: Category test\n\nFigure 3.2: test use\nTesting whether sample comes normal distribution covered One-sample tests. need visualise data /use Shapiro-Wilk test.size sample makes things easier. maths (specifically due something called central limit theorem even going attempt touch upon ) large samples can use tests assume normality parent population (Student‚Äôs t-test, Welch‚Äôs t-test paired t-test) even parent populations certainly normal. really want understand exactly works, rigorous mathematics. , moment ‚Äôm going say ‚Äôs OK take facts faith just trust .Paired samples mean every data point one sample matching data point sample linked inextricable way. typical example involve group 20 test subjects measured experiment. Providing experiment didn‚Äôt anything fatal test subjects data consist two samples; 20 pre-experiment measurements 20 post-experiment measurements. However, test subjects used pre-experiment data point can matched exactly one post-experiment data points. sense two samples said ‚Äúpaired.‚Äùcouple tests (Bartlett‚Äôs test Levene‚Äôs test) can used see two samples come distributions variance. covered later section.Resampling techniques aren‚Äôt covered course require mixture statistical understanding programming skill. Ask demonstrator (Google üòâ) want know .","code":""},{"path":"introduction.html","id":"cs1-tidy-data","chapter":"3 Introduction","heading":"3.3 Tidy data","text":"two samples data can stored one three formats R:two separate vectors,stacked data frame,unstacked data frame/list.Two separate vectors case (hopefully) obvious.using data frame different options organise data. best way formatting data R using tidy data format.Tidy data following properties:variable columnEach observation rowEach value cellStacked form (long format data) data arranged way variable (thing measured) column. consider dataset containing meerkat weights (g) two different countries stacked format data look like:unstacked (wide format) form variable (measured thing) present one column. example, let‚Äôs say measured meerkat weight two countries period years. organise data way year measured values split country:tidy data easiest way analyses R strongly encourage start adopting format standard data collection processing.","code":"## # A tibble: 6 √ó 2\n##   country  weight\n##   <chr>     <dbl>\n## 1 Botswana    514\n## 2 Botswana    568\n## 3 Botswana    519\n## 4 Uganda      624\n## 5 Uganda      662\n## 6 Uganda      633## # A tibble: 3 √ó 3\n##    year Botswana Uganda\n##   <dbl>    <dbl>  <dbl>\n## 1  1990      514    624\n## 2  1992      568    662\n## 3  1995      519    633"},{},{"path":"cs1-one-sample-t-test.html","id":"cs1-one-sample-t-test","chapter":"4 One-sample t-test","heading":"4 One-sample t-test","text":"","code":""},{"path":"cs1-one-sample-t-test.html","id":"section-commands","chapter":"4 One-sample t-test","heading":"4.1 Section commands","text":"New commands used section:","code":""},{"path":"cs1-one-sample-t-test.html","id":"data-and-hypotheses","chapter":"4 One-sample t-test","heading":"4.2 Data and hypotheses","text":"example, suppose measure body lengths male guppies (mm) collected Guanapo River Trinidad. want test whether data support hypothesis mean body actually 20 mm. form following null alternative hypotheses:\\(H_0\\): mean body length equal 20mm (\\(\\mu =\\) 20).\\(H_1\\): mean body length equal 20mm (\\(\\mu \\neq\\) 20).use one-sample, two-tailed t-test see reject null hypothesis .use one-sample test one sample.use two-tailed t-test want know data suggest true (population) mean different 20 mm either direction rather just see greater less 20 mm (case use one-tailed test).‚Äôre using t-test don‚Äôt know better yet ‚Äôm telling . ‚Äôll look precise assumptions/requirements need moment.Make sure downloaded data (see: Datasets) placed data/raw folder within working directory.First load relevant libraries:read data create vector containing data.first line reads data R creates object called tibble, type data frame. data frame contains 3 columns: unique id, river encoding river length measured guppy length.","code":"\n# load tidyverse\nlibrary(tidyverse)\n\n# load rstatix, a tidyverse-friendly stats package\nlibrary(rstatix)\n# import the data\nfishlengthDF <- read_csv(\"data/tidy/CS1-onesample.csv\")\n\nfishlengthDF## # A tibble: 29 √ó 3\n##       id river   length\n##    <dbl> <chr>    <dbl>\n##  1     1 Guanapo   19.1\n##  2     2 Guanapo   23.3\n##  3     3 Guanapo   18.2\n##  4     4 Guanapo   16.4\n##  5     5 Guanapo   19.7\n##  6     6 Guanapo   16.6\n##  7     7 Guanapo   17.5\n##  8     8 Guanapo   19.9\n##  9     9 Guanapo   19.1\n## 10    10 Guanapo   18.8\n## # ‚Ä¶ with 19 more rows"},{"path":"cs1-one-sample-t-test.html","id":"summarise-and-visualise","chapter":"4 One-sample t-test","heading":"4.3 Summarise and visualise","text":"Summarise data visualise :data appear contain obvious errors, whilst mean median less 20 (18.3 18.8 respectively) absolutely certain sample mean sufficiently different value ‚Äústatistically significant,‚Äù although may anticipate result.","code":"\nsummary(fishlengthDF)##        id        river               length    \n##  Min.   : 1   Length:29          Min.   :11.2  \n##  1st Qu.: 8   Class :character   1st Qu.:17.5  \n##  Median :15   Mode  :character   Median :18.8  \n##  Mean   :15                      Mean   :18.3  \n##  3rd Qu.:22                      3rd Qu.:19.7  \n##  Max.   :29                      Max.   :23.3\nfishlengthDF %>% \n  ggplot(aes(x = river, y = length)) +\n  geom_boxplot()"},{"path":"cs1-one-sample-t-test.html","id":"assumptions","chapter":"4 One-sample t-test","heading":"4.4 Assumptions","text":"comes one-sample tests, two options:t-testWilcoxon signed-rank testFor us use t-test analysis (results valid) make two assumptions:parent distribution sample taken normally distributed (sample data normally distributed ).worth noting though t-test actually pretty robust situations sample data normal. sufficiently large sample sizes (guess good mine, conventionally means 30 data points), can use t-test without worrying whether underlying population normally distributed .data point sample independent others. general something can tested instead considered sampling procedure. example, taking repeated measurements individual generate data independent.second point know nothing ignore (issue needs considered experimental design), whereas first assumption can checked.\nthree ways checking normality:increasing order rigour, haveHistogramQuantile-quantile plotShapiro-Wilk test","code":""},{"path":"cs1-one-sample-t-test.html","id":"histogram-of-the-data","chapter":"4 One-sample t-test","heading":"4.4.1 Histogram of the data","text":"Plot histogram data, gives:distribution appears uni-modal symmetric, isn‚Äôt obviously non-normal. However, lot distributions simple properties aren‚Äôt normal, isn‚Äôt exactly rigorous. Thankfully , rigorous tests.NB. even looking distribution assess assumption normality already going far beyond anyone else ever . Nevertheless, continue.","code":"\nfishlengthDF %>% \n  ggplot(aes(x = length)) +\n  geom_histogram(bins = 15)"},{"path":"cs1-one-sample-t-test.html","id":"q-q-plot-of-the-data","chapter":"4 One-sample t-test","heading":"4.4.2 Q-Q plot of the data","text":"Q-Q plot short quantile-quantile plot. diagnostic plot (sometimes called) way comparing two distributions. Q-Q plots work won‚Äôt explained ask demonstrator really want know going .Construct Q-Q Plot quantiles data quantiles normal distribution:important know data normally distributed points lie (close ) diagonal line graph.case, points lie quite close line part sample quantiles (points) either end sample distribution either smaller (line left) larger (line right) expected supposed normally distributed. suggests sample distribution bit spread expected came normal distribution.important recognise isn‚Äôt simple unambiguous answer interpreting types graph, terms whether assumption normality well met instead often boils matter experience.rare situation indeed assumptions necessary test met unequivocally certain degree personal interpretation always needed. ask whether data normal ‚Äúenough‚Äù confident validity test.four examples QQ plots different types distributions:two graphs relate 200 data points drawn normal distribution. Even can see points lie perfectly diagonal line QQ plot, certain amount deviation top bottom graph can happen just chance (draw different set point graph look slightly different).two graphs relate 200 data points drawn uniform distribution. Uniform distributions condensed normal distributions, reflected QQ plot pronounced S-shaped pattern (colloquially known snaking).two graphs relate 200 data points drawn t distribution. t distributions spread normal distributions, reflected QQ plot pronounced S-shaped pattern , time snaking reflection observed uniform distribution.two graphs relate 200 data points drawn exponential distribution. Exponential distributions symmetric skewed compared normal distributions. significant right-skew distribution reflected QQ plot points curve away diagonal line ends (left-skew points line ends).four cases worth noting deviations ends plot.","code":"\n# pipe the data to ggplot()\n# then construct a Q-Q plot\nfishlengthDF %>%\n  ggplot(aes(sample = length)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\")"},{"path":"cs1-one-sample-t-test.html","id":"shapiro-wilk-test","chapter":"4 One-sample t-test","heading":"4.4.3 Shapiro-Wilk test","text":"one number formal statistical test assess whether given sample numbers come normal distribution. calculates probability getting sample data underlying distribution fact normal. easy carry R.Perform Shapiro-Wilk test data:variable indicated variable used perform test onstatistic gives calculated W-value (0.9493842)p gives calculated p-value (0.1764229)p-value bigger 0.05 (say) can say insufficient evidence reject null hypothesis sample came normal distribution.important recognise Shapiro-Wilk test without limitations. rather sensitive sample size considered. general, small sample sizes, test relaxed normality (nearly datasets considered normal), whereas large sample sizes test can overly strict, can fail recognise datasets nearly normal indeed.","code":"\nfishlengthDF %>% \n  shapiro_test(length)## # A tibble: 1 √ó 3\n##   variable statistic     p\n##   <chr>        <dbl> <dbl>\n## 1 length       0.949 0.176"},{"path":"cs1-one-sample-t-test.html","id":"assumptions-overview","chapter":"4 One-sample t-test","heading":"4.4.4 Assumptions overview","text":"terms assessing assumptions test always worth considering several methods, graphical analytic, just relying single method.fishlengthDF example, graphical Q-Q plot analysis especially conclusive suggestion snaking plots, Shapiro-Wilk test gave non-significant p-value (0.1764). Putting two together, along original histogram recognition 30 data points dataset personally happy assumptions t-test met well enough trust result t-test, may ‚Ä¶case consider alternative test less stringent assumptions (less powerful): one-sample Wilcoxon signed-rank test.","code":""},{"path":"cs1-one-sample-t-test.html","id":"implement-the-test","chapter":"4 One-sample t-test","heading":"4.5 Implement the test","text":"Perform one-sample, two-tailed t-test:t_test() function requires three arguments:formula, give length ~ 1 indicate one-sample test lengththe mu mean tested null hypothesis, 20the alternative argument gives type alternative hypothesis must one two.sided, greater less. prior assumptions whether alternative fish length greater less 20, choose two.sided.","code":"\nfishlengthDF %>% \n  t_test(length ~ 1,\n         mu = 20,\n         alternative = \"two.sided\")"},{"path":"cs1-one-sample-t-test.html","id":"interpreting-the-output-and-report-results","chapter":"4 One-sample t-test","heading":"4.6 Interpreting the output and report results","text":"output now see console window:statistic column gives us t-statistic -3.5492 (‚Äôll need reporting)df column tells us 28 degrees freedom (‚Äôll need reporting)p column gives us p-value 0.00139The p-value ‚Äôre mostly interested . gives probability us getting sample null hypothesis actually true.:high p-value means high probability observing sample null hypothesis probably true whereasa low p-value means low probability observing sample null hypothesis probably true.important realise p-value just indication absolute certainty interpretation.People, however like definite answers pick artificial probability threshold (called significance level) order able say something decisive. standard significance level 0.05 since p-value smaller choose say ‚Äúunlikely particular sample null hypothesis true.‚Äù Consequently, can reject null hypothesis state :one-sample t-test indicated mean body length male guppies (\\(\\mu\\) = 18.29mm) differs significantly 20 mm (t = -3.55, df = 28, p = 0.0014).sentence adequate concluding statement test write paper report. Note included (brackets) information actual mean value group(\\(\\mu\\) = 18.29mm), test statistic (t = -3.55), degrees freedom (df = 28), p-value (p = 0.0014). journals required report whether p-value less critical value (e.g.¬†p < 0.05) always recommend reporting actual p-value obtained.Please feel free ask demonstrator aspect section unclear form core classical hypothesis testing logic applies rest tests.","code":"## # A tibble: 1 √ó 7\n##   .y.    group1 group2         n statistic    df       p\n## * <chr>  <chr>  <chr>      <int>     <dbl> <dbl>   <dbl>\n## 1 length 1      null model    29     -3.55    28 0.00139"},{"path":"cs1-one-sample-t-test.html","id":"exercise","chapter":"4 One-sample t-test","heading":"4.7 Exercise","text":"Exercise 4.1  following data dissolving times (seconds) drug agitated gastric juice:42.7, 43.4, 44.6, 45.1, 45.6, 45.9, 46.8, 47.6Do results provide evidence suggest dissolving time drug different 45 seconds?Create tidy data frame save .csv formatWrite null alternative hypotheses.Summarise visualise data perform appropriate one-sample t-test.\ncan say dissolving time? (sentence use report )\ncan say dissolving time? (sentence use report )Check assumptions test.\ntest valid?\ntest valid?\\(H_0\\) : mean \\(=\\) 45s\\(H_1\\) : mean \\(\\neq\\) 45sWe can create data frame Excel save .csv file, example CS1-gastric_juices.csv. contains two columns, id column dissolving_time column measured values.can look histogram boxplot data:8 data points, histogram rather uninformative. even reduce number bins get meaningful visualisation. Thankfully boxplot bit useful . can see:don‚Äôt appear major errors data entry aren‚Äôt huge outliersThe median value box-plot (thick black line) pretty close 45 wouldn‚Äôt surprised mean data isn‚Äôt significantly different 45. can confirm looking mean median values calculated using summary command earlier.data appear symmetric, whilst can‚Äôt tell ‚Äôre normal ‚Äôre least massively skewed.Normality:Shapiro test p-value 0.964 (given bigger 0.05) suggests data normal enough.Q-Q plot isn‚Äôt perfect, deviation points away line since points aren‚Äôt accelerating away line , since 8 points, can claim, slight reservations, assumption normality appears adequately well met.Overall, somewhat confident assumption normality well-enough met t-test appropriate method analysing data. Note ridiculous number caveats slightly political/slippery language ‚Äôm using. intentional reflects ambiguous nature assumption checking. important approach statistics need embrace.reality, found situation also try non-parametric test data (Wilcoxon signed-rank test) see whether get conclusion whether median dissolving time differs 45s. Technically, don‚Äôt know Wilcoxon test yet haven‚Äôt done section materials. Anyway, get conclusion confidence result test goes considerably; doesn‚Äôt matter well assumption met, get result. hand get completely different conclusion carrying non-parametric test bets ; now little confidence test result don‚Äôt know one believe (case assumptions test bit unclear). example Wilcoxon test also gives us non-significant result good.one-sample t-test indicated mean dissolving time drug significantly different 45s (t=0.366 , df=7 , p=0.725)","code":"\n# load the data\ndissolving <- read_csv(\"data/tidy/CS1-gastric_juices.csv\")\n\n# have a look at the data\ndissolving## # A tibble: 8 √ó 2\n##      id dissolving_time\n##   <dbl>           <dbl>\n## 1     1            42.7\n## 2     2            43.4\n## 3     3            44.6\n## 4     4            45.1\n## 5     5            45.6\n## 6     6            45.9\n## 7     7            46.8\n## 8     8            47.6\n# summarise the data\nsummary(dissolving)##        id       dissolving_time\n##  Min.   :1.00   Min.   :42.70  \n##  1st Qu.:2.75   1st Qu.:44.30  \n##  Median :4.50   Median :45.35  \n##  Mean   :4.50   Mean   :45.21  \n##  3rd Qu.:6.25   3rd Qu.:46.12  \n##  Max.   :8.00   Max.   :47.60\n# create a histogram\ndissolving %>% \n  ggplot(aes(x = dissolving_time)) +\n  geom_histogram(bins = 4)\n# create a boxplot\ndissolving %>% \n  ggplot(aes(y = dissolving_time)) +\n  geom_boxplot()\n# perform Shapiro-Wilk test\ndissolving %>% \n  shapiro_test(dissolving_time)## # A tibble: 1 √ó 3\n##   variable        statistic     p\n##   <chr>               <dbl> <dbl>\n## 1 dissolving_time     0.980 0.964\n# create a Q-Q plot\ndissolving %>% \n  ggplot(aes(sample = dissolving_time)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\")\n# perform one-sample t-test\ndissolving %>% \n  t_test(dissolving_time ~ 1,\n         mu = 45,\n         alternative = \"two.sided\")## # A tibble: 1 √ó 7\n##   .y.             group1 group2         n statistic    df     p\n## * <chr>           <chr>  <chr>      <int>     <dbl> <dbl> <dbl>\n## 1 dissolving_time 1      null model     8     0.366     7 0.725"},{"path":"cs1-one-sample-t-test.html","id":"hypotheses","chapter":"4 One-sample t-test","heading":"4.7.1 Hypotheses","text":"\\(H_0\\) : mean \\(=\\) 45s\\(H_1\\) : mean \\(\\neq\\) 45s","code":""},{"path":"cs1-one-sample-t-test.html","id":"data-summarise-visualise","chapter":"4 One-sample t-test","heading":"4.7.2 Data, summarise & visualise","text":"can create data frame Excel save .csv file, example CS1-gastric_juices.csv. contains two columns, id column dissolving_time column measured values.can look histogram boxplot data:8 data points, histogram rather uninformative. even reduce number bins get meaningful visualisation. Thankfully boxplot bit useful . can see:don‚Äôt appear major errors data entry aren‚Äôt huge outliersThe median value box-plot (thick black line) pretty close 45 wouldn‚Äôt surprised mean data isn‚Äôt significantly different 45. can confirm looking mean median values calculated using summary command earlier.data appear symmetric, whilst can‚Äôt tell ‚Äôre normal ‚Äôre least massively skewed.","code":"\n# load the data\ndissolving <- read_csv(\"data/tidy/CS1-gastric_juices.csv\")\n\n# have a look at the data\ndissolving## # A tibble: 8 √ó 2\n##      id dissolving_time\n##   <dbl>           <dbl>\n## 1     1            42.7\n## 2     2            43.4\n## 3     3            44.6\n## 4     4            45.1\n## 5     5            45.6\n## 6     6            45.9\n## 7     7            46.8\n## 8     8            47.6\n# summarise the data\nsummary(dissolving)##        id       dissolving_time\n##  Min.   :1.00   Min.   :42.70  \n##  1st Qu.:2.75   1st Qu.:44.30  \n##  Median :4.50   Median :45.35  \n##  Mean   :4.50   Mean   :45.21  \n##  3rd Qu.:6.25   3rd Qu.:46.12  \n##  Max.   :8.00   Max.   :47.60\n# create a histogram\ndissolving %>% \n  ggplot(aes(x = dissolving_time)) +\n  geom_histogram(bins = 4)\n# create a boxplot\ndissolving %>% \n  ggplot(aes(y = dissolving_time)) +\n  geom_boxplot()"},{"path":"cs1-one-sample-t-test.html","id":"assumptions-1","chapter":"4 One-sample t-test","heading":"4.7.3 Assumptions","text":"Normality:Shapiro test p-value 0.964 (given bigger 0.05) suggests data normal enough.Q-Q plot isn‚Äôt perfect, deviation points away line since points aren‚Äôt accelerating away line , since 8 points, can claim, slight reservations, assumption normality appears adequately well met.Overall, somewhat confident assumption normality well-enough met t-test appropriate method analysing data. Note ridiculous number caveats slightly political/slippery language ‚Äôm using. intentional reflects ambiguous nature assumption checking. important approach statistics need embrace.reality, found situation also try non-parametric test data (Wilcoxon signed-rank test) see whether get conclusion whether median dissolving time differs 45s. Technically, don‚Äôt know Wilcoxon test yet haven‚Äôt done section materials. Anyway, get conclusion confidence result test goes considerably; doesn‚Äôt matter well assumption met, get result. hand get completely different conclusion carrying non-parametric test bets ; now little confidence test result don‚Äôt know one believe (case assumptions test bit unclear). example Wilcoxon test also gives us non-significant result good.","code":"\n# perform Shapiro-Wilk test\ndissolving %>% \n  shapiro_test(dissolving_time)## # A tibble: 1 √ó 3\n##   variable        statistic     p\n##   <chr>               <dbl> <dbl>\n## 1 dissolving_time     0.980 0.964\n# create a Q-Q plot\ndissolving %>% \n  ggplot(aes(sample = dissolving_time)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\")"},{"path":"cs1-one-sample-t-test.html","id":"implement-test","chapter":"4 One-sample t-test","heading":"4.7.4 Implement test","text":"one-sample t-test indicated mean dissolving time drug significantly different 45s (t=0.366 , df=7 , p=0.725)","code":"\n# perform one-sample t-test\ndissolving %>% \n  t_test(dissolving_time ~ 1,\n         mu = 45,\n         alternative = \"two.sided\")## # A tibble: 1 √ó 7\n##   .y.             group1 group2         n statistic    df     p\n## * <chr>           <chr>  <chr>      <int>     <dbl> <dbl> <dbl>\n## 1 dissolving_time 1      null model     8     0.366     7 0.725"},{},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"cs1-onesample-wilcoxon-signed-rank","chapter":"5 Wilcoxon signed-rank test","heading":"5 Wilcoxon signed-rank test","text":"test also considers single sample, however test (contrast one sample t-test) don‚Äôt assume parent distribution normally distributed. still need parent distribution (consequently sample) symmetric though. test look see median parent distributions differs significantly given hypothesised value (contrast t-test looks mean).","code":""},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"section-commands-1","chapter":"5 Wilcoxon signed-rank test","heading":"5.1 Section commands","text":"New commands used section:, use fishlengthDF dataset. one-sample Wilcoxon signed-rank test allows see median body length different specified value. want test whether data support hypothesis median body actually 20 mm. following null alternative hypotheses similar used one sample t-test:\\(H_0\\): median body length equal 20 mm (\\(\\mu =\\) 20).\\(H_1\\): median body length equal 20 mm (\\(\\mu \\neq\\) 20).use one-sample, two-tailed Wilcoxon signed-rank test see reject null hypothesis .","code":""},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"summarise-and-visualise-1","chapter":"5 Wilcoxon signed-rank test","heading":"5.2 Summarise and visualise","text":"previous section, nothing really changed now (‚Äôre good start practical!)","code":""},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"assumptions-2","chapter":"5 Wilcoxon signed-rank test","heading":"5.3 Assumptions","text":"order use one-sample Wilcoxon rank-sum test analysis (results strictly valid) make two assumptions:parent distribution sample symmetricEach data point sample independent others. t-test common feature nearly statistical tests. Lack independence data really tough deal (impossible) large part proper experimental design ensuring .Whilst formal statistical tests symmetry opt simple visual inspection using boxplot histogram.Plot histogram boxplot data:get following plots (‚Äôll learn create panels like later!):can see whilst distribution isn‚Äôt perfectly symmetric, neither heavily skewed left right can make call distribution symmetric enough us happy results test.","code":"\n# create a histogram\nfishlengthDF %>% \n  ggplot(aes(x = length)) +\n  geom_histogram(bins = 10)\n\n# create boxplot\nfishlengthDF %>% \n  ggplot(aes(y = length)) +\n  geom_boxplot()"},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"implement-the-test-1","chapter":"5 Wilcoxon signed-rank test","heading":"5.4 Implement the test","text":"Perform one-sample, two-tailed Wilcoxon signed-rank test:syntax identical one-sample t-test carried earlier.formula, give length ~ 1 indicate one-sample test lengththe mu median tested null hypothesis, 20the alternative argument gives type alternative hypothesis must one two.sided, greater less. prior assumptions whether alternative median fish length greater less 20, choose two.sided.","code":"\nfishlengthDF %>% \n  wilcox_test(length ~ 1,\n              mu = 20,\n              alternative = \"two.sided\")"},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"interpreting-the-output-and-report-results-1","chapter":"5 Wilcoxon signed-rank test","heading":"5.5 Interpreting the output and report results","text":"output now see console windowthe statistic column gives us t-statistic 67.5 (‚Äôll need reporting)n column gives us sample size 29the p column gives us p-value 0.00122Again, p-value ‚Äôre interested . gives probability us getting sample null hypothesis actually true.\n, case since p-value less 0.05 can reject null hypothesis state :one-sample Wilcoxon signed-rank test indicated median body length male guppies (\\(\\mu\\) = 18.8 mm) differs significantly 20 mm (V = 67.5, n = 29, p = 0.0012).sentence adequate concluding statement test write paper report. Note included (brackets) information median value group (\\(\\mu\\) = 18.8 mm), test statistic (V = 67.5), number observations (n = 29), p-value (p = 0.0012).","code":"## # A tibble: 1 √ó 6\n##   .y.    group1 group2         n statistic       p\n## * <chr>  <chr>  <chr>      <int>     <dbl>   <dbl>\n## 1 length 1      null model    29      67.5 0.00122"},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"exercise-1","chapter":"5 Wilcoxon signed-rank test","heading":"5.6 Exercise","text":"Exercise 5.1  Performing Wilcoxon signed-rank test:Analyse drug dataset using one-sample Wilcoxon signed-rank testDiscuss (virtual) neighbour two tests feel best suited data.matter case?\\(H_0\\) : median \\(=\\) 45s\\(H_1\\) : median \\(\\neq\\) 45sFrom box-plot previous exercise already know data symmetric enough test valid.one-sample Wilcoxon-signed rank test indicated median dissolving time drug significantly different 45 s (V=22, n=8 , p=0.64)terms choosing two test can see meet respective assumptions tests valid. case tests also agree terms conclusions .e.¬†average dissolving time (either mean median) doesn‚Äôt differ significantly proposed value 45 s.one answer doesn‚Äôt matter test use.Another answer pick test measures quantity ‚Äôre interested .e. care medians use Wilcoxon test, whereas care means use t-test.final answer , since test valid prefer use test greater power. t-tests always power Wilcoxon tests (long ‚Äôre valid) report one. (‚Äôll talk last session power effectively capacity test detect significant difference - power better).","code":"\ndissolving %>% \n  wilcox_test(dissolving_time ~ 1,\n              mu = 45,\n              alternative = \"two.sided\")## # A tibble: 1 √ó 6\n##   .y.             group1 group2         n statistic     p\n## * <chr>           <chr>  <chr>      <int>     <dbl> <dbl>\n## 1 dissolving_time 1      null model     8        22 0.641"},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"hypotheses-1","chapter":"5 Wilcoxon signed-rank test","heading":"5.6.1 Hypotheses","text":"\\(H_0\\) : median \\(=\\) 45s\\(H_1\\) : median \\(\\neq\\) 45s","code":""},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"assumptions-3","chapter":"5 Wilcoxon signed-rank test","heading":"5.6.2 Assumptions","text":"box-plot previous exercise already know data symmetric enough test valid.","code":""},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"wilcoxon-signed-rank-test","chapter":"5 Wilcoxon signed-rank test","heading":"5.6.3 Wilcoxon signed-rank test","text":"one-sample Wilcoxon-signed rank test indicated median dissolving time drug significantly different 45 s (V=22, n=8 , p=0.64)","code":"\ndissolving %>% \n  wilcox_test(dissolving_time ~ 1,\n              mu = 45,\n              alternative = \"two.sided\")## # A tibble: 1 √ó 6\n##   .y.             group1 group2         n statistic     p\n## * <chr>           <chr>  <chr>      <int>     <dbl> <dbl>\n## 1 dissolving_time 1      null model     8        22 0.641"},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"discussion","chapter":"5 Wilcoxon signed-rank test","heading":"5.6.4 Discussion","text":"terms choosing two test can see meet respective assumptions tests valid. case tests also agree terms conclusions .e.¬†average dissolving time (either mean median) doesn‚Äôt differ significantly proposed value 45 s.one answer doesn‚Äôt matter test use.Another answer pick test measures quantity ‚Äôre interested .e. care medians use Wilcoxon test, whereas care means use t-test.final answer , since test valid prefer use test greater power. t-tests always power Wilcoxon tests (long ‚Äôre valid) report one. (‚Äôll talk last session power effectively capacity test detect significant difference - power better).","code":""},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"key-points","chapter":"5 Wilcoxon signed-rank test","heading":"5.7 Key points","text":"One-sample tests used single sample continuous dataWe can summarise data using summary() function visualise geom_boxplot()t-test assumes data normally distributed independent otherThe Wilcoxon signed-rank test assume normal distribution, require independent samplesThe t_test() compares mean parent distribution differs hypothesised value, whereas wilcox_test() compares median.good way assessing assumptions visually check looking distribution geom_histogram() quantile-quantile plots stat_qq() stat_qqline()","code":""},{},{"path":"cs1-students-t-test.html","id":"cs1-students-t-test","chapter":"6 Student‚Äôs t-test","heading":"6 Student‚Äôs t-test","text":"test assume sample data sets normally distributed equal variance. test see means two samples differ significantly .language used section slightly different used section 3.1. Although language used section 3.1 technically correct, sentences somewhat onerous read. ‚Äôve opted easier reading style expense technical accuracy. Please feel free re-write section (leisure).","code":""},{"path":"cs1-students-t-test.html","id":"section-commands-2","chapter":"6 Student‚Äôs t-test","heading":"6.1 Section commands","text":"New commands used section:","code":""},{"path":"cs1-students-t-test.html","id":"data-and-hypotheses-1","chapter":"6 Student‚Äôs t-test","heading":"6.2 Data and hypotheses","text":"example, suppose now measure body lengths male guppies (mm) collected two rivers Trinidad; Aripo Guanapo. want test whether mean body length differs samples. form following null alternative hypotheses:\\(H_0\\): mean body length differ two groups \\((\\mu = \\mu G)\\)\\(H_1\\): mean body length differ two groups \\((\\mu \\neq \\mu G)\\)use two-sample, two-tailed t-test see can reject null hypothesis.use two-sample test now two samples.use two-tailed t-test want know data suggest true (population) means different one another rather one mean specifically bigger smaller .‚Äôre using Student‚Äôs t-test sample sizes big ‚Äôre assuming parent populations equal variance (can check later).data stored file data/tidy/CS1-twosample.csv.Read R:","code":"\nrivers <- read_csv(\"data/tidy/CS1-twosample.csv\")"},{"path":"cs1-students-t-test.html","id":"cs1-students-sumvisual","chapter":"6 Student‚Äôs t-test","heading":"6.3 Summarise and visualise","text":"Let‚Äôs summarise data‚Ä¶visualise :boxplot appear suggest two samples different means, moreover guppies Guanapo may smaller guppies Aripo. isn‚Äôt immediately obvious two populations don‚Äôt equal variances though, plough .","code":"\n# get common summary stats for the length column\nrivers %>% \n  select(-id) %>% \n  group_by(river) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 2 √ó 11\n##   river   variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 Aripo   length      39  17.5  26.4   20.1   2.2  20.3  1.78 0.285 0.577\n## 2 Guanapo length      29  11.2  23.3   18.8   2.2  18.3  2.58 0.48  0.983\nrivers %>% \n  ggplot(aes(x = river, y = length)) +\n  geom_boxplot()"},{"path":"cs1-students-t-test.html","id":"assumptions-4","chapter":"6 Student‚Äôs t-test","heading":"6.4 Assumptions","text":"order use Student‚Äôs t-test (results strictly valid) make three assumptions:parent distributions samples taken normally distributed (lead sample data normally distributed ).data point samples independent others.parent distributions variance.example first assumption can ignored sample sizes large enough (maths, Aripo containing 39 Guanapo 29 samples). samples smaller use tests previous section.second point can nothing unless know data collected, ignore .third point regarding equality variance can tested using either Bartlett‚Äôs test (samples normally distributed) Levene‚Äôs test (samples normally distributed).\ngets bit trickier. Although don‚Äôt care samples normally distributed t-test valid (sample size big enough compensate), need know normally distributed order decide variance test use.perform Shapiro-Wilk test samples separately.can see whilst Guanapo data probably normally distributed (p = 0.1764 > 0.05), Aripo data unlikely normally distributed (p = 0.02802 < 0.05). Remember p-value gives probability observing sample parent population actually normally distributed.Shapiro-Wilk test quite sensitive sample size. means large sample even small deviations normality cause sample fail test, whereas smaller samples allowed pass much larger deviations. Aripo data nearly 40 points compared Guanapo data much easier Aripo sample fail compared Guanapo data.","code":"\n# group data by river and perform test\nrivers %>% \n  group_by(river) %>% \n  shapiro_test(length)## # A tibble: 2 √ó 4\n##   river   variable statistic      p\n##   <chr>   <chr>        <dbl>  <dbl>\n## 1 Aripo   length       0.936 0.0280\n## 2 Guanapo length       0.949 0.176"},{"path":"cs1-students-t-test.html","id":"exercise-qq-rivers","chapter":"6 Student‚Äôs t-test","heading":"6.5 Exercise: Q-Q plots rivers","text":"Exercise 6.1  Q-Q plots rivers dataCreate Q-Q plots two samples discuss neighbour see light results Shapiro-Wilk test.Q-Q plots mirror found Shapiro-Wilk tests: data Aripo pretty normally distributed, whereas assumption normality Guanapo data less certain.","code":"\n# we group the data by river\n# then create a panel per river\n# containing the Q-Q plot for that river\nrivers %>% \n  group_by(river) %>%\n  ggplot(aes(sample = length)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\") +\n  facet_wrap(facets = vars(river))"},{"path":"cs1-students-t-test.html","id":"equality-of-variance","chapter":"6 Student‚Äôs t-test","heading":"6.6 Equality of variance","text":"Remember statistical tests provide answers, merely suggest patterns. Human interpretation still crucial aspect .Shapiro-Wilk test shown data normal enough order test equality variance use Levene‚Äôs test.key bit information p column. p-value (0.1876) test. tells us probability observing two samples come distributions variance. probability greater arbitrary significance level 0.05 can somewhat confident necessary assumptions carrying Student‚Äôs t-test two samples valid. (woohoo!)","code":"\nrivers %>% \n  levene_test(length ~ river)## # A tibble: 1 √ó 4\n##     df1   df2 statistic     p\n##   <int> <int>     <dbl> <dbl>\n## 1     1    66      1.77 0.188"},{"path":"cs1-students-t-test.html","id":"bartletts-test","chapter":"6 Student‚Äôs t-test","heading":"6.6.1 Bartlett‚Äôs test","text":"wanted carry Bartlett‚Äôs test (.e.¬†data sufficiently normally distributed) command :relevant p-value given 3rd line.use bartlett.test() base R. Surprisingly, rstatix package built-equivalent.wanted get output Bartlett test tidy format, following, take rivers data set pipe bartlett.test() function. Note need define data using dot (.), first input bartlett.test() data. pipe output tidy() function, part broom library, kindly converts output tidy format. Handy!","code":"\nbartlett.test(length ~ river, data = rivers)## \n##  Bartlett test of homogeneity of variances\n## \n## data:  length by river\n## Bartlett's K-squared = 4.4734, df = 1, p-value = 0.03443\n# load the broom package\nlibrary(broom)\n\n# perform Bartlett's test on the data and tidy\nrivers %>% \n  bartlett.test(length ~ river,\n                data = .) %>% \n  tidy()## # A tibble: 1 √ó 4\n##   statistic p.value parameter method                                   \n##       <dbl>   <dbl>     <dbl> <chr>                                    \n## 1      4.47  0.0344         1 Bartlett test of homogeneity of variances"},{"path":"cs1-students-t-test.html","id":"implement-test-1","chapter":"6 Student‚Äôs t-test","heading":"6.7 Implement test","text":"case ‚Äôre ignoring fact data normal enough, according Shapiro-Wilk test. However, sample sizes pretty large t-test also pretty robust case, can perform t-test. Remember, allowed variances two groups (Aripo Guanapo) equal.Perform two-sample, two-tailed, t-test:following:take data set pipe t_test() functionThe t_test() function takes formula format variable ~ categoryAgain alternative two.sided prior knowledge whether alternative greater lessThe last argument says whether variance two samples can assumed equal (Student‚Äôs t-test) unequal (Welch‚Äôs t-test)","code":"\n# two-sample, two-tailed t-test\nrivers %>% \n  t_test(length ~ river,\n         alternative = \"two.sided\",\n         var.equal = TRUE)"},{"path":"cs1-students-t-test.html","id":"interpret-output-and-report-results","chapter":"6 Student‚Äôs t-test","heading":"6.8 Interpret output and report results","text":"Let‚Äôs look results t-test performed original (stacked) data frame:first 5 columns give information variable (.y.), groups sample size groupThe statistic column gives t-value 3.8433 (need reporting)df column tell us 66 degrees freedom (need reporting)p column gives us p-value 0.0002754Again, p-value ‚Äôre interested . Since p-value small (much smaller standard significance level) choose say ‚Äúunlikely two samples came parent distribution can reject null hypothesis‚Äù state :Student‚Äôs t-test indicated mean body length male guppies Guanapo river (18.29 mm) differs significantly mean body length male guppies Aripo river (20.33 mm) (t = 3.8433, df = 66, p = 0.0003).Now ‚Äôs conversation starter.","code":"## # A tibble: 1 √ó 8\n##   .y.    group1 group2     n1    n2 statistic    df        p\n## * <chr>  <chr>  <chr>   <int> <int>     <dbl> <dbl>    <dbl>\n## 1 length Aripo  Guanapo    39    29      3.84    66 0.000275"},{"path":"cs1-students-t-test.html","id":"exercise-turtles","chapter":"6 Student‚Äôs t-test","heading":"6.9 Exercise: Turtles","text":"Exercise 6.2  Serum cholesterol concentrations turtlesUsing following data, test null hypothesis male female turtles mean serum cholesterol concentrations.Create tidy data frame save .csv fileWrite null alternative hypothesesImport data RSummarise visualise dataCheck assumptions (normality variance) using appropriate tests plotsPerform two-sample t-testWrite sentence summarises results foundHere data tidy format, variable column, row observation (unique identifier observation).\\(H_0\\) : male mean \\(=\\) female mean\\(H_1\\) : male mean \\(\\neq\\) female meanI‚Äôd always recommend storing data tidy, stacked format (fact can‚Äôt think situation want store data untidy, unstacked format!) example manually input data Excel following layout, saving data CSV file reading :Let‚Äôs summarise data (although visualisation probably much easier work ):visualise data:always use plot summary assess three things:look like ‚Äôve loaded data correctly?\ntwo groups extreme values plots seem match dataset, ‚Äôm happy haven‚Äôt done anything massively wrong .\ntwo groups extreme values plots seem match dataset, ‚Äôm happy haven‚Äôt done anything massively wrong .think difference two groups?\nneed result formal test make sense given data, ‚Äôs important develop sense think going happen . Whilst ranges two groups suggests Female serum levels might higher males look things closely realise isn‚Äôt case. boxplot shows median values two groups virtually identical backed summary statistics calculated: medians 224.1, means fairly close (225.7 vs 224.2). Based , fact 13 observations total surprised test came back showing difference groups.\nneed result formal test make sense given data, ‚Äôs important develop sense think going happen . Whilst ranges two groups suggests Female serum levels might higher males look things closely realise isn‚Äôt case. boxplot shows median values two groups virtually identical backed summary statistics calculated: medians 224.1, means fairly close (225.7 vs 224.2). Based , fact 13 observations total surprised test came back showing difference groups.think assumptions?\nNormality looks bit worrying: whilst Male group appears nice symmetric (might normal), Female group appears quite skewed (since median much closer bottom top). ‚Äôll look carefully formal checks decided whether think data normal enough us use t-test.\nHomogeneity variance. stage spread data within group looks similar, potential skew Female group ‚Äôll want check assumptions carefully.\nNormality looks bit worrying: whilst Male group appears nice symmetric (might normal), Female group appears quite skewed (since median much closer bottom top). ‚Äôll look carefully formal checks decided whether think data normal enough us use t-test.Homogeneity variance. stage spread data within group looks similar, potential skew Female group ‚Äôll want check assumptions carefully.NormalityLet‚Äôs look normality groups separately. several ways getting serum values Males Females separately. ‚Äôll use unstacking method, use Shapiro-Wilk followed qqplots.p-values Shapiro-Wilk tests non-significant suggests data normal enough. bit surprising given saw boxplot two bits information can use reassure us.p-value Female group smaller Male group (suggesting Female group closer non-normal Male group) makes sense.Shapiro-Wilk test generally quite relaxed normality small sample sizes (notoriously strict large sample sizes). group 6 data points , data actually really, really skewed distribution. Given Female group 6 data points , ‚Äôs surprising Shapiro-Wilk test came back saying everything OK.results Q-Q plots echo ‚Äôve already seen Shapiro-Wilk analyses. Male group doesn‚Äôt look bad whereas Female group looks somewhat dodgy.Overall, assumption normality data doesn‚Äôt appear well met , bear mind data points group might just seeing pattern data due random chance rather underlying populations actually normally distributed. Personally, though ‚Äôd edge towards non-normal .Homogeneity VarianceIt‚Äôs clear whether data normal , isn‚Äôt clear test use . sensible approach hope agree (fingers crossed!)Bartlett‚Äôs test gives us:Levene‚Äôs test gives us:good news Levene Bartlett agree homogeneity variance two groups (thank goodness!).Overall, means ‚Äôre sure normality, homogeneity variance pretty good.result Bartlett test know can carry two-sample Student‚Äôs t-test (opposed two-sample Welch‚Äôs t-test, ‚Äôre confused, see Figure 3.2)p-value 0.544, test tells insufficient evidence suggest means two groups different. suitable summary sentence :Student‚Äôs two-sample t-test indicated mean serum cholesterol level differ significantly Male Female turtles (t = 0.627, df = 11, p = 0.544).reality, ambiguous normality assumption assessment, dataset actually carry two different tests; two-sample t-test equal variance Mann-Whitney U test. agreed wouldn‚Äôt matter much one reported (‚Äôd personally report short sentence say ‚Äôm wasn‚Äôt clear whether assumption normality met), acceptable report just one.","code":"\n# load the data\nturtle <- read_csv(\"data/tidy/CS1-turtle.csv\")\n\n# and have a look\nturtle## # A tibble: 13 √ó 3\n##       id serum sex   \n##    <dbl> <dbl> <chr> \n##  1     1  220. Male  \n##  2     2  219. Male  \n##  3     3  230. Male  \n##  4     4  229. Male  \n##  5     5  222  Male  \n##  6     6  224. Male  \n##  7     7  226. Male  \n##  8     8  223. Female\n##  9     9  222. Female\n## 10    10  230. Female\n## 11    11  224. Female\n## 12    12  224. Female\n## 13    13  231. Female\n# create summary statistics for each group\nturtle %>% \n  select(-id) %>% \n  group_by(sex) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 2 √ó 11\n##   sex    variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>  <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 Female serum        6  222.  231.   224.  5.22  226.  3.87  1.58  4.06\n## 2 Male   serum        7  219.  230.   224.  6.6   224.  4.26  1.61  3.94\n# visualise the data\nturtle %>% \n  ggplot(aes(x = sex, y = serum)) +\n  geom_boxplot()\n# perform Shapiro-Wilk test on each group\nturtle %>% \n  group_by(sex) %>% \n  shapiro_test(serum)## # A tibble: 2 √ó 4\n##   sex    variable statistic     p\n##   <chr>  <chr>        <dbl> <dbl>\n## 1 Female serum        0.842 0.135\n## 2 Male   serum        0.944 0.674\n# create Q-Q plots for both groups\nturtle %>% \n  ggplot(aes(sample = serum)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\") +\n  facet_wrap(facets = vars(sex))\n# perform Bartlett's test\nbartlett.test(serum ~ sex,\n              data = turtle)## \n##  Bartlett test of homogeneity of variances\n## \n## data:  serum by sex\n## Bartlett's K-squared = 0.045377, df = 1, p-value = 0.8313\n# perform Levene's test\nturtle %>% \n  levene_test(serum ~ sex)## # A tibble: 1 √ó 4\n##     df1   df2 statistic     p\n##   <int> <int>     <dbl> <dbl>\n## 1     1    11     0.243 0.631\n# perform two-sample t-test\nturtle %>% \n  t_test(serum ~ sex,\n         alternative = \"two.sided\",\n         var.equal = TRUE)## # A tibble: 1 √ó 8\n##   .y.   group1 group2    n1    n2 statistic    df     p\n## * <chr> <chr>  <chr>  <int> <int>     <dbl> <dbl> <dbl>\n## 1 serum Female Male       6     7     0.627    11 0.544"},{"path":"cs1-students-t-test.html","id":"data","chapter":"6 Student‚Äôs t-test","heading":"6.9.1 Data","text":"data tidy format, variable column, row observation (unique identifier observation).","code":""},{"path":"cs1-students-t-test.html","id":"hypotheses-2","chapter":"6 Student‚Äôs t-test","heading":"6.9.2 Hypotheses","text":"\\(H_0\\) : male mean \\(=\\) female mean\\(H_1\\) : male mean \\(\\neq\\) female mean","code":""},{"path":"cs1-students-t-test.html","id":"load-summarise-and-visualise-data","chapter":"6 Student‚Äôs t-test","heading":"6.9.3 Load, summarise and visualise data","text":"‚Äôd always recommend storing data tidy, stacked format (fact can‚Äôt think situation want store data untidy, unstacked format!) example manually input data Excel following layout, saving data CSV file reading :Let‚Äôs summarise data (although visualisation probably much easier work ):visualise data:always use plot summary assess three things:look like ‚Äôve loaded data correctly?\ntwo groups extreme values plots seem match dataset, ‚Äôm happy haven‚Äôt done anything massively wrong .\ntwo groups extreme values plots seem match dataset, ‚Äôm happy haven‚Äôt done anything massively wrong .think difference two groups?\nneed result formal test make sense given data, ‚Äôs important develop sense think going happen . Whilst ranges two groups suggests Female serum levels might higher males look things closely realise isn‚Äôt case. boxplot shows median values two groups virtually identical backed summary statistics calculated: medians 224.1, means fairly close (225.7 vs 224.2). Based , fact 13 observations total surprised test came back showing difference groups.\nneed result formal test make sense given data, ‚Äôs important develop sense think going happen . Whilst ranges two groups suggests Female serum levels might higher males look things closely realise isn‚Äôt case. boxplot shows median values two groups virtually identical backed summary statistics calculated: medians 224.1, means fairly close (225.7 vs 224.2). Based , fact 13 observations total surprised test came back showing difference groups.think assumptions?\nNormality looks bit worrying: whilst Male group appears nice symmetric (might normal), Female group appears quite skewed (since median much closer bottom top). ‚Äôll look carefully formal checks decided whether think data normal enough us use t-test.\nHomogeneity variance. stage spread data within group looks similar, potential skew Female group ‚Äôll want check assumptions carefully.\nNormality looks bit worrying: whilst Male group appears nice symmetric (might normal), Female group appears quite skewed (since median much closer bottom top). ‚Äôll look carefully formal checks decided whether think data normal enough us use t-test.Homogeneity variance. stage spread data within group looks similar, potential skew Female group ‚Äôll want check assumptions carefully.","code":"\n# load the data\nturtle <- read_csv(\"data/tidy/CS1-turtle.csv\")\n\n# and have a look\nturtle## # A tibble: 13 √ó 3\n##       id serum sex   \n##    <dbl> <dbl> <chr> \n##  1     1  220. Male  \n##  2     2  219. Male  \n##  3     3  230. Male  \n##  4     4  229. Male  \n##  5     5  222  Male  \n##  6     6  224. Male  \n##  7     7  226. Male  \n##  8     8  223. Female\n##  9     9  222. Female\n## 10    10  230. Female\n## 11    11  224. Female\n## 12    12  224. Female\n## 13    13  231. Female\n# create summary statistics for each group\nturtle %>% \n  select(-id) %>% \n  group_by(sex) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 2 √ó 11\n##   sex    variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>  <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 Female serum        6  222.  231.   224.  5.22  226.  3.87  1.58  4.06\n## 2 Male   serum        7  219.  230.   224.  6.6   224.  4.26  1.61  3.94\n# visualise the data\nturtle %>% \n  ggplot(aes(x = sex, y = serum)) +\n  geom_boxplot()"},{"path":"cs1-students-t-test.html","id":"assumptions-5","chapter":"6 Student‚Äôs t-test","heading":"6.9.4 Assumptions","text":"NormalityLet‚Äôs look normality groups separately. several ways getting serum values Males Females separately. ‚Äôll use unstacking method, use Shapiro-Wilk followed qqplots.p-values Shapiro-Wilk tests non-significant suggests data normal enough. bit surprising given saw boxplot two bits information can use reassure us.p-value Female group smaller Male group (suggesting Female group closer non-normal Male group) makes sense.Shapiro-Wilk test generally quite relaxed normality small sample sizes (notoriously strict large sample sizes). group 6 data points , data actually really, really skewed distribution. Given Female group 6 data points , ‚Äôs surprising Shapiro-Wilk test came back saying everything OK.results Q-Q plots echo ‚Äôve already seen Shapiro-Wilk analyses. Male group doesn‚Äôt look bad whereas Female group looks somewhat dodgy.Overall, assumption normality data doesn‚Äôt appear well met , bear mind data points group might just seeing pattern data due random chance rather underlying populations actually normally distributed. Personally, though ‚Äôd edge towards non-normal .Homogeneity VarianceIt‚Äôs clear whether data normal , isn‚Äôt clear test use . sensible approach hope agree (fingers crossed!)Bartlett‚Äôs test gives us:Levene‚Äôs test gives us:good news Levene Bartlett agree homogeneity variance two groups (thank goodness!).Overall, means ‚Äôre sure normality, homogeneity variance pretty good.","code":"\n# perform Shapiro-Wilk test on each group\nturtle %>% \n  group_by(sex) %>% \n  shapiro_test(serum)## # A tibble: 2 √ó 4\n##   sex    variable statistic     p\n##   <chr>  <chr>        <dbl> <dbl>\n## 1 Female serum        0.842 0.135\n## 2 Male   serum        0.944 0.674\n# create Q-Q plots for both groups\nturtle %>% \n  ggplot(aes(sample = serum)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\") +\n  facet_wrap(facets = vars(sex))\n# perform Bartlett's test\nbartlett.test(serum ~ sex,\n              data = turtle)## \n##  Bartlett test of homogeneity of variances\n## \n## data:  serum by sex\n## Bartlett's K-squared = 0.045377, df = 1, p-value = 0.8313\n# perform Levene's test\nturtle %>% \n  levene_test(serum ~ sex)## # A tibble: 1 √ó 4\n##     df1   df2 statistic     p\n##   <int> <int>     <dbl> <dbl>\n## 1     1    11     0.243 0.631"},{"path":"cs1-students-t-test.html","id":"implement-two-sample-t-test","chapter":"6 Student‚Äôs t-test","heading":"6.9.5 Implement two-sample t-test","text":"result Bartlett test know can carry two-sample Student‚Äôs t-test (opposed two-sample Welch‚Äôs t-test, ‚Äôre confused, see Figure 3.2)p-value 0.544, test tells insufficient evidence suggest means two groups different. suitable summary sentence :Student‚Äôs two-sample t-test indicated mean serum cholesterol level differ significantly Male Female turtles (t = 0.627, df = 11, p = 0.544).","code":"\n# perform two-sample t-test\nturtle %>% \n  t_test(serum ~ sex,\n         alternative = \"two.sided\",\n         var.equal = TRUE)## # A tibble: 1 √ó 8\n##   .y.   group1 group2    n1    n2 statistic    df     p\n## * <chr> <chr>  <chr>  <int> <int>     <dbl> <dbl> <dbl>\n## 1 serum Female Male       6     7     0.627    11 0.544"},{"path":"cs1-students-t-test.html","id":"discussion-1","chapter":"6 Student‚Äôs t-test","heading":"6.9.6 Discussion","text":"reality, ambiguous normality assumption assessment, dataset actually carry two different tests; two-sample t-test equal variance Mann-Whitney U test. agreed wouldn‚Äôt matter much one reported (‚Äôd personally report short sentence say ‚Äôm wasn‚Äôt clear whether assumption normality met), acceptable report just one.","code":""},{},{"path":"cs1-mannwhitney-u-test.html","id":"cs1-mannwhitney-u-test","chapter":"7 Mann-Whitney U test","heading":"7 Mann-Whitney U test","text":"test also compares two samples, however test (contrast Student‚Äôs t-test) don‚Äôt assume parent distributions normally distributed. order compare medians two groups still need parent distributions (consequently samples) shape variance. test look see medians two parent distributions differ significantly .","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"section-commands-3","chapter":"7 Mann-Whitney U test","heading":"7.1 Section commands","text":"new commands used section.","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"data-and-hypotheses-2","chapter":"7 Mann-Whitney U test","heading":"7.2 Data and hypotheses","text":", use rivers dataset. want test whether median body length male guppies differs samples. form following null alternative hypotheses:\\(H_0\\): difference median body length two groups 0 \\((\\mu - \\mu G = 0)\\)\\(H_1\\): difference median body length two groups 0 \\((\\mu - \\mu G \\neq 0)\\)use two-tailed Mann-Whitney U test see can reject null hypothesis.","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"summarise-and-visualise-2","chapter":"7 Mann-Whitney U test","heading":"7.3 Summarise and visualise","text":"previous section.","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"assumptions-6","chapter":"7 Mann-Whitney U test","heading":"7.4 Assumptions","text":"checked previously.","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"implement-test-2","chapter":"7 Mann-Whitney U test","heading":"7.5 Implement test","text":"Perform two-tailed, Mann-Whitney U test:first argument must formula format: variable ~ categoryThe second argument gives type alternative hypothesis must one two.sided, greater less","code":"\nrivers %>% \n  wilcox_test(length ~ river,\n              alternative = \"two.sided\")## # A tibble: 1 √ó 7\n##   .y.    group1 group2     n1    n2 statistic        p\n## * <chr>  <chr>  <chr>   <int> <int>     <dbl>    <dbl>\n## 1 length Aripo  Guanapo    39    29       841 0.000646"},{"path":"cs1-mannwhitney-u-test.html","id":"interpret-output-and-report-results-1","chapter":"7 Mann-Whitney U test","heading":"7.6 Interpret output and report results","text":"may get warning message console stating compute exact p-value ties. just means data points exactly value affects internal mathematics slightly. However, given p-value small, something need worry .first 5 columns give information variable (.y.), groups sample size groupThe statistic column gives t-value 841 (need reporting)p column gives us p-value 0.0006464.Given p-value less 0.05 can reject null hypothesis confidence level.\n, p-value 3rd line ‚Äôre interested . Since p-value small (much smaller standard significance level) choose say ‚Äúunlikely two samples came parent distribution can reject null hypothesis.‚Äùput completely, can state :Mann-Whitney test indicated median body length male guppies Guanapo river (18.8 mm) differs significantly median body length male guppies Aripo river (20.1 mm) (W = 841, p = 0.0006).","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"exercise-2","chapter":"7 Mann-Whitney U test","heading":"7.7 Exercise","text":"Exercise 7.1  Analyse turtle dataset using Mann Whitney test.follow process Student‚Äôs t-test.\\(H_0\\) : male median \\(=\\) female median\\(H_1\\) : male median \\(\\neq\\) female medianThis .‚Äôve already checked variances two groups similar, ‚Äôre OK . Whilst Mann-Whitney test doesn‚Äôt require normality symmetry distributions require distributions shape. example, just handful data points group, ‚Äôs quite hard make call one way another. advice case say unless ‚Äôs obvious distributions different can just allow assumption pass, ‚Äôre going see obvious differences distribution shape considerably data points .gives us exactly conclusion got two-sample t-test .e. isn‚Äôt significant difference two groups.Mann-Whitney test indicated wasn‚Äôt significant difference median Serum Cholesterol levels male female turtles (W = 26, p = 0.534)","code":"\nturtle %>% \n  wilcox_test(serum ~ sex,\n              alternative = \"two.sided\")## # A tibble: 1 √ó 7\n##   .y.   group1 group2    n1    n2 statistic     p\n## * <chr> <chr>  <chr>  <int> <int>     <dbl> <dbl>\n## 1 serum Female Male       6     7        26 0.534"},{"path":"cs1-mannwhitney-u-test.html","id":"hypotheses-3","chapter":"7 Mann-Whitney U test","heading":"7.7.1 Hypotheses","text":"\\(H_0\\) : male median \\(=\\) female median\\(H_1\\) : male median \\(\\neq\\) female median","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"summarise-and-visualise-3","chapter":"7 Mann-Whitney U test","heading":"7.7.2 Summarise and visualise","text":".","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"assumptions-7","chapter":"7 Mann-Whitney U test","heading":"7.7.3 Assumptions","text":"‚Äôve already checked variances two groups similar, ‚Äôre OK . Whilst Mann-Whitney test doesn‚Äôt require normality symmetry distributions require distributions shape. example, just handful data points group, ‚Äôs quite hard make call one way another. advice case say unless ‚Äôs obvious distributions different can just allow assumption pass, ‚Äôre going see obvious differences distribution shape considerably data points .","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"carry-out-a-mann-whitney-test","chapter":"7 Mann-Whitney U test","heading":"7.7.4 Carry out a Mann-Whitney test","text":"gives us exactly conclusion got two-sample t-test .e. isn‚Äôt significant difference two groups.Mann-Whitney test indicated wasn‚Äôt significant difference median Serum Cholesterol levels male female turtles (W = 26, p = 0.534)","code":"\nturtle %>% \n  wilcox_test(serum ~ sex,\n              alternative = \"two.sided\")## # A tibble: 1 √ó 7\n##   .y.   group1 group2    n1    n2 statistic     p\n## * <chr> <chr>  <chr>  <int> <int>     <dbl> <dbl>\n## 1 serum Female Male       6     7        26 0.534"},{},{"path":"cs1-paired-two-sample-t-test.html","id":"cs1-paired-two-sample-t-test","chapter":"8 Paired two-sample t-test","heading":"8 Paired two-sample t-test","text":"paired t-test used two samples continuous data can paired (examples sort data weights individuals diet). test applicable number paired points within samples large (>30) , number points small, test also works parent distributions normally distributed.","code":""},{"path":"cs1-paired-two-sample-t-test.html","id":"section-commands-4","chapter":"8 Paired two-sample t-test","heading":"8.1 Section commands","text":"","code":""},{"path":"cs1-paired-two-sample-t-test.html","id":"data-and-hypotheses-3","chapter":"8 Paired two-sample t-test","heading":"8.2 Data and hypotheses","text":"example, suppose measure cortisol levels 20 adult females (nmol/l) first thing morning evening. want test whether cortisol levels differs two measurement times. initially form following null alternative hypotheses:\\(H_0\\): difference cortisol level times (\\(\\mu M = \\mu E\\))\\(H_1\\): difference cortisol levels times (\\(\\mu M \\neq \\mu E\\))use two-sample, two-tailed paired t-test see can reject null hypothesis.use two-sample test now two samplesWe use two-tailed t-test want know data suggest true (population) means different one another rather one mean specifically bigger smaller otherWe use paired test data point first sample can linked another data point second sample connecting factorWe‚Äôre using t-test ‚Äôre assuming parent populations normal equal variance (‚Äôll check bit)data stored tidy format file data/tidy/CS1-twopaired.csv.\nRead R:can see data frame consists three columns:patient_id, unique ID patienttime cortisol level measuredcortisol, contains measured value.patient_id two measurements: one morning one afternoon.","code":"\n# load the data\ncortisol <- read_csv(\"data/tidy/CS1-twopaired.csv\")\n\n# have a look at the data\ncortisol## # A tibble: 40 √ó 3\n##    patient_id time    cortisol\n##         <dbl> <chr>      <dbl>\n##  1          1 morning     311.\n##  2          2 morning     146.\n##  3          3 morning     297 \n##  4          4 morning     271.\n##  5          5 morning     268.\n##  6          6 morning     264.\n##  7          7 morning     358.\n##  8          8 morning     316.\n##  9          9 morning     336.\n## 10         10 morning     221.\n## # ‚Ä¶ with 30 more rows"},{"path":"cs1-paired-two-sample-t-test.html","id":"summarise-and-visualise-4","chapter":"8 Paired two-sample t-test","heading":"8.3 Summarise and visualise","text":"use also visualise actual data points, get sense data spread . avoid overlapping data points (try using geom_point() instead geom_jitter()), jitter data points. geom_jitter() add small amount variation point.However, plot capture cortisol level individual subject changed though. can explore individual changes morning evening creating boxplot differences two times measurement., need put data wide format, using pivot_wider().differences cortisol levels appear much less zero, (meaning evening cortisol levels appear much lower morning ones). expect test give pretty significant result.alternative representation plot data points evening morning connect patient:gives similar picture boxplot telling us, patients cortisol levels higher morning evening.","code":"\n# create a boxplot\ncortisol %>% \n  ggplot(aes(x = time, y = cortisol)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2) +\n  ylab(\"Cortisol level (nmol/l)\")\n# calculate the difference between evening and morning values\ncortisol_diff <- cortisol %>%\n  pivot_wider(names_from = time, values_from = cortisol) %>% \n  mutate(cortisol_change = evening - morning)\n\n# plot the data\n  ggplot(cortisol_diff, aes(y = cortisol_change)) +\n  geom_boxplot() +\n  ylab(\"Change in cortisol (nmol/l)\")\n# plot cortisol levels by patient\ncortisol %>% \n  ggplot(aes(x = time, y = cortisol, group = patient_id)) +\n  geom_point() +\n  geom_line()"},{"path":"cs1-paired-two-sample-t-test.html","id":"assumptions-8","chapter":"8 Paired two-sample t-test","heading":"8.4 Assumptions","text":"exercise!","code":""},{"path":"cs1-paired-two-sample-t-test.html","id":"implement-test-3","chapter":"8 Paired two-sample t-test","heading":"8.5 Implement test","text":"Perform two-sample, two-tailed, paired t-test:first argument gives formulaThe second argument gives type alternative hypothesis must one two.sided, greater lessThe third argument says data paired","code":"\n# perform the test\ncortisol %>% \n  t_test(cortisol ~ time,\n         alternative = \"two.sided\",\n         paired = TRUE)"},{"path":"cs1-paired-two-sample-t-test.html","id":"interpret-output-and-report-results-2","chapter":"8 Paired two-sample t-test","heading":"8.6 Interpret output and report results","text":"perspective value interested p column (p-value = 5.29 \\(\\cdot\\) 10-5). Given substantially less 0.05 can reject null hypothesis state:two-tailed, paired t-test indicated cortisol level adult females differed significantly morning (313.5 nmol/l) evening (197.4 nmol/l) (t = -5.1, df = 19, p = 5.3 * 10-5).","code":"## # A tibble: 1 √ó 8\n##   .y.      group1  group2     n1    n2 statistic    df         p\n## * <chr>    <chr>   <chr>   <int> <int>     <dbl> <dbl>     <dbl>\n## 1 cortisol evening morning    20    20     -5.18    19 0.0000529"},{"path":"cs1-paired-two-sample-t-test.html","id":"exercise-assumptions","chapter":"8 Paired two-sample t-test","heading":"8.7 Exercise: Assumptions","text":"Exercise 8.1  Checking assumptionsCheck assumptions necessary paired t-test.\npaired t-test appropriate test?paired test really just one-sample test disguise. actually don‚Äôt care much distributions individual groups. Instead care properties differences. paired t-test valid dataset, need differences morning evening values normally distributed.Let‚Äôs check Shapiro-Wilk Q-Q plots using cortisol_diff variable created earlier.Shapiro-Wilk test says data normal enough whilst Q-Q plot mostly fine, suggestion snaking bottom left. ‚Äôm actually OK suggestion snaking actually due single point (last point left). cover point thumb (finger choice) remaining points Q-Q plot look pretty good, suggestion snaking actually driven single point (can happen chance). ‚Äôm actually happy assumption normality well met case. single point check useful thing remember assessing diagnostic plots., yep, paired t-test appropriate dataset.","code":"\n# perform Shapiro-Wilk test on cortisol differences\ncortisol_diff %>% \n  shapiro_test(cortisol_change)## # A tibble: 1 √ó 3\n##   variable        statistic     p\n##   <chr>               <dbl> <dbl>\n## 1 cortisol_change     0.924 0.116\n# and create the Q-Q plot\ncortisol_diff %>% \n  ggplot(aes(sample = cortisol_change)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\")"},{},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"cs1-twosample-wilcoxon-signed-rank","chapter":"9 Wilcoxon signed-rank test","heading":"9 Wilcoxon signed-rank test","text":"Wilcoxon signed-rank test alternative paired t-test. require data drawn normal distributions, require distribution differences symmetric. ‚Äôre effectively testing see median differences two samples differs significantly zero.","code":""},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"section-commands-5","chapter":"9 Wilcoxon signed-rank test","heading":"9.1 Section commands","text":"new commands section.","code":""},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"data-and-hypotheses-4","chapter":"9 Wilcoxon signed-rank test","heading":"9.2 Data and hypotheses","text":"Using cortisol dataset form following null alternative hypotheses:\\(H_0\\): median difference cortisol levels two groups 0 \\((\\mu M = \\mu E)\\)\\(H_1\\): median difference cortisol levels two groups 0 \\((\\mu M \\neq \\mu E)\\)use two-tailed Wilcoxon signed-rank test see can reject null hypothesis.","code":""},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"summarise-and-visualise-5","chapter":"9 Wilcoxon signed-rank test","heading":"9.3 Summarise and visualise","text":"Already implemented previously.","code":""},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"assumptions-9","chapter":"9 Wilcoxon signed-rank test","heading":"9.4 Assumptions","text":"checked previously.","code":""},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"implement-test-4","chapter":"9 Wilcoxon signed-rank test","heading":"9.5 Implement test","text":"Perform two-tailed, Wilcoxon signed-rank test:first argument gives formulaThe second argument gives type alternative hypothesis must one two.sided, greater lessThe third argument says data paired","code":"\n# perform the test\ncortisol %>% \n  wilcox_test(cortisol ~ time,\n              alternative = \"two.sided\",\n              paired = TRUE)"},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"interpret-output-and-report-results-3","chapter":"9 Wilcoxon signed-rank test","heading":"9.6 Interpret output and report results","text":"p-value given p column (p-value = 0.000168). Given less 0.05 can still reject null hypothesis.two-tailed, Wilcoxon signed-rank test indicated median cortisol level adult females differed significantly morning (320.5 nmol/l) evening (188.9 nmol/l) (V = 197, p = 0.00017).","code":"## # A tibble: 1 √ó 7\n##   .y.      group1  group2     n1    n2 statistic        p\n## * <chr>    <chr>   <chr>   <int> <int>     <dbl>    <dbl>\n## 1 cortisol evening morning    20    20        13 0.000168"},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"exercise-deer-legs","chapter":"9 Wilcoxon signed-rank test","heading":"9.7 Exercise: Deer legs","text":"Exercise 9.1  Deer legsUsing following data, test null hypothesis fore hind legs deer length.results provide evidence suggest fore- hind-leg length differ deer?Write null alternative hypothesesChoose tidy representation data create csv file (‚Äôll stop asking now ‚Ä¶)Import data RSummarise visualise dataCheck assumptions (normality variance) using appropriate testsDiscuss (virtual) neighbour test appropriate?Perform testWrite sentence summarises results found\\(H_0\\) : foreleg average (mean median) \\(=\\) hindleg average (mean median)\\(H_1\\) : foreleg average \\(\\neq\\) hindleg averageFirst , need get data tidy format (every variable column, observation row). Excel, adding ID gives us:ordering data important ; first hindleg row corresponds first foreleg row, second second . indicate use id column, observation unique ID.Let‚Äôs look data see can see.suggests might difference legs, hindlegs longer forelegs. However, representation obscures fact paired data. really need look difference leg length deer data observation:Additionally, can also plot data observation:gives us much clearer picture. looks though hindlegs 4 cm longer forelegs, average. also suggests leg differences might normally distributed (data look bit skewed boxplot).need consider distribution difference leg lengths rather individual distributions.Shapiro-Wilk test Q-Q plot suggest difference data aren‚Äôt normally distributed, rules paired t-test. therefore consider paired Wilcoxon test next. Remember test requires distribution differences symmetric, whereas box-plot suggested data much skewed., frustratingly, neither tests appropriate dataset. differences foreleg hindleg lengths neither normal enough paired t-test symmetric enough Wilcoxon test don‚Äôt enough data just use t-test (‚Äôd need 30 points ). situation? Well answer aren‚Äôt actually traditional statistical tests valid dataset stands!two options available someone:try transforming raw data (take logs, square root, reciprocals) hope one leads modified dataset satisfies assumptions one tests ‚Äôve covered, oruse permutation test approach (work beyond scope course).reason included example first practical purely illustrate simple dataset apparently clear message (leg lengths differ within deer) can intractable. don‚Äôt need complex datasets go beyond capabilities classical statistics.Jeremy Clarkson put :bombshell, ‚Äôs time end. Goodnight!","code":"## # A tibble: 10 √ó 2\n##    hindleg foreleg\n##      <dbl>   <dbl>\n##  1     142     138\n##  2     140     136\n##  3     144     147\n##  4     144     139\n##  5     142     143\n##  6     146     141\n##  7     149     143\n##  8     150     145\n##  9     142     136\n## 10     148     146\n# load the data\ndeer <- read_csv(\"data/examples/cs1-deer.csv\")\n\n# have a look\ndeer## # A tibble: 20 √ó 3\n##       id leg     length\n##    <dbl> <chr>    <dbl>\n##  1     1 hindleg    142\n##  2     2 hindleg    140\n##  3     3 hindleg    144\n##  4     4 hindleg    144\n##  5     5 hindleg    142\n##  6     6 hindleg    146\n##  7     7 hindleg    149\n##  8     8 hindleg    150\n##  9     9 hindleg    142\n## 10    10 hindleg    148\n## 11     1 foreleg    138\n## 12     2 foreleg    136\n## 13     3 foreleg    147\n## 14     4 foreleg    139\n## 15     5 foreleg    143\n## 16     6 foreleg    141\n## 17     7 foreleg    143\n## 18     8 foreleg    145\n## 19     9 foreleg    136\n## 20    10 foreleg    146\n# summarise the data\ndeer %>% \n  select(-id) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 1 √ó 10\n##   variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 length      20   136   150    143  5.25  143.  4.01 0.896  1.88\n# or even summarise by leg type\ndeer %>% \n  select(-id) %>% \n  group_by(leg) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 2 √ó 11\n##   leg     variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 foreleg length      10   136   147    142  6.25  141.  4.03  1.27  2.88\n## 2 hindleg length      10   140   150    144  5.5   145.  3.40  1.08  2.43\n# we can also visualise the data\ndeer %>% \n  ggplot(aes(x = leg, y = length)) +\n  geom_boxplot()\n# create a data set that contains the difference in leg length\nleg_diff <- deer %>% \n  pivot_wider(names_from = leg, values_from = length) %>% \n  mutate(leg_diff = hindleg - foreleg)\n# plot the difference in leg length\nleg_diff %>% \n  ggplot(aes(y = leg_diff)) +\n  geom_boxplot()\n# plot the data by observation\ndeer %>% \n  ggplot(aes(x = leg, y = length, group = id)) +\n  geom_point() +\n  geom_line()\n# perform Shapiro-Wilk test on leg differences\nleg_diff %>% \n  shapiro_test(leg_diff)## # A tibble: 1 √ó 3\n##   variable statistic      p\n##   <chr>        <dbl>  <dbl>\n## 1 leg_diff     0.814 0.0212\n# and create a Q-Q plot\nleg_diff %>% \n  ggplot(aes(sample = leg_diff)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\")"},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"hypotheses-4","chapter":"9 Wilcoxon signed-rank test","heading":"9.7.1 Hypotheses","text":"\\(H_0\\) : foreleg average (mean median) \\(=\\) hindleg average (mean median)\\(H_1\\) : foreleg average \\(\\neq\\) hindleg average","code":""},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"import-data-summarise-and-visualise","chapter":"9 Wilcoxon signed-rank test","heading":"9.7.2 Import data, summarise and visualise","text":"First , need get data tidy format (every variable column, observation row). Excel, adding ID gives us:ordering data important ; first hindleg row corresponds first foreleg row, second second . indicate use id column, observation unique ID.Let‚Äôs look data see can see.suggests might difference legs, hindlegs longer forelegs. However, representation obscures fact paired data. really need look difference leg length deer data observation:Additionally, can also plot data observation:gives us much clearer picture. looks though hindlegs 4 cm longer forelegs, average. also suggests leg differences might normally distributed (data look bit skewed boxplot).","code":"\n# load the data\ndeer <- read_csv(\"data/examples/cs1-deer.csv\")\n\n# have a look\ndeer## # A tibble: 20 √ó 3\n##       id leg     length\n##    <dbl> <chr>    <dbl>\n##  1     1 hindleg    142\n##  2     2 hindleg    140\n##  3     3 hindleg    144\n##  4     4 hindleg    144\n##  5     5 hindleg    142\n##  6     6 hindleg    146\n##  7     7 hindleg    149\n##  8     8 hindleg    150\n##  9     9 hindleg    142\n## 10    10 hindleg    148\n## 11     1 foreleg    138\n## 12     2 foreleg    136\n## 13     3 foreleg    147\n## 14     4 foreleg    139\n## 15     5 foreleg    143\n## 16     6 foreleg    141\n## 17     7 foreleg    143\n## 18     8 foreleg    145\n## 19     9 foreleg    136\n## 20    10 foreleg    146\n# summarise the data\ndeer %>% \n  select(-id) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 1 √ó 10\n##   variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 length      20   136   150    143  5.25  143.  4.01 0.896  1.88\n# or even summarise by leg type\ndeer %>% \n  select(-id) %>% \n  group_by(leg) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 2 √ó 11\n##   leg     variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 foreleg length      10   136   147    142  6.25  141.  4.03  1.27  2.88\n## 2 hindleg length      10   140   150    144  5.5   145.  3.40  1.08  2.43\n# we can also visualise the data\ndeer %>% \n  ggplot(aes(x = leg, y = length)) +\n  geom_boxplot()\n# create a data set that contains the difference in leg length\nleg_diff <- deer %>% \n  pivot_wider(names_from = leg, values_from = length) %>% \n  mutate(leg_diff = hindleg - foreleg)\n# plot the difference in leg length\nleg_diff %>% \n  ggplot(aes(y = leg_diff)) +\n  geom_boxplot()\n# plot the data by observation\ndeer %>% \n  ggplot(aes(x = leg, y = length, group = id)) +\n  geom_point() +\n  geom_line()"},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"assumptions-10","chapter":"9 Wilcoxon signed-rank test","heading":"9.7.3 Assumptions","text":"need consider distribution difference leg lengths rather individual distributions.Shapiro-Wilk test Q-Q plot suggest difference data aren‚Äôt normally distributed, rules paired t-test. therefore consider paired Wilcoxon test next. Remember test requires distribution differences symmetric, whereas box-plot suggested data much skewed.","code":"\n# perform Shapiro-Wilk test on leg differences\nleg_diff %>% \n  shapiro_test(leg_diff)## # A tibble: 1 √ó 3\n##   variable statistic      p\n##   <chr>        <dbl>  <dbl>\n## 1 leg_diff     0.814 0.0212\n# and create a Q-Q plot\nleg_diff %>% \n  ggplot(aes(sample = leg_diff)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\")"},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"conclusions","chapter":"9 Wilcoxon signed-rank test","heading":"9.7.4 Conclusions","text":", frustratingly, neither tests appropriate dataset. differences foreleg hindleg lengths neither normal enough paired t-test symmetric enough Wilcoxon test don‚Äôt enough data just use t-test (‚Äôd need 30 points ). situation? Well answer aren‚Äôt actually traditional statistical tests valid dataset stands!two options available someone:try transforming raw data (take logs, square root, reciprocals) hope one leads modified dataset satisfies assumptions one tests ‚Äôve covered, oruse permutation test approach (work beyond scope course).reason included example first practical purely illustrate simple dataset apparently clear message (leg lengths differ within deer) can intractable. don‚Äôt need complex datasets go beyond capabilities classical statistics.Jeremy Clarkson put :bombshell, ‚Äôs time end. Goodnight!","code":""},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"key-points-1","chapter":"9 Wilcoxon signed-rank test","heading":"9.8 Key points","text":"use two-sample tests see two samples continuous data come parent distributionThis essentially boils testing mean median differs two samplesThere 5 key two-sample tests: Student‚Äôs t-test, Welch‚Äôs t-test, Mann-Whitney U test, paired t-test Wilcoxon signed-rank testWhich one use depends normality distribution, sample size, paired unpaired data variance samplesParametric tests used data normally distributed sample size largeNon-parametric tests used data normally distributed sample size smallEquality variance determines test appropriateYou can ask 3 questions determine test:\ndata paired?\nneed parametric non-parametric test\ncan assume equality variance?\ndata paired?need parametric non-parametric testcan assume equality variance?","code":""},{},{"path":"cs2-intro.html","id":"cs2-intro","chapter":"10 Introduction","heading":"10 Introduction","text":"","code":""},{"path":"cs2-intro.html","id":"objectives-1","chapter":"10 Introduction","heading":"10.1 Objectives","text":"Aim: introduce R commands analysing single categorical predictors.end practical participants able perform following statistical analyses:One-way Analysis Variance (ANOVA)Kruskal-Wallis testFor , participants able :Perform test RInterpret outputCheck assumptions testCarry post-hoc test appropriateThe tests covered practical :One-way ANOVAKruskall-Wallis test","code":""},{"path":"cs2-intro.html","id":"background-1","chapter":"10 Introduction","heading":"10.2 Background","text":"practical focuses implementation various statistical tests relating categorical predictors. boil ANOVA Kruskal-Wallis (non-parametric alternative).\n, focus underlying theory tests (although demonstrators happy answer questions may ).test section :explains purpose test,explains visualise data,explains perform test R,explains interpret output report results, andexplains assess assumptions required perform test.","code":""},{},{"path":"introduction-1.html","id":"introduction-1","chapter":"11 Introduction","heading":"11 Introduction","text":"practical introducing can compare data different groups.","code":""},{"path":"introduction-1.html","id":"cs2-datasets","chapter":"11 Introduction","heading":"11.1 Datasets","text":"section uses various data sets. located data/raw/ folder working directory. Please see Datasets information.","code":""},{},{"path":"cs2-anova.html","id":"cs2-anova","chapter":"12 ANOVA","heading":"12 ANOVA","text":"","code":""},{"path":"cs2-anova.html","id":"objectives-2","chapter":"12 ANOVA","heading":"12.1 Objectives","text":"QuestionsHow analyse multiple samples continuous data?ANOVA?check differences groups?ObjectivesBe able perform ANOVA RUnderstand ANOVA output evaluate assumptionsUnderstand post-hoc testing R","code":""},{"path":"cs2-anova.html","id":"purpose-and-aim-2","chapter":"12 ANOVA","heading":"12.2 Purpose and aim","text":"Analysis variance ANOVA test can used multiple samples continuous data. Whilst possible use ANOVA two samples, generally used three groups. used find samples came parent distributions mean. can thought generalisation two-sample Student‚Äôs t-test.","code":""},{"path":"cs2-anova.html","id":"section-commands-6","chapter":"12 ANOVA","heading":"12.3 Section commands","text":"New commands used section.also use functionality new library, ggResidPanel:","code":"\n# load ggResidPanel, for ggplot-friendly diagnostics plots\nlibrary(ggResidpanel)"},{"path":"cs2-anova.html","id":"data-and-hypotheses-5","chapter":"12 ANOVA","heading":"12.4 Data and hypotheses","text":"example, suppose measure feeding rate oyster catchers (shellfish per hour) three sites characterised degree shelter wind, imaginatively called exposed (E), partially sheltered (P) sheltered (S). want test whether data support hypothesis feeding rates don‚Äôt differ locations. form following null alternative hypotheses:\\(H_0\\): mean feeding rates three sites \\(\\mu E = \\mu P = \\mu S\\)\\(H_1\\): mean feeding rates equal.use one-way ANOVA test check .use one-way ANOVA test one predictor variable (categorical variable location).‚Äôre using ANOVA two groups don‚Äôt know better yet respect exact assumptions.data stored file CS2-oystercatcher.csv.","code":""},{"path":"cs2-anova.html","id":"summarise-and-visualise-6","chapter":"12 ANOVA","heading":"12.5 Summarise and visualise","text":"First read data.oystercatcher data set contains three columns:unique ID column ida feeding column containing feeding ratesa site column information amount shelter feeding locationFirst, get basic descriptive statistics:Next, plot data site:Looking data, appears noticeable difference feeding rates three sites. probably expect reasonably significant statistical result .","code":"\n# load data\noystercatcher <- read_csv(\"data/tidy/CS2-oystercatcher.csv\")\n\n# and have a look\noystercatcher## # A tibble: 15 √ó 3\n##       id feeding site     \n##    <dbl>   <dbl> <chr>    \n##  1     1    14.2 Exposed  \n##  2     2    16.5 Exposed  \n##  3     3     9.3 Exposed  \n##  4     4    15.1 Exposed  \n##  5     5    13.4 Exposed  \n##  6     6    18.4 Partial  \n##  7     7    13   Partial  \n##  8     8    17.4 Partial  \n##  9     9    20.4 Partial  \n## 10    10    16.5 Partial  \n## 11    11    24.1 Sheltered\n## 12    12    22.2 Sheltered\n## 13    13    25.3 Sheltered\n## 14    14    25.1 Sheltered\n## 15    15    21.5 Sheltered\n# get some basic descriptive statistics\noystercatcher %>% \n  select(-id) %>% \n  group_by(site) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 3 √ó 11\n##   site      variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>     <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 Exposed   feeding      5   9.3  16.5   14.2   1.7  13.7  2.72 1.21   3.37\n## 2 Partial   feeding      5  13    20.4   17.4   1.9  17.1  2.73 1.22   3.39\n## 3 Sheltered feeding      5  21.5  25.3   24.1   2.9  23.6  1.71 0.767  2.13\n# plot the data\noystercatcher %>% \n  ggplot(aes(x = site, y = feeding)) +\n  geom_boxplot()"},{"path":"cs2-anova.html","id":"assumptions-11","chapter":"12 ANOVA","heading":"12.6 Assumptions","text":"use ANOVA test, make three assumptions:parent distributions samples taken normally distributedEach data point samples independent othersThe parent distributions varianceIn similar way two-sample tests consider normality equality variance assumptions using tests graphical inspection (ignore independence assumption).","code":""},{"path":"cs2-anova.html","id":"normality","chapter":"12 ANOVA","heading":"12.6.1 Normality","text":"First perform Shapiro-Wilk test site separately.can see three groups appear normally distributed good.ANOVA however, considering group turn often considered quite excessive , cases, sufficient consider normality combined set residuals data. ‚Äôll explain residuals properly next session effectively difference data point group mean. residuals can obtained directly linear model fitted data., create linear model, extract residuals check normality:, can see combined residuals three groups appear normally distributed (expected given normally distributed individually!)","code":"\n# Shapiro-Wilk test on each site\noystercatcher %>% \n  select(-id) %>% \n  group_by(site) %>% \n  shapiro_test(feeding)\n# define the model\nlm_oystercatcher <- lm(feeding ~ site,\n                       data = oystercatcher)\n\n# extract the residuals\nresid_oyster <- residuals(lm_oystercatcher)\n\n# perform Shapiro-Wilk test on residuals\nresid_oyster %>% \n  shapiro_test()## # A tibble: 1 √ó 3\n##   variable statistic p.value\n##   <chr>        <dbl>   <dbl>\n## 1 .            0.936   0.334"},{"path":"cs2-anova.html","id":"equality-of-variance-1","chapter":"12 ANOVA","heading":"12.6.2 Equality of Variance","text":"now test equality variance using Bartlett‚Äôs test (since ‚Äôve just found individual groups normally distributed).Perform Bartlett‚Äôs test data:relevant p-value given 3rd line. see group appear variance.","code":"\n# check equality of variance\nbartlett.test(feeding ~ site,\n              data = oystercatcher)## \n##  Bartlett test of homogeneity of variances\n## \n## data:  feeding by site\n## Bartlett's K-squared = 0.90632, df = 2, p-value = 0.6356"},{"path":"cs2-anova.html","id":"graphical-interpretation-and-diagnostic-plots","chapter":"12 ANOVA","heading":"12.6.3 Graphical interpretation and diagnostic Plots","text":"R provides convenient set graphs allow us assess assumptions graphically. simply ask R plot lm object created, can see diagnostic plots.variety ways can create diagnostic plots. right wrong - just preference.functionality base R really helpful, ‚Äôs easy create diagnostic plots built-functionality. example, can create set basic diagnostic plots using:first session already created diagnostic Q-Q plots directly data, using stat_qq() stat_qq_line(). specific plots becomes bit cumbersome. option create ggplot-friendly diagnostic plots, using ggResidPanel package. consistency tidyverse syntax used, use now , equally valid use base R functions.Create standard set diagnostic plots using ggResidPanel:Residual Plot: creates plot residuals versus predicted values. want points spread symmetrically around blue line.Q-Q Plot: creates normal quantile plot residualsIndex Plot: creates plot residuals versus observation numbers. solid blue horizontal line 0 included reference. plot can used look patterns dataHistogram: creates histogram residuals, using bins = 30 defaultThe default equivalent base R follows:second line creates three diagnostic plots (actually tries create four plots can‚Äôt dataset ‚Äôll also see warning text output screen (something hat values). ‚Äôll go next session ‚Äôs easier explain).example, two plots (top-left bottom-left) show effectively thing: distribution data group look like. allow informal check equality variance assumption.\ntop-left graph want data symmetric 0 horizontal line spread (please ignore red line; unhelpful addition graphs).\nbottom-left graph, look red line want approximately horizontal.\ntop-left graph want data symmetric 0 horizontal line spread (please ignore red line; unhelpful addition graphs).bottom-left graph, look red line want approximately horizontal.top-right graph familiar Q-Q plot used previously assess normality, looks combined residuals groups (much way looked Shapiro-Wilk test combined residuals).can see graphs much line ‚Äôve just looked using test, reassuring. groups appear spread data, whilst Q-Q plot isn‚Äôt perfect, appears assumption normality alright.stage, point nearly always stick graphical method assessing assumptions test. Assumptions rarely either completely met met always degree personal assessment.Whilst formal statistical tests (like Shapiro) technically fine, can often create false sense things absolutely right wrong spite fact still probabilistic statistical tests. exercises using approaches whilst gain confidence experience interpreting graphical output whilst absolutely fine use future strongly recommend don‚Äôt rely solely statistical tests isolation.","code":"# create a neat 2x2 window\npar(mfrow = c(2,2))\n# create the diagnostic plots\nplot(model_name)\n# and return the window back to normal\npar(mfrow = c(1,1))\nlm_oystercatcher %>% \n  resid_panel()\n# create a neat 2x2 window\npar(mfrow = c(2,2))\n# create the diagnostic plots\nplot(lm_oystercatcher)## hat values (leverages) are all = 0.2\n##  and there are no factor predictors; no plot no. 5\n# and return the window back to normal\npar(mfrow = c(1,1))"},{"path":"cs2-anova.html","id":"implement-test-5","chapter":"12 ANOVA","heading":"12.7 Implement test","text":"Perform ANOVA test data:fits linear model data (.e.¬†finds means three groups calculates load intermediary data need statistical analysis) stores information R object (‚Äôve called lm_oystercatchers, can call like). second line actually carries ANOVA analysis.anova() command takes linear model object main argument","code":"\nanova(lm_oystercatcher)"},{"path":"cs2-anova.html","id":"interpret-output-and-report-results-4","chapter":"12 ANOVA","heading":"12.8 Interpret output and report results","text":"output now see console window:1st line just tells ANOVA testThe 2nd line tells response variable (case feeding)3rd, 4th 5th lines ANOVA table contain useful values:\nDf column contains degrees freedom values row, 2 12 (‚Äôll need reporting)\nF value column contains F statistic, 21.508 (‚Äôll need reporting).\np-value 0.0001077 number directly Pr(>F) 4th line.\nvalues table (Sum Sq Mean Sq) columns used calculate F statistic don‚Äôt need know .\nDf column contains degrees freedom values row, 2 12 (‚Äôll need reporting)F value column contains F statistic, 21.508 (‚Äôll need reporting).p-value 0.0001077 number directly Pr(>F) 4th line.values table (Sum Sq Mean Sq) columns used calculate F statistic don‚Äôt need know .6th line symbolic codes represent big (small) p-value ; , p-value smaller 0.001 *** symbol next (). Whereas p-value 0.01 0.05 simply * character next , etc. Thankfully can cope actual numbers don‚Äôt need short-hand code determine reporting experiments (please tell ‚Äôs true‚Ä¶!), p-value ‚Äôre interested shows us probability getting samples null hypothesis actually true.Since p-value small (much smaller standard significance level 0.05) can say ‚Äúunlikely three samples came parent distribution‚Äù can reject null hypothesis state :one-way ANOVA showed mean feeding rate oystercatchers differed significantly locations (F = 21.51, df = 2, 12, p = 0.00011).Note included (brackets) information test statistic (F = 21.51), degrees freedom (df = 2, 12), p-value (p = 0.00011).","code":"## Analysis of Variance Table\n## \n## Response: feeding\n##           Df  Sum Sq Mean Sq F value    Pr(>F)    \n## site       2 254.812 127.406  21.508 0.0001077 ***\n## Residuals 12  71.084   5.924                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"cs2-anova.html","id":"post-hoc-testing-tukeys-rank-test","chapter":"12 ANOVA","heading":"12.9 Post-hoc testing (Tukey‚Äôs rank test)","text":"One drawback using ANOVA test tests see means , get significant result using ANOVA can say means , rather anything pairs groups differ. example, consider following boxplot three samples.group random sample 20 points normal distribution variance 1. Groups 1 2 come parent population mean 0 whereas group 3 come parent population mean 2. data clearly satisfy assumptions ANOVA test.","code":""},{"path":"cs2-anova.html","id":"read-in-the-data-and-plot","chapter":"12 ANOVA","heading":"12.9.1 Read in the data and plot","text":"","code":"\n# load the data\ntukey <- read_csv(\"data/tidy/CS2-tukey.csv\")\n\n# have a look at the data\ntukey## # A tibble: 60 √ó 3\n##       id response group  \n##    <dbl>    <dbl> <chr>  \n##  1     1    1.58  sample1\n##  2     2    0.380 sample1\n##  3     3   -0.997 sample1\n##  4     4   -0.771 sample1\n##  5     5    0.169 sample1\n##  6     6   -0.698 sample1\n##  7     7   -0.167 sample1\n##  8     8    1.38  sample1\n##  9     9   -0.839 sample1\n## 10    10   -1.05  sample1\n## # ‚Ä¶ with 50 more rows\n# plot the data\ntukey %>% \n  ggplot(aes(x = group, y = response)) +\n  geom_boxplot()"},{"path":"cs2-anova.html","id":"test-for-a-significant-difference-in-group-means","chapter":"12 ANOVA","heading":"12.9.2 Test for a significant difference in group means","text":"p-value 2.39 \\(\\cdot\\) 10-7 test conclusively rejected hypothesis means equal.However, due sample means different, rather just one groups different others. order drill investigate use new test called Tukey‚Äôs range test (Tukey‚Äôs honest significant difference test ‚Äì always makes think terrible cowboy/western dialogue). compare groups pairwise fashion reports whether significant difference exists.","code":"\n# create a linear model\nlm_tukey <- lm(response ~ group,\n               data = tukey)\n\n# perform an ANOVA\nanova(lm_tukey)## Analysis of Variance Table\n## \n## Response: response\n##           Df Sum Sq Mean Sq F value    Pr(>F)    \n## group      2 33.850 16.9250   20.16 2.392e-07 ***\n## Residuals 57 47.854  0.8395                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"cs2-anova.html","id":"performing-tukeys-test","chapter":"12 ANOVA","heading":"12.9.3 Performing Tukey‚Äôs test","text":"tukey_hsd() function takes linear model (lm_tukey) input. output pair--pair comparison different groups (samples 1 3). interested p.adj column, gives us adjusted p-value. null hypothesis case difference mean two groups. can see first row shows isn‚Äôt significant difference sample1 sample2 2nd 3rd rows show significant difference sample1 sample3, well sample2 sample3. matches expected based boxplot.","code":"\n# perform Tukey's range test on linear model\nlm_tukey %>% \n  tukey_hsd()## # A tibble: 3 √ó 9\n##   term  group1  group2  null.value estimate conf.low conf.high       p.adj\n## * <chr> <chr>   <chr>        <dbl>    <dbl>    <dbl>     <dbl>       <dbl>\n## 1 group sample1 sample2          0    0.304   -0.393      1.00 0.55       \n## 2 group sample1 sample3          0    1.72     1.03       2.42 0.000000522\n## 3 group sample2 sample3          0    1.42     0.722      2.12 0.0000246  \n## # ‚Ä¶ with 1 more variable: p.adj.signif <chr>"},{"path":"cs2-anova.html","id":"assumptions-12","chapter":"12 ANOVA","heading":"12.9.4 Assumptions","text":"use Tukey‚Äôs range test matter debate (strangely enough lot statistical analysis techniques currently matters opinion rather mathematical fact ‚Äì explain little whole field appears bloody confusing!)people claim perform Tukey‚Äôs range test (post-hoc tests) preceding ANOVA test showed significant difference groups ANOVA test shown significant differences groups stop .people say rubbish can hell like, like long tell people .background rather involved one reasons debate prevent -called data-dredging p-hacking. scientists/analysts fixated getting ‚Äúsignificant‚Äù result perform huge variety statistical techniques find one shows data significant (particular problem psychological studies ‚Äì point fingers though, working hard sort stuff . Kudos!).Whether use post-hoc testing depend experimental design questions ‚Äôre attempting answer.Tukey‚Äôs range test, decide use , requires three assumptions ANOVA test:Normality distributionsEquality variance groupsIndependence observations","code":""},{"path":"cs2-anova.html","id":"exercise-lobster-weight","chapter":"12 ANOVA","heading":"12.10 Exercise: Lobster weight","text":"Exercise 12.1  Juvenile lobster weightJuvenile lobsters aquaculture grown three different diets (fresh mussels, semi-dry pellets dry flakes). nine weeks, wet weight :evidence diet affects growth rate lobsters?Write null alternative hypothesesImport data R\ndata stored tidy format data/tidy/CS2-lobsters.csv\ndata stored tidy format data/tidy/CS2-lobsters.csvSummarise visualise dataCheck assumptions using appropriate tests graphical analysesPerform ANOVA testWrite sentence summarise results foundPerform post-hoc test report findings\\(H_0\\) : means equal\\(H_1\\) : means equalThe data stored .csv file columns called id, weight diet.Next, visualise data:always use plot summary assess three things:load data properly?see three groups reasonable values. aren‚Äôt data points obviously wrong (negative, zero massively big) right number groups. looks didn‚Äôt anything obviously wrong.expect result statistical test?Whilst Mussels group look higher two groups, Pellets Flakes appear almost identical terms average values, ‚Äôs quite bit overlap Mussels group. non-significant result likely answer, surprised see significant p-value - especially given small sample size .think assumptions?groups appear mainly symmetric (although Pellets bit weird) ‚Äôre immediately massively worried lack normality. , Flakes Mussels appear similar variances ‚Äôs bit hard decide ‚Äôs going Pellets. ‚Äôs hard say ‚Äôs going assumptions ‚Äôll wait see tests say.NormalityWe‚Äôll really thorough consider normality group separately jointly using Shapiro-Wilk test, well looking Q-Q plot. reality, examples , ‚Äôll use Q-Q plot.First, perform Shapiro-Wilk test individual groups:Flakes Mussels fine, , suspected earlier, Pellets appears marginally significant Normality test result.Let‚Äôs look Shapiro-Wilk test data together:hand says everything fine. Let‚Äôs look Q-Q plot., ‚Äôve used extra argument normal diagnostic plots call. default option plot 4 diagnostic plots. can tell resid_panel() plot specific one, using plots = arguments. want know look help documentation using ?resid_panel.Q-Q plot looks OK, perfect, good enough us confidence normality data.Overall, ‚Äôd happy assumption normality adequately well met . suggested lack normality Pellets just significant take account 5 data points group. lot points group, Q-Q plot considerably worse wouldn‚Äôt confident.Equality VarianceWe‚Äôll consider Bartlett test ‚Äôll look diagnostic plots .code ‚Äôve specified diagnostic plots wanted. also added loess smoother line (smoother = TRUE) plotsThe Residuals Plot. ‚Äôre looking points evenly spread either side line. Looks good.Location-Scale Plot (displayed default base R‚Äôs diagnostic plots). ‚Äôre looking red line. line less horizontal, equality variance assumption met.three methods agree isn‚Äôt issues equality variance:Bartlett test p-value large non-significantthe spread points three groups residuals vs fitted graph roughly samethe red line scale-location graph pretty horizontalOverall, assumption pretty well met.assumptions normality equality variance met can confident one-way ANOVA appropriate test.one-way ANOVA test indicated mean weight juvenile lobsters differ significantly diets (F = 1.64, df = 2,15, p = 0.23).can see actually, significant difference pairs groups dataset.want reiterate carrying post-hoc test getting non-significant result ANOVA something think carefully depends research question .research question :diet affect lobster weight? effect diet lobster weight?got non-significant result ANOVA test just stopped answer. Going digging ‚Äúsignificant‚Äù results running tests main factor contributes towards lack reproducibility research.hand research question :specific diets better worse lobster weight others?probably just skipped one-way ANOVA test entirely just jumped straight Tukey‚Äôs range test, important point result one-way ANOVA test doesn‚Äôt preclude carrying Tukey test.","code":"\n# load the data\nlobsters <- read_csv(\"data/tidy/CS2-lobsters.csv\")\n\n# look at the data\nlobsters## # A tibble: 18 √ó 3\n##       id weight diet   \n##    <dbl>  <dbl> <chr>  \n##  1     1  152.  Mussels\n##  2     2  132.  Mussels\n##  3     3  104.  Mussels\n##  4     4  154.  Mussels\n##  5     5  132   Mussels\n##  6     6  119   Mussels\n##  7     7  162.  Mussels\n##  8     8  118.  Pellets\n##  9     9  111.  Pellets\n## 10    10  129.  Pellets\n## 11    11  110.  Pellets\n## 12    12  175.  Pellets\n## 13    13  102.  Flakes \n## 14    14  103.  Flakes \n## 15    15   90.4 Flakes \n## 16    16  133.  Flakes \n## 17    17  129.  Flakes \n## 18    18  129.  Flakes\n# create some summary statistics\nlobsters %>% \n  select(-id) %>% \n  group_by(diet) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 3 √ó 11\n##   diet    variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 Flakes  weight       6  90.4  133.   116.  27.3  114.  18.2  7.42  19.1\n## 2 Mussels weight       7 104.   162.   132.  27.0  136.  20.6  7.79  19.1\n## 3 Pellets weight       5 110.   175.   118.  17.8  128.  27.2 12.1   33.7\nlobsters %>% \n  ggplot(aes(x = diet, y = weight)) +\n  geom_boxplot()\n# Shapiro-Wilk on lobster groups\nlobsters %>% \n  group_by(diet) %>% \n  shapiro_test(weight)## # A tibble: 3 √ó 4\n##   diet    variable statistic      p\n##   <chr>   <chr>        <dbl>  <dbl>\n## 1 Flakes  weight       0.844 0.140 \n## 2 Mussels weight       0.948 0.710 \n## 3 Pellets weight       0.767 0.0425\n# create a linear model\nlm_lobsters <- lm(weight ~ diet,\n                  data = lobsters)\n\n# extract the residuals\nresid_lobsters <- residuals(lm_lobsters)\n\n# and perform the Shapiro-Wilk test on the residuals\nresid_lobsters %>% \n  shapiro_test()## # A tibble: 1 √ó 3\n##   variable statistic p.value\n##   <chr>        <dbl>   <dbl>\n## 1 .            0.948   0.391\n# Q-Q plots\nlm_lobsters %>% \n  resid_panel(plots = \"qq\")\n# perform Bartlett's test\nbartlett.test(weight ~ diet,\n              data = lobsters)## \n##  Bartlett test of homogeneity of variances\n## \n## data:  weight by diet\n## Bartlett's K-squared = 0.71273, df = 2, p-value = 0.7002\n# plot the residuals and scale-location plots\nlm_lobsters %>% \n  resid_panel(plots = c(\"resid\", \"ls\"),\n              smoother = TRUE)\nanova(lm_lobsters)## Analysis of Variance Table\n## \n## Response: weight\n##           Df Sum Sq Mean Sq F value Pr(>F)\n## diet       2 1567.2  783.61  1.6432 0.2263\n## Residuals 15 7153.1  476.87\nlm_lobsters %>% \n  tukey_hsd()## # A tibble: 3 √ó 9\n##   term  group1  group2  null.value estimate conf.low conf.high p.adj p.adj.signif\n## * <chr> <chr>   <chr>        <dbl>    <dbl>    <dbl>     <dbl> <dbl> <chr>       \n## 1 diet  Flakes  Mussels          0    21.9     -9.66      53.5 0.202 ns          \n## 2 diet  Flakes  Pellets          0    14.0    -20.3       48.4 0.551 ns          \n## 3 diet  Mussels Pellets          0    -7.85   -41.1       25.4 0.815 ns"},{"path":"cs2-anova.html","id":"hypotheses-5","chapter":"12 ANOVA","heading":"12.10.1 Hypotheses","text":"\\(H_0\\) : means equal\\(H_1\\) : means equal","code":""},{"path":"cs2-anova.html","id":"import-data-summarise-and-visualise-1","chapter":"12 ANOVA","heading":"12.10.2 Import Data, summarise and visualise","text":"data stored .csv file columns called id, weight diet.Next, visualise data:always use plot summary assess three things:load data properly?see three groups reasonable values. aren‚Äôt data points obviously wrong (negative, zero massively big) right number groups. looks didn‚Äôt anything obviously wrong.expect result statistical test?Whilst Mussels group look higher two groups, Pellets Flakes appear almost identical terms average values, ‚Äôs quite bit overlap Mussels group. non-significant result likely answer, surprised see significant p-value - especially given small sample size .think assumptions?groups appear mainly symmetric (although Pellets bit weird) ‚Äôre immediately massively worried lack normality. , Flakes Mussels appear similar variances ‚Äôs bit hard decide ‚Äôs going Pellets. ‚Äôs hard say ‚Äôs going assumptions ‚Äôll wait see tests say.","code":"\n# load the data\nlobsters <- read_csv(\"data/tidy/CS2-lobsters.csv\")\n\n# look at the data\nlobsters## # A tibble: 18 √ó 3\n##       id weight diet   \n##    <dbl>  <dbl> <chr>  \n##  1     1  152.  Mussels\n##  2     2  132.  Mussels\n##  3     3  104.  Mussels\n##  4     4  154.  Mussels\n##  5     5  132   Mussels\n##  6     6  119   Mussels\n##  7     7  162.  Mussels\n##  8     8  118.  Pellets\n##  9     9  111.  Pellets\n## 10    10  129.  Pellets\n## 11    11  110.  Pellets\n## 12    12  175.  Pellets\n## 13    13  102.  Flakes \n## 14    14  103.  Flakes \n## 15    15   90.4 Flakes \n## 16    16  133.  Flakes \n## 17    17  129.  Flakes \n## 18    18  129.  Flakes\n# create some summary statistics\nlobsters %>% \n  select(-id) %>% \n  group_by(diet) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 3 √ó 11\n##   diet    variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 Flakes  weight       6  90.4  133.   116.  27.3  114.  18.2  7.42  19.1\n## 2 Mussels weight       7 104.   162.   132.  27.0  136.  20.6  7.79  19.1\n## 3 Pellets weight       5 110.   175.   118.  17.8  128.  27.2 12.1   33.7\nlobsters %>% \n  ggplot(aes(x = diet, y = weight)) +\n  geom_boxplot()"},{"path":"cs2-anova.html","id":"explore-assumptions","chapter":"12 ANOVA","heading":"12.10.3 Explore Assumptions","text":"NormalityWe‚Äôll really thorough consider normality group separately jointly using Shapiro-Wilk test, well looking Q-Q plot. reality, examples , ‚Äôll use Q-Q plot.First, perform Shapiro-Wilk test individual groups:Flakes Mussels fine, , suspected earlier, Pellets appears marginally significant Normality test result.Let‚Äôs look Shapiro-Wilk test data together:hand says everything fine. Let‚Äôs look Q-Q plot., ‚Äôve used extra argument normal diagnostic plots call. default option plot 4 diagnostic plots. can tell resid_panel() plot specific one, using plots = arguments. want know look help documentation using ?resid_panel.Q-Q plot looks OK, perfect, good enough us confidence normality data.Overall, ‚Äôd happy assumption normality adequately well met . suggested lack normality Pellets just significant take account 5 data points group. lot points group, Q-Q plot considerably worse wouldn‚Äôt confident.Equality VarianceWe‚Äôll consider Bartlett test ‚Äôll look diagnostic plots .code ‚Äôve specified diagnostic plots wanted. also added loess smoother line (smoother = TRUE) plotsThe Residuals Plot. ‚Äôre looking points evenly spread either side line. Looks good.Location-Scale Plot (displayed default base R‚Äôs diagnostic plots). ‚Äôre looking red line. line less horizontal, equality variance assumption met.three methods agree isn‚Äôt issues equality variance:Bartlett test p-value large non-significantthe spread points three groups residuals vs fitted graph roughly samethe red line scale-location graph pretty horizontalOverall, assumption pretty well met.","code":"\n# Shapiro-Wilk on lobster groups\nlobsters %>% \n  group_by(diet) %>% \n  shapiro_test(weight)## # A tibble: 3 √ó 4\n##   diet    variable statistic      p\n##   <chr>   <chr>        <dbl>  <dbl>\n## 1 Flakes  weight       0.844 0.140 \n## 2 Mussels weight       0.948 0.710 \n## 3 Pellets weight       0.767 0.0425\n# create a linear model\nlm_lobsters <- lm(weight ~ diet,\n                  data = lobsters)\n\n# extract the residuals\nresid_lobsters <- residuals(lm_lobsters)\n\n# and perform the Shapiro-Wilk test on the residuals\nresid_lobsters %>% \n  shapiro_test()## # A tibble: 1 √ó 3\n##   variable statistic p.value\n##   <chr>        <dbl>   <dbl>\n## 1 .            0.948   0.391\n# Q-Q plots\nlm_lobsters %>% \n  resid_panel(plots = \"qq\")\n# perform Bartlett's test\nbartlett.test(weight ~ diet,\n              data = lobsters)## \n##  Bartlett test of homogeneity of variances\n## \n## data:  weight by diet\n## Bartlett's K-squared = 0.71273, df = 2, p-value = 0.7002\n# plot the residuals and scale-location plots\nlm_lobsters %>% \n  resid_panel(plots = c(\"resid\", \"ls\"),\n              smoother = TRUE)"},{"path":"cs2-anova.html","id":"carry-out-one-way-anova","chapter":"12 ANOVA","heading":"12.10.4 Carry out one-way ANOVA","text":"assumptions normality equality variance met can confident one-way ANOVA appropriate test.","code":"\nanova(lm_lobsters)## Analysis of Variance Table\n## \n## Response: weight\n##           Df Sum Sq Mean Sq F value Pr(>F)\n## diet       2 1567.2  783.61  1.6432 0.2263\n## Residuals 15 7153.1  476.87"},{"path":"cs2-anova.html","id":"result","chapter":"12 ANOVA","heading":"12.10.5 Result","text":"one-way ANOVA test indicated mean weight juvenile lobsters differ significantly diets (F = 1.64, df = 2,15, p = 0.23).","code":""},{"path":"cs2-anova.html","id":"post-hoc-testing-with-tukey","chapter":"12 ANOVA","heading":"12.10.6 Post-hoc testing with Tukey","text":"can see actually, significant difference pairs groups dataset.want reiterate carrying post-hoc test getting non-significant result ANOVA something think carefully depends research question .research question :diet affect lobster weight? effect diet lobster weight?got non-significant result ANOVA test just stopped answer. Going digging ‚Äúsignificant‚Äù results running tests main factor contributes towards lack reproducibility research.hand research question :specific diets better worse lobster weight others?probably just skipped one-way ANOVA test entirely just jumped straight Tukey‚Äôs range test, important point result one-way ANOVA test doesn‚Äôt preclude carrying Tukey test.","code":"\nlm_lobsters %>% \n  tukey_hsd()## # A tibble: 3 √ó 9\n##   term  group1  group2  null.value estimate conf.low conf.high p.adj p.adj.signif\n## * <chr> <chr>   <chr>        <dbl>    <dbl>    <dbl>     <dbl> <dbl> <chr>       \n## 1 diet  Flakes  Mussels          0    21.9     -9.66      53.5 0.202 ns          \n## 2 diet  Flakes  Pellets          0    14.0    -20.3       48.4 0.551 ns          \n## 3 diet  Mussels Pellets          0    -7.85   -41.1       25.4 0.815 ns"},{"path":"cs2-anova.html","id":"key-points-2","chapter":"12 ANOVA","heading":"12.11 Key points","text":"use ANOVA test difference means multiple continuous variablesIn R first define linear model lm(), using format response ~ predictorNext, perform ANOVA linear model anova()check assumptions diagnostic plots check residuals normally distributedWe use post-hoc testing check significant differences group means, example using Tukey‚Äôs range test","code":""},{},{"path":"kruskal-wallis-test.html","id":"kruskal-wallis-test","chapter":"13 Kruskal-Wallis test","heading":"13 Kruskal-Wallis test","text":"","code":""},{"path":"kruskal-wallis-test.html","id":"objectives-3","chapter":"13 Kruskal-Wallis test","heading":"13.1 Objectives","text":"QuestionsHow analyse multiple samples continuous data data normally distributed?Kruskal-Wallis test?check differences groups?ObjectivesBe able perform Kruskal-Wallis test RUnderstand output test evaluate assumptionsBe able perform post-hoc testing Kruskal-Wallis test","code":""},{"path":"kruskal-wallis-test.html","id":"purpose-and-aim-3","chapter":"13 Kruskal-Wallis test","heading":"13.2 Purpose and aim","text":"Kruskal-Wallis one-way analysis variance test analogue ANOVA can used assumption normality met. way extension Mann-Whitney test two groups.","code":""},{"path":"kruskal-wallis-test.html","id":"section-commands-7","chapter":"13 Kruskal-Wallis test","heading":"13.3 Section commands","text":"New commands used section:","code":""},{"path":"kruskal-wallis-test.html","id":"data-and-hypotheses-6","chapter":"13 Kruskal-Wallis test","heading":"13.4 Data and hypotheses","text":"example, suppose behavioural ecologist records rate spider monkeys behaved aggressively towards one another function closely related two monkeys . familiarity two monkeys involved interaction classified high, low none. want test data support hypothesis aggression rates differ according strength relatedness. form following null alternative hypotheses:\\(H_0\\): median aggression rates types familiarity \\(H_1\\): median aggression rates equalWe use Kruskal-Wallis test check .data stored file data/raw/CS2-spidermonkey.csv.First read data :","code":"\nspidermonkey <- read_csv(\"data/tidy/CS2-spidermonkey.csv\")"},{"path":"kruskal-wallis-test.html","id":"summarise-and-visualise-7","chapter":"13 Kruskal-Wallis test","heading":"13.5 Summarise and visualise","text":"data appear show significant difference aggression rates three types familiarity. probably expect reasonably significant result .","code":"\n# look at the data\nspidermonkey## # A tibble: 21 √ó 3\n##       id aggression familiarity\n##    <dbl>      <dbl> <chr>      \n##  1     1        0.2 high       \n##  2     2        0.1 high       \n##  3     3        0.4 high       \n##  4     4        0.8 high       \n##  5     5        0.3 high       \n##  6     6        0.5 high       \n##  7     7        0.2 high       \n##  8     8        0.5 low        \n##  9     9        0.4 low        \n## 10    10        0.3 low        \n## # ‚Ä¶ with 11 more rows\n# summarise the data\nspidermonkey %>% \n  select(-id) %>% \n  group_by(familiarity) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 3 √ó 11\n##   familiarity variable       n   min   max median   iqr  mean    sd    se    ci\n##   <chr>       <chr>      <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 high        aggression     7   0.1   0.8    0.3  0.25 0.357 0.237 0.09  0.219\n## 2 low         aggression     7   0.3   1.2    0.5  0.3  0.629 0.315 0.119 0.291\n## 3 none        aggression     7   0.9   1.6    1.2  0.25 1.26  0.23  0.087 0.213\n# create boxplot\nspidermonkey %>% \n  ggplot(aes(x = familiarity, y = aggression)) +\n  geom_boxplot()"},{"path":"kruskal-wallis-test.html","id":"assumptions-13","chapter":"13 Kruskal-Wallis test","heading":"13.6 Assumptions","text":"use Kruskal-Wallis test make three assumptions:parent distributions samples drawn shape (‚Äôre normal use one-way ANOVA)data point samples independent othersThe parent distributions varianceIndependence ‚Äôll ignore usual. Similar shape best assessed earlier visualisation data. means need check equality variance.","code":""},{"path":"kruskal-wallis-test.html","id":"equality-of-variance-2","chapter":"13 Kruskal-Wallis test","heading":"13.6.1 Equality of variance","text":"test equality variance using Levene‚Äôs test (since can‚Äôt assume normal parent distributions rules Bartlett‚Äôs test).relevant p-value given p column (0.893). quite large see group appear variance.also warning group coerced factor. need worry - Levene‚Äôs test needs compare different groups aggression encoded numeric value, converts categorical one running test.","code":"\n# perform Levene's test\nspidermonkey %>% \n  levene_test(aggression ~ familiarity)## Warning in leveneTest.default(y = y, group = group, ...): group coerced to\n## factor.## # A tibble: 1 √ó 4\n##     df1   df2 statistic     p\n##   <int> <int>     <dbl> <dbl>\n## 1     2    18     0.114 0.893"},{"path":"kruskal-wallis-test.html","id":"implement-test-6","chapter":"13 Kruskal-Wallis test","heading":"13.7 Implement test","text":"Perform Kruskal-Wallis test data:kruskal_test() takes formula following format: variable ~ category","code":"\n# implement Kruskal-Wallis test\nspidermonkey %>% \n  kruskal_test(aggression ~ familiarity)\n\nkruskal.test(aggression ~ familiarity, data = spidermonkey)"},{"path":"kruskal-wallis-test.html","id":"interpret-output-and-report-results-5","chapter":"13 Kruskal-Wallis test","heading":"13.8 Interpret output and report results","text":"output now see console window:p-value given p column. shows us probability getting samples null hypothesis actually true.Since p-value small (much smaller standard significance level 0.05) can say ‚Äúunlikely three samples came parent distribution can reject null hypothesis‚Äù state :one-way Kruskal-Wallis rank sum test showed aggression rates spidermonkeys depends upon degree familiarity (KW = 13.597, df = 2, p = 0.0011).","code":"## # A tibble: 1 √ó 6\n##   .y.            n statistic    df       p method        \n## * <chr>      <int>     <dbl> <int>   <dbl> <chr>         \n## 1 aggression    21      13.6     2 0.00112 Kruskal-Wallis## \n##  Kruskal-Wallis rank sum test\n## \n## data:  aggression by familiarity\n## Kruskal-Wallis chi-squared = 13.597, df = 2, p-value = 0.001115"},{"path":"kruskal-wallis-test.html","id":"post-hoc-testing-dunns-test","chapter":"13 Kruskal-Wallis test","heading":"13.9 Post-hoc testing (Dunn‚Äôs test)","text":"equivalent Tukey‚Äôs range test non-normal data Dunn‚Äôs test.Dunn‚Äôs test used check significant differences group medians:give following output:dunn_test() function performs Kruskal-Wallis test data, followed post-hoc pairwise multiple comparison.comparison pairs groups reported table bottom. row contains single comparison. interested p p.adj columns, contain p-values want. table shows isn‚Äôt significant difference high low groups, p-value (0.1598) high. two comparisons high familiarity familiarity groups low groups significant though.dunn_test() function several arguments, p.adjust.method likely interest. can define method needs used account multiple comparisons. default \"holm\". ‚Äôll cover chapter Power analysis.","code":"\n# perform Dunn's test\nspidermonkey %>% \n  dunn_test(aggression ~ familiarity)## # A tibble: 3 √ó 9\n##   .y.        group1 group2    n1    n2 statistic        p    p.adj p.adj.signif\n## * <chr>      <chr>  <chr>  <int> <int>     <dbl>    <dbl>    <dbl> <chr>       \n## 1 aggression high   low        7     7      1.41 0.160    0.160    ns          \n## 2 aggression high   none       7     7      3.66 0.000257 0.000771 ***         \n## 3 aggression low    none       7     7      2.25 0.0245   0.0490   *"},{"path":"kruskal-wallis-test.html","id":"exercise-lobster-weight-1","chapter":"13 Kruskal-Wallis test","heading":"13.10 Exercise: Lobster weight","text":"Exercise 13.1  Kruskal-Wallis Dunn‚Äôs test lobster dataPerform Kruskal-Wallis test post-hoc test lobster data set.\\(H_0\\) : medians equal\\(H_1\\) : medians equalAll done previously., since data normal enough definitely similar enough Kruskal-Wallis test equality variance assessment diagnostic plots. completeness though look Levene‚Äôs testGiven p-value high, agrees previous assessment equality variance assumption well met. Rock .Kruskal-Wallis test indicated median weight juvenile lobsters differ significantly diets (KW = 3.26, df = 2, p = 0.20).Although rather unnecessary (likely unwanted, since don‚Äôt want p-hacking), detect significant differences diets, can perform non-parametric equivalent Tukey‚Äôs range test: Dunn‚Äôs test.can see none comparisons significant, either based uncorrected p-values (p) p-values adjusted multiple comparisons (p.adj). consistent found previously.","code":"\nlobsters %>% \n  levene_test(weight ~ diet)## # A tibble: 1 √ó 4\n##     df1   df2 statistic     p\n##   <int> <int>     <dbl> <dbl>\n## 1     2    15   0.00280 0.997\n# implement Kruskal-Wallis test\nlobsters %>% \n  kruskal_test(weight ~ diet)## # A tibble: 1 √ó 6\n##   .y.        n statistic    df     p method        \n## * <chr>  <int>     <dbl> <int> <dbl> <chr>         \n## 1 weight    18      3.26     2 0.196 Kruskal-Wallis\n# perform Dunn's test\nlobsters %>% \n  dunn_test(weight ~ diet)## # A tibble: 3 √ó 9\n##   .y.    group1  group2     n1    n2 statistic      p p.adj p.adj.signif\n## * <chr>  <chr>   <chr>   <int> <int>     <dbl>  <dbl> <dbl> <chr>       \n## 1 weight Flakes  Mussels     6     7     1.79  0.0738 0.221 ns          \n## 2 weight Flakes  Pellets     6     5     0.670 0.503  0.629 ns          \n## 3 weight Mussels Pellets     7     5    -1.01  0.315  0.629 ns"},{"path":"kruskal-wallis-test.html","id":"hypotheses-6","chapter":"13 Kruskal-Wallis test","heading":"13.10.1 Hypotheses","text":"\\(H_0\\) : medians equal\\(H_1\\) : medians equal","code":""},{"path":"kruskal-wallis-test.html","id":"import-data-summarise-and-visualise-2","chapter":"13 Kruskal-Wallis test","heading":"13.10.2 Import data, summarise and visualise","text":"done previously.","code":""},{"path":"kruskal-wallis-test.html","id":"assumptions-14","chapter":"13 Kruskal-Wallis test","heading":"13.10.3 Assumptions","text":", since data normal enough definitely similar enough Kruskal-Wallis test equality variance assessment diagnostic plots. completeness though look Levene‚Äôs testGiven p-value high, agrees previous assessment equality variance assumption well met. Rock .","code":"\nlobsters %>% \n  levene_test(weight ~ diet)## # A tibble: 1 √ó 4\n##     df1   df2 statistic     p\n##   <int> <int>     <dbl> <dbl>\n## 1     2    15   0.00280 0.997"},{"path":"kruskal-wallis-test.html","id":"kruskal-wallis-test-1","chapter":"13 Kruskal-Wallis test","heading":"13.10.4 Kruskal-Wallis test","text":"Kruskal-Wallis test indicated median weight juvenile lobsters differ significantly diets (KW = 3.26, df = 2, p = 0.20).","code":"\n# implement Kruskal-Wallis test\nlobsters %>% \n  kruskal_test(weight ~ diet)## # A tibble: 1 √ó 6\n##   .y.        n statistic    df     p method        \n## * <chr>  <int>     <dbl> <int> <dbl> <chr>         \n## 1 weight    18      3.26     2 0.196 Kruskal-Wallis"},{"path":"kruskal-wallis-test.html","id":"post-hoc-dunns-test","chapter":"13 Kruskal-Wallis test","heading":"13.10.5 Post-hoc Dunn‚Äôs test","text":"Although rather unnecessary (likely unwanted, since don‚Äôt want p-hacking), detect significant differences diets, can perform non-parametric equivalent Tukey‚Äôs range test: Dunn‚Äôs test.can see none comparisons significant, either based uncorrected p-values (p) p-values adjusted multiple comparisons (p.adj). consistent found previously.","code":"\n# perform Dunn's test\nlobsters %>% \n  dunn_test(weight ~ diet)## # A tibble: 3 √ó 9\n##   .y.    group1  group2     n1    n2 statistic      p p.adj p.adj.signif\n## * <chr>  <chr>   <chr>   <int> <int>     <dbl>  <dbl> <dbl> <chr>       \n## 1 weight Flakes  Mussels     6     7     1.79  0.0738 0.221 ns          \n## 2 weight Flakes  Pellets     6     5     0.670 0.503  0.629 ns          \n## 3 weight Mussels Pellets     7     5    -1.01  0.315  0.629 ns"},{"path":"kruskal-wallis-test.html","id":"key-points-3","chapter":"13 Kruskal-Wallis test","heading":"13.11 Key points","text":"use Kruskal-Wallis test see difference medians multiple continuous variablesIn R first define linear model lm(), using format response ~ predictorNext, perform Kruskal-Wallis test linear model kruskal_test()assume parent distributions shape; data point independent parent distributions varianceWe test equality variance using levene_test()Post-hoc testing check significant differences group medians done dunn_test()","code":""},{},{"path":"cs3-intro.html","id":"cs3-intro","chapter":"14 Introduction","heading":"14 Introduction","text":"","code":""},{"path":"cs3-intro.html","id":"objectives-4","chapter":"14 Introduction","heading":"14.1 Objectives","text":"Aim: introduce R commands analysing simple linear modelsBy end practical participants able perform following statistical analyses:Simple Linear RegressionCorrelationFor , participants able :Perform test RInterpret outputCheck assumptions test","code":""},{"path":"cs3-intro.html","id":"background-2","chapter":"14 Introduction","heading":"14.2 Background","text":"practical focuses implementation various statistical tests relating simple linear regression correlation., focus underlying theory tests (although demonstrators happy answer questions may ).test section :explains purpose test,explains visualise data,explains perform test R,explains interpret output report results, andexplains assess assumptions required perform test.","code":""},{},{"path":"introduction-2.html","id":"introduction-2","chapter":"15 Introduction","heading":"15 Introduction","text":"practical introducing can compare data different continuous variables.","code":""},{"path":"introduction-2.html","id":"cs3-datasets","chapter":"15 Introduction","heading":"15.1 Datasets","text":"section uses various data sets. located data/raw/ folder working directory. Please see Datasets information.","code":""},{},{"path":"correlation-coefficients.html","id":"correlation-coefficients","chapter":"16 Correlation coefficients","heading":"16 Correlation coefficients","text":"","code":""},{"path":"correlation-coefficients.html","id":"objectives-5","chapter":"16 Correlation coefficients","heading":"16.1 Objectives","text":"QuestionsWhat correlation coefficients?kind correlation coefficients use ?ObjectivesBe able calculate correlation coefficients RUse visual tools explore correlations variablesKnow limitations correlation coefficients","code":""},{"path":"correlation-coefficients.html","id":"purpose-and-aim-4","chapter":"16 Correlation coefficients","heading":"16.2 Purpose and aim","text":"Correlation refers relationship two variables (datasets) one another. Two datasets said correlated independent one another. Correlations can useful can indicate predictive relationship may exist. However just two datasets correlated mean causally related.","code":""},{"path":"correlation-coefficients.html","id":"section-commands-8","chapter":"16 Correlation coefficients","heading":"16.3 Section commands","text":"New commands used section:","code":""},{"path":"correlation-coefficients.html","id":"data-and-hypotheses-7","chapter":"16 Correlation coefficients","heading":"16.4 Data and hypotheses","text":"use USArrests dataset example. rather bleak dataset contains statistics arrests per 100,000 residents assault, murder robbery 50 US states 1973, alongside proportion population lived urban areas time. USArrests data frame 50 observations five variables: state, murder, assault, urban_pop robbery.data stored file data/tidy/CS3-usarrests.csv.First read data:","code":"\n# load the data\nUSArrests <- read_csv(\"data/tidy/CS3-usarrests.csv\")\n\n# have a look at the data\nUSArrests## # A tibble: 50 √ó 5\n##    state       murder assault urban_pop robbery\n##    <chr>        <dbl>   <dbl>     <dbl>   <dbl>\n##  1 Alabama       13.2     236        58    21.2\n##  2 Alaska        10       263        48    44.5\n##  3 Arizona        8.1     294        80    31  \n##  4 Arkansas       8.8     190        50    19.5\n##  5 California     9       276        91    40.6\n##  6 Colorado       7.9     204        78    38.7\n##  7 Connecticut    3.3     110        77    11.1\n##  8 Delaware       5.9     238        72    15.8\n##  9 Florida       15.4     335        80    31.9\n## 10 Georgia       17.4     211        60    25.8\n## # ‚Ä¶ with 40 more rows"},{"path":"correlation-coefficients.html","id":"pearsons-product-moment-correlation-coefficient","chapter":"16 Correlation coefficients","heading":"16.5 Pearson‚Äôs product moment correlation coefficient","text":"Pearson‚Äôs r (quantity also known) measure linear correlation two variables. value -1 +1, +1 means perfect positive correlation, -1 means perfect negative correlation 0 means correlation .can look correlations need reformat data little bit. functions ‚Äôre going use require data frames contain numbers input. want keep state information linked data, need define state column name rows.need update original USArrests data frame, ‚Äôre just piping displaying output can see ‚Äôs going .","code":"\n# convert the state column to row names\nUSArrests %>% \n  column_to_rownames(var = \"state\")"},{"path":"correlation-coefficients.html","id":"summarise-and-visualise-8","chapter":"16 Correlation coefficients","heading":"16.6 Summarise and visualise","text":"Knowing reformatting works, can first visualise data:argument lower.panel tells R add redundant reflected lower set plots, diagonalFrom visual inspection scatter plots can see appears slight positive correlation pairs variables, although may weak cases (murder urban_pop example).","code":"\n# create correlation plot\nUSArrests %>% \n  column_to_rownames(var = \"state\") %>% \n  pairs(lower.panel = NULL)"},{"path":"correlation-coefficients.html","id":"implement-test-7","chapter":"16 Correlation coefficients","heading":"16.7 Implement test","text":"can calculate Pearson‚Äôs correlation coefficients pair variables (e.g.¬†coefficient murder assault). several functions allow . cor() function base R cor_mat() rstatix package, spit results matrix (grid) format. ‚Äôll use cor_mat() can keep using tibble data sets.First create matrix, keeping state data linked row namesThe method argument tells R correlation coefficient use (pearson (default), kendall, spearman)","code":"\n# calculate Pearson's correlation coefficients\nUSArrests %>% \n  column_to_rownames(var = \"state\") %>% \n  cor_mat(method = \"pearson\")"},{"path":"correlation-coefficients.html","id":"interpret-output-and-report-results-6","chapter":"16 Correlation coefficients","heading":"16.8 Interpret output and report results","text":"give following output:table gives correlation coefficient pair variables data frame. correlated variables murder assault r value 0.80. appears agree well set scatter plots produced earlier.","code":"## # A tibble: 4 √ó 5\n##   rowname   murder assault urban_pop robbery\n## * <chr>      <dbl>   <dbl>     <dbl>   <dbl>\n## 1 murder      1       0.8       0.07    0.56\n## 2 assault     0.8     1         0.26    0.67\n## 3 urban_pop   0.07    0.26      1       0.41\n## 4 robbery     0.56    0.67      0.41    1"},{"path":"correlation-coefficients.html","id":"exercise-state-data-pearson","chapter":"16 Correlation coefficients","heading":"16.9 Exercise: State data (Pearson)","text":"Exercise 16.1  Pearson‚Äôs correlation USA state dataWe use data file data/tidy/CS3-statedata.csv dataset exercise. rather benign dataset contains information general properties US state, population (1975), per capita income (1974), illiteracy proportion (1970), life expectancy (1969), murder rate per 100,000 people (‚Äôs getting away ), percentage population high-school graduates, average number days minimum temperature freezing 1931 1960, state area square miles. dataset contains 50 rows 8 columns, column names: population, income, illiteracy, life_exp, murder, hs_grad, frost area.Load data (remembering tell R first column CSV file used specify row names dataset) use pairs() command visually identify 3 different pairs variables appear bethe positively correlatedthe negatively correlatednot correlated allCalculate Pearson‚Äôs r variable pairs see well able identify correlation visually.get correlation coefficients format allows us manipulate , use cor_test() function. something similar cor_mat() function - calculates pairwise correlation coefficients. However, outputs results table format, instead matrix.two variables compared given var1 var2 columns. correlation coefficient given cor column.extract maximum, minimum least correlated pairs, easy filter correlation table bit , pair now appears twice (orientation, murder & assault, assault & murder).Now unique pairs corresponding correlation coefficients, can extract information need:taken together:positively correlated variables illiteracy murderThe negatively correlated variables life_exp murderThe uncorrelated variables population area","code":"\nUSAstate <- read_csv(\"data/tidy/CS3-statedata.csv\")\n\n# have a look at the data\nUSAstate## # A tibble: 50 √ó 9\n##    state       population income illiteracy life_exp murder hs_grad frost   area\n##    <chr>            <dbl>  <dbl>      <dbl>    <dbl>  <dbl>   <dbl> <dbl>  <dbl>\n##  1 Alabama           3615   3624        2.1     69.0   15.1    41.3    20  50708\n##  2 Alaska             365   6315        1.5     69.3   11.3    66.7   152 566432\n##  3 Arizona           2212   4530        1.8     70.6    7.8    58.1    15 113417\n##  4 Arkansas          2110   3378        1.9     70.7   10.1    39.9    65  51945\n##  5 California       21198   5114        1.1     71.7   10.3    62.6    20 156361\n##  6 Colorado          2541   4884        0.7     72.1    6.8    63.9   166 103766\n##  7 Connecticut       3100   5348        1.1     72.5    3.1    56     139   4862\n##  8 Delaware           579   4809        0.9     70.1    6.2    54.6   103   1982\n##  9 Florida           8277   4815        1.3     70.7   10.7    52.6    11  54090\n## 10 Georgia           4931   4091        2       68.5   13.9    40.6    60  58073\n## # ‚Ä¶ with 40 more rows\n# visual comparisons of variables\nUSAstate %>% \n  column_to_rownames(var = \"state\") %>% \n  pairs(lower.panel = NULL)\n# calculate Pearson's correlation coefficients\nUSAstate %>% \n  column_to_rownames(var = \"state\") %>%\n  cor_test(method = \"pearson\")## # A tibble: 64 √ó 8\n##    var1       var2          cor     statistic      p conf.low conf.high method \n##    <chr>      <chr>       <dbl>         <dbl>  <dbl>    <dbl>     <dbl> <chr>  \n##  1 population population  1           Inf     0        1         1      Pearson\n##  2 population income      0.21          1.47  0.147   -0.0744    0.460  Pearson\n##  3 population illiteracy  0.11          0.750 0.457   -0.176     0.375  Pearson\n##  4 population life_exp   -0.068        -0.473 0.639   -0.340     0.214  Pearson\n##  5 population murder      0.34          2.54  0.0146   0.0722    0.568  Pearson\n##  6 population hs_grad    -0.098        -0.686 0.496   -0.367     0.185  Pearson\n##  7 population frost      -0.33         -2.44  0.0184  -0.559    -0.0593 Pearson\n##  8 population area        0.023         0.156 0.877   -0.257     0.299  Pearson\n##  9 income     population  0.21          1.47  0.147   -0.0744    0.460  Pearson\n## 10 income     income      1     464943848.    0        1         1      Pearson\n## # ‚Ä¶ with 54 more rows\n# calculate the correlation coefficients\n# select the unique pairs\n# and store in a new object\nUSAstate_cor <- USAstate %>% \n  column_to_rownames(var = \"state\") %>%\n  cor_test(method = \"pearson\") %>% \n  # filter out the self-pairs (e.g. murder & murder)\n  filter(cor != 1) %>% \n  # arrange the data by correlation coefficient\n  arrange(cor) %>% \n  # each correlation appears twice\n  # because the pairs are duplicated\n  group_by(cor) %>% \n  # slice the first row of each group\n  slice(seq(1, n(), by = 2)) %>% \n  # remove the grouping\n  ungroup()\n\n# have a look at the ouput\nUSAstate_cor## # A tibble: 28 √ó 8\n##    var1       var2         cor statistic        p conf.low conf.high method \n##    <chr>      <chr>      <dbl>     <dbl>    <dbl>    <dbl>     <dbl> <chr>  \n##  1 life_exp   murder     -0.78    -8.66  2.26e-11   -0.870   -0.642  Pearson\n##  2 illiteracy frost      -0.67    -6.29  9.16e- 8   -0.801   -0.484  Pearson\n##  3 illiteracy hs_grad    -0.66    -6.04  2.17e- 7   -0.791   -0.464  Pearson\n##  4 illiteracy life_exp   -0.59    -5.04  6.97e- 6   -0.745   -0.371  Pearson\n##  5 murder     frost      -0.54    -4.43  5.4 e- 5   -0.711   -0.307  Pearson\n##  6 murder     hs_grad    -0.49    -3.87  3.25e- 4   -0.675   -0.243  Pearson\n##  7 income     illiteracy -0.44    -3.37  1.51e- 3   -0.638   -0.181  Pearson\n##  8 population frost      -0.33    -2.44  1.84e- 2   -0.559   -0.0593 Pearson\n##  9 income     murder     -0.23    -1.64  1.08e- 1   -0.478    0.0516 Pearson\n## 10 life_exp   area       -0.11    -0.748 4.58e- 1   -0.374    0.176  Pearson\n## # ‚Ä¶ with 18 more rows\n# get most positively correlated pair\nUSAstate_cor %>%\n  filter(cor == max(cor))\n\n# get most negatively correlated pair\nUSAstate_cor %>%\n  filter(cor == min(cor))\n\n# get least correlated pair\nUSAstate_cor %>%\n  # abs() computes the absolute value\n  filter(cor == min(abs(cor)))"},{"path":"correlation-coefficients.html","id":"read-in-the-data","chapter":"16 Correlation coefficients","heading":"16.9.1 Read in the data","text":"","code":"\nUSAstate <- read_csv(\"data/tidy/CS3-statedata.csv\")\n\n# have a look at the data\nUSAstate## # A tibble: 50 √ó 9\n##    state       population income illiteracy life_exp murder hs_grad frost   area\n##    <chr>            <dbl>  <dbl>      <dbl>    <dbl>  <dbl>   <dbl> <dbl>  <dbl>\n##  1 Alabama           3615   3624        2.1     69.0   15.1    41.3    20  50708\n##  2 Alaska             365   6315        1.5     69.3   11.3    66.7   152 566432\n##  3 Arizona           2212   4530        1.8     70.6    7.8    58.1    15 113417\n##  4 Arkansas          2110   3378        1.9     70.7   10.1    39.9    65  51945\n##  5 California       21198   5114        1.1     71.7   10.3    62.6    20 156361\n##  6 Colorado          2541   4884        0.7     72.1    6.8    63.9   166 103766\n##  7 Connecticut       3100   5348        1.1     72.5    3.1    56     139   4862\n##  8 Delaware           579   4809        0.9     70.1    6.2    54.6   103   1982\n##  9 Florida           8277   4815        1.3     70.7   10.7    52.6    11  54090\n## 10 Georgia           4931   4091        2       68.5   13.9    40.6    60  58073\n## # ‚Ä¶ with 40 more rows"},{"path":"correlation-coefficients.html","id":"pair-wise-comparisons-visual","chapter":"16 Correlation coefficients","heading":"16.9.2 Pair-wise comparisons (visual)","text":"","code":"\n# visual comparisons of variables\nUSAstate %>% \n  column_to_rownames(var = \"state\") %>% \n  pairs(lower.panel = NULL)"},{"path":"correlation-coefficients.html","id":"calculate-the-correlation-coefficients","chapter":"16 Correlation coefficients","heading":"16.9.3 Calculate the correlation coefficients","text":"get correlation coefficients format allows us manipulate , use cor_test() function. something similar cor_mat() function - calculates pairwise correlation coefficients. However, outputs results table format, instead matrix.two variables compared given var1 var2 columns. correlation coefficient given cor column.extract maximum, minimum least correlated pairs, easy filter correlation table bit , pair now appears twice (orientation, murder & assault, assault & murder).Now unique pairs corresponding correlation coefficients, can extract information need:taken together:positively correlated variables illiteracy murderThe negatively correlated variables life_exp murderThe uncorrelated variables population area","code":"\n# calculate Pearson's correlation coefficients\nUSAstate %>% \n  column_to_rownames(var = \"state\") %>%\n  cor_test(method = \"pearson\")## # A tibble: 64 √ó 8\n##    var1       var2          cor     statistic      p conf.low conf.high method \n##    <chr>      <chr>       <dbl>         <dbl>  <dbl>    <dbl>     <dbl> <chr>  \n##  1 population population  1           Inf     0        1         1      Pearson\n##  2 population income      0.21          1.47  0.147   -0.0744    0.460  Pearson\n##  3 population illiteracy  0.11          0.750 0.457   -0.176     0.375  Pearson\n##  4 population life_exp   -0.068        -0.473 0.639   -0.340     0.214  Pearson\n##  5 population murder      0.34          2.54  0.0146   0.0722    0.568  Pearson\n##  6 population hs_grad    -0.098        -0.686 0.496   -0.367     0.185  Pearson\n##  7 population frost      -0.33         -2.44  0.0184  -0.559    -0.0593 Pearson\n##  8 population area        0.023         0.156 0.877   -0.257     0.299  Pearson\n##  9 income     population  0.21          1.47  0.147   -0.0744    0.460  Pearson\n## 10 income     income      1     464943848.    0        1         1      Pearson\n## # ‚Ä¶ with 54 more rows\n# calculate the correlation coefficients\n# select the unique pairs\n# and store in a new object\nUSAstate_cor <- USAstate %>% \n  column_to_rownames(var = \"state\") %>%\n  cor_test(method = \"pearson\") %>% \n  # filter out the self-pairs (e.g. murder & murder)\n  filter(cor != 1) %>% \n  # arrange the data by correlation coefficient\n  arrange(cor) %>% \n  # each correlation appears twice\n  # because the pairs are duplicated\n  group_by(cor) %>% \n  # slice the first row of each group\n  slice(seq(1, n(), by = 2)) %>% \n  # remove the grouping\n  ungroup()\n\n# have a look at the ouput\nUSAstate_cor## # A tibble: 28 √ó 8\n##    var1       var2         cor statistic        p conf.low conf.high method \n##    <chr>      <chr>      <dbl>     <dbl>    <dbl>    <dbl>     <dbl> <chr>  \n##  1 life_exp   murder     -0.78    -8.66  2.26e-11   -0.870   -0.642  Pearson\n##  2 illiteracy frost      -0.67    -6.29  9.16e- 8   -0.801   -0.484  Pearson\n##  3 illiteracy hs_grad    -0.66    -6.04  2.17e- 7   -0.791   -0.464  Pearson\n##  4 illiteracy life_exp   -0.59    -5.04  6.97e- 6   -0.745   -0.371  Pearson\n##  5 murder     frost      -0.54    -4.43  5.4 e- 5   -0.711   -0.307  Pearson\n##  6 murder     hs_grad    -0.49    -3.87  3.25e- 4   -0.675   -0.243  Pearson\n##  7 income     illiteracy -0.44    -3.37  1.51e- 3   -0.638   -0.181  Pearson\n##  8 population frost      -0.33    -2.44  1.84e- 2   -0.559   -0.0593 Pearson\n##  9 income     murder     -0.23    -1.64  1.08e- 1   -0.478    0.0516 Pearson\n## 10 life_exp   area       -0.11    -0.748 4.58e- 1   -0.374    0.176  Pearson\n## # ‚Ä¶ with 18 more rows\n# get most positively correlated pair\nUSAstate_cor %>%\n  filter(cor == max(cor))\n\n# get most negatively correlated pair\nUSAstate_cor %>%\n  filter(cor == min(cor))\n\n# get least correlated pair\nUSAstate_cor %>%\n  # abs() computes the absolute value\n  filter(cor == min(abs(cor)))"},{"path":"correlation-coefficients.html","id":"spearmans-rank-correlation-coefficient","chapter":"16 Correlation coefficients","heading":"16.10 Spearman‚Äôs rank correlation coefficient","text":"test first calculates rank numerical data (.e.¬†position smallest (negative) largest (positive)) two variables calculates Pearson‚Äôs product moment correlation coefficient using ranks. consequence, test less sensitive outliers distribution.","code":""},{"path":"correlation-coefficients.html","id":"implement-test-8","chapter":"16 Correlation coefficients","heading":"16.11 Implement test","text":"using USArrests data set , run command:Remember cor_mat() requires matrix, use state column row namesThe argument method tells R correlation coefficient use","code":"\nUSArrests %>% \n  column_to_rownames(var = \"state\") %>% \n  cor_mat(method = \"spearman\")"},{"path":"correlation-coefficients.html","id":"interpret-output-and-report-results-7","chapter":"16 Correlation coefficients","heading":"16.12 Interpret output and report results","text":"gives following output:table gives correlation coefficient pair variables data frame. Slightly annoyingly, pair occurs twice opposite direction.","code":"## # A tibble: 4 √ó 5\n##   rowname   murder assault urban_pop robbery\n## * <chr>      <dbl>   <dbl>     <dbl>   <dbl>\n## 1 murder      1       0.82      0.11    0.68\n## 2 assault     0.82    1         0.28    0.71\n## 3 urban_pop   0.11    0.28      1       0.44\n## 4 robbery     0.68    0.71      0.44    1"},{"path":"correlation-coefficients.html","id":"exercise-state-data-spearman","chapter":"16 Correlation coefficients","heading":"16.13 Exercise: State data (Spearman)","text":"Exercise 16.2  Spearman‚Äôs correlation USA state dataCalculate Spearman‚Äôs correlation coefficient data/tidy/CS3-statedata.csv dataset.variable‚Äôs correlations affected use Spearman‚Äôs rank compared Pearson‚Äôs r?reference scatter plot produced earlier, can explain might ?Remember use column_to_rownames(var = \"state\") argument load data matrixInstead eye-balling differences, think can determine difference two correlation matricesThe cor_plot() function can useful visualise matricesIn order determine variables affected choice Spearman vs Pearson just plot matrices side side try spot going , one reasons ‚Äôre using R can bit programmatic things. Also, eyes aren‚Äôt good processing parsing sort information display. better way somehow visualise data.Let‚Äôs calculate difference two correlation matrices. create correlation matrix using cor_mat(). Next remove rowname, ‚Äôre left just data frame containing numbers. way can subtract values two data frames.Lastly, use cor_plot() function plot heatmap differences.one cases using tidyverse actually necessarily easiest way. similar thing using base R syntax:plot coloured blue red, indicating biggest positive differences correlation coefficients blue. biggest negative differences coloured red, whereas least difference indicated white.plot symmetric along leading diagonal (hopefully obvious reasons) can see majority squares light blue light red colour, means isn‚Äôt much difference Spearman Pearson vast majority variables. squares appear darkest look along area row/column suggesting ‚Äôs big difference correlation coefficients .can now revisit pairwise scatter plot see makes sense:can see clearly correspond plots noticeable outliers. example, Alaska twice big next biggest state, Texas. Big outliers data can large impact Pearson coefficient, whereas Spearman coefficient robust effects outliers. can see detail look area vs income graph coefficients. Pearson gives value 0.36, slight positive correlation, whereas Spearman gives value 0.057, basically uncorrelated. single outlier (Alaska) top-right scatter plot big effect Pearson practically ignored Spearman.Well done, Mr.¬†Spearman.","code":"\nUSAstate %>% \n  column_to_rownames(var = \"state\") %>% \n  cor_mat(method = \"spearman\")## # A tibble: 8 √ó 9\n##   rowname    population income illiteracy life_exp murder hs_grad frost   area\n## * <chr>           <dbl>  <dbl>      <dbl>    <dbl>  <dbl>   <dbl> <dbl>  <dbl>\n## 1 population       1     0.12        0.31    -0.1    0.35   -0.38 -0.46 -0.12 \n## 2 income           0.12  1          -0.31     0.32  -0.22    0.51  0.2   0.057\n## 3 illiteracy       0.31 -0.31        1       -0.56   0.67   -0.65 -0.68 -0.25 \n## 4 life_exp        -0.1   0.32       -0.56     1     -0.78    0.52  0.3   0.13 \n## 5 murder           0.35 -0.22        0.67    -0.78   1      -0.44 -0.54  0.11 \n## 6 hs_grad         -0.38  0.51       -0.65     0.52  -0.44    1     0.4   0.44 \n## 7 frost           -0.46  0.2        -0.68     0.3   -0.54    0.4   1     0.11 \n## 8 area            -0.12  0.057      -0.25     0.13   0.11    0.44  0.11  1\n# create a data frame that contains all the Pearson's coefficients\nUSAstate_pear <- USAstate %>% \n  column_to_rownames(var = \"state\") %>% \n  cor_mat(method = \"pearson\") %>% \n  # remove the row names\n  select(-rowname)\n\n# create a data frame that contains all the Pearson's coefficients\nUSAstate_spear <- USAstate %>% \n  column_to_rownames(var = \"state\") %>% \n  cor_mat(method = \"spearman\") %>% \n  # remove the row names\n  select(-rowname)\n\n# calculate the difference between Pearson's and Spearman's\nUSAstate_diff <- USAstate_pear - USAstate_spear\n\n# use the column names of the data set as rownames\nrownames(USAstate_diff) <- names(USAstate_diff)\n\nUSAstate_diff %>%\n  cor_plot()\n# read in the data with the base R read.csv function\n# and assign the first column as row names\nUSAstate_base <- read.csv(\"data/tidy/CS3-statedata.csv\", row.names = 1)\n\n# calculate a correlation matrix using Pearson's\ncorPear <- cor(USAstate_base, method = \"pearson\")\n\n# calculate a correlation matrix using Spearman\ncorSpea <- cor(USAstate_base, method = \"spearman\")\n\n# calculate the difference between the two matrices\ncorDiff <- corPear - corSpea\n\n# and plot it, like before\ncorDiff %>% \n  cor_plot()\n# visual comparisons of variables\nUSAstate %>% \n  column_to_rownames(var = \"state\") %>% \n  pairs(lower.panel = NULL)"},{"path":"correlation-coefficients.html","id":"key-points-4","chapter":"16 Correlation coefficients","heading":"16.14 Key points","text":"Correlation degree two variables linearly relatedCorrelation imply causationWe can visualise correlations using pairs() cor_plot() functionsUsing cor_mat() cor_test() functions can calculate correlation matricesTwo main correlation coefficients Pearson‚Äôs r Spearman‚Äôs rank, Spearman‚Äôs rank less sensitive outliers","code":""},{},{"path":"linear-regression.html","id":"linear-regression","chapter":"17 Linear regression","heading":"17 Linear regression","text":"","code":""},{"path":"linear-regression.html","id":"objectives-6","chapter":"17 Linear regression","heading":"17.1 Objectives","text":"QuestionsWhen use linear regression?interpret results?ObjectivesBe able perform linear regression RUse ANOVA check slope regression differs zeroUnderstand underlying assumptions linear regression analysisUse diagnostic plots check assumptions","code":""},{"path":"linear-regression.html","id":"purpose-and-aim-5","chapter":"17 Linear regression","heading":"17.2 Purpose and aim","text":"Regression analysis tests association two variables, also allows one investigate quantitatively nature relationship present, thus determine whether one variable may used predict values another.\nSimple linear regression essentially models dependence scalar dependent variable (y) independent (explanatory) variable (x) according relationship:\\[\\begin{equation*} \ny = \\beta_0 + \\beta_1 x\n\\end{equation*}\\]\\(\\beta_0\\) value intercept \\(\\beta_1\\) slope fitted line. aim simple linear regression analysis assess whether coefficient slope, \\(\\beta_1\\), actually different zero. different zero can say \\(x\\) significant effect \\(y\\) (since changing \\(x\\) leads predicted change \\(y\\)), whereas isn‚Äôt significantly different zero, say isn‚Äôt sufficient evidence relationship. course, order assess whether slope significantly different zero first need calculate values \\(\\beta_0\\) \\(\\beta_1\\).","code":""},{"path":"linear-regression.html","id":"section-commands-9","chapter":"17 Linear regression","heading":"17.3 Section commands","text":"new commands used section.","code":""},{"path":"linear-regression.html","id":"data-and-hypotheses-8","chapter":"17 Linear regression","heading":"17.4 Data and hypotheses","text":"perform simple linear regression analysis two variables murder assault USArrests dataset. wish determine whether assault variable significant predictor murder variable. means need find coefficients \\(\\beta_0\\) \\(\\beta_1\\) best fit following macabre equation:\\[\\begin{equation*}\nMurder  = \\beta_0 + \\beta_1 \\cdot Assault\n\\end{equation*}\\]testing following null alternative hypotheses:\\(H_0\\): assault significant predictor murder, \\(\\beta_1 = 0\\)\\(H_1\\): assault significant predictor murder, \\(\\beta_1 \\neq 0\\)","code":""},{"path":"linear-regression.html","id":"summarise-and-visualise-9","chapter":"17 Linear regression","heading":"17.5 Summarise and visualise","text":"can visualise data :appears relatively strong positive relationship two variables whilst reasonable scatter points around trend line, probably expect significant result case.","code":"\n# create scatterplot of the data\nUSArrests %>% \n  ggplot(aes(x = assault, y = murder)) +\n  geom_point()"},{"path":"linear-regression.html","id":"assumptions-15","chapter":"17 Linear regression","heading":"17.6 Assumptions","text":"order linear regression analysis valid 4 key assumptions need met:data must linear (entirely possible calculate straight line data straight - doesn‚Äôt mean !)residuals must normally distributedThe residuals must correlated fitted valuesThe fit depend overly much single point (point high leverage).Whether assumptions met can easily checked visually producing four key diagnostic plots.First need define linear model:first argument lm formula saying murder depends assaults. seen , syntax generally dependent variable ~ independent variable.second argument specifies dataset useNext, can create diagnostic plots model:top left graph plots Residuals plot. data best explained straight line uniform distribution points horizontal blue line (sufficient points red line, smoother line, top blue line). plot pretty good.top right graph shows Q-Q plot allows visual inspection normality. residuals normally distributed, points lie diagonal dotted line. isn‚Äôt bad slight snaking towards upper end appears outlier.bottom left Location-scale graph allows us investigate whether correlation residuals predicted values whether variance residuals changes significantly. , red line horizontal. correlation change variance red line horizontal. plot fine.last graph shows Cook‚Äôs distance tests one point unnecessarily large effect fit. important aspect see points larger 0.5 (meaning ‚Äôd careful) 1.0 (meaning ‚Äôd definitely check point large effect model). , point undue influence. plot good.Formally, concern looking diagnostic plots, linear regression valid. However, disappointingly, people ever check whether linear regression assumptions met quoting results.Let‚Äôs change leading example!","code":"\nlm_1 <- lm(murder ~ assault,\n           data = USArrests)\nlm_1 %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)"},{"path":"linear-regression.html","id":"implement-test-9","chapter":"17 Linear regression","heading":"17.7 Implement test","text":"already defined linear model, can closer look :function lm returns linear model (lm) object essentially list containing everything necessary understand analyse linear model. However, just type model name () just prints screen actual coefficients model .e.¬†intercept slope line.found line best fit given :\\[\\begin{equation*}\nMurder = 0.63 + 0.042 \\cdot Assault\n\\end{equation*}\\]Assess whether slope significantly different zero:, use anova() command assess significance. shouldn‚Äôt surprising stage introductory lectures made sense. mathematical perspective, one-way ANOVA simple linear regression exactly makes sense use command analyse R.","code":"\n# show the linear model\nlm_1## \n## Call:\n## lm(formula = murder ~ assault, data = USArrests)\n## \n## Coefficients:\n## (Intercept)      assault  \n##     0.63168      0.04191\nanova(lm_1)"},{"path":"linear-regression.html","id":"interpret-output-and-report-results-8","chapter":"17 Linear regression","heading":"17.8 Interpret output and report results","text":"exactly format table saw one-way ANOVA:1st line just tells ANOVA testThe 2nd line tells response variable (case Murder)3rd, 4th 5th lines ANOVA table contain useful values:\nDf column contains degrees freedom values row, 1 48 (‚Äôll need reporting)\nF value column contains F statistic, 86.454 (‚Äôll need reporting).\np-value 2.596e-12 number directly Pr(>F) 4th line.\nvalues table (Sum Sq Mean Sq) column used calculate F statistic don‚Äôt need know .\nDf column contains degrees freedom values row, 1 48 (‚Äôll need reporting)F value column contains F statistic, 86.454 (‚Äôll need reporting).p-value 2.596e-12 number directly Pr(>F) 4th line.values table (Sum Sq Mean Sq) column used calculate F statistic don‚Äôt need know ., p-value ‚Äôre interested shows us probability getting data null hypothesis actually true slope line actually zero.\nSince p-value excruciatingly tiny can reject null hypothesis state :simple linear regression showed assault rate US states significant predictor number murders (F = 86.45, df = 1,48, p = 2.59x10-12).Plotting regression lineIt can helpful plot regression line original data see far data predicted linear values. can :plot data using geom_point()Next, add linear model using geom_smooth(method = \"lm\"), hiding confidence intervals (se = FALSE)","code":"## Analysis of Variance Table\n## \n## Response: murder\n##           Df Sum Sq Mean Sq F value    Pr(>F)    \n## assault    1 597.70  597.70  86.454 2.596e-12 ***\n## Residuals 48 331.85    6.91                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# plot the data\nUSArrests %>% \n  ggplot(aes(x = assault, y = murder)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)"},{"path":"linear-regression.html","id":"exercise-3","chapter":"17 Linear regression","heading":"17.9 Exercise","text":"Exercise 17.1  Linear regressionCalculate two simple linear regressions using data/tidy/CS3-statedata.csv dataset, first variable life_exp variable murder variable hs_grad frost.following cases:Find value slope intercept coefficients regressionsDetermine slope significantly different zero (.e.¬†relationship two variables)Produce scatter plot data line best fit superimposed top.Produce diagnostic plots discuss (virtual) neighbour carried simple linear regression caseMurder Life ExpectancyLet‚Äôs see murder variable can used predict life_exp variable. Let‚Äôs plot first .visualise reasons:check data aren‚Äôt obviously wrong. sensible values life expectancy (nothing massively large small), plausible values murder rates (‚Äôm au fait US murder rates 1976 small positive numbers seem plausible).check see expect statistical analysis. appear reasonable downward trend data. surprised didn‚Äôt get significant result given amount data spread data lineWe check assumptions (roughly though ‚Äôll properly minute). Nothing immediately gives cause concern; data appear linear, spread data around line appears homogeneous symmetrical. outliers either.Now, let‚Äôs check assumptions diagnostic plots.Residuals plot appears symmetric enough (similar distribution points horizontal blue line) happy linearity. Similarly red line Location-Scale plot looks horizontal enough happy homogeneity variance. aren‚Äôt influential points Cook‚Äôs distance plot. plot give bit concern Q-Q plot. see clear evidence snaking, although degree snaking isn‚Äôt actually bad. just means can pretty certain distribution residuals isn‚Äôt normal, also isn‚Äôt non-normal. situation? Well, three possible options:Appeal Central Limit Theorem. states large enough sample size don‚Äôt worry whether distribution residuals normally distributed. Large enough bit moving target honest depends non-normal underlying data . data little bit non-normal can get away using smaller sample data massively skewed (example). exact science, anything 30 data points considered lot mild moderate non-normality (case). data skewed looking data points (50-100). , example can legitimately just carry analysis without worrying.Try transforming data. try applying mathematical functions response variable (life_exp) hope repeating analysis transformed variable make things better. honest might work won‚Äôt know try. Dealing transformed variables legitimate approach can make interpreting model bit challenging. particular example none traditional transformations (log, square-root, reciprocal) anything fix slight lack normality (can take word try using; lm(log(LifeExp ~ Murder, data = USAstate)) example.Go permutation methods / bootstrapping. approach definitely work. don‚Äôt time explain (‚Äôs subject entire practical). approach also requires us reasonably large sample size work well assume distribution sample good approximation distribution entire dataset.case, large enough sample size deviation normality isn‚Äôt bad, can just crack standard analysis., let‚Äôs actually analysis:find murder rate statistically significant predictor life expectancy US states. Woohoo!High School Graduation Frosty DaysNow let‚Äôs investigate relationship proportion High School Graduates state (hs_grad) mean number days freezing (frost) within state., look data.doesn‚Äôt appear ridiculous errors data; High School graduation proportions 0-100% range mean number sub-zero days state 0 365, numbers plausible.Whilst trend upwards, wouldn‚Äôt surprise came back significant, ‚Äôm bit concerned ‚Ä¶assumptions. ‚Äôm mainly concerned data aren‚Äôt linear. appears noticeable pattern data sort minimum around 50-60 Frost days. means ‚Äôs hard assess assumptions.Let‚Äôs check properlyNow, let‚Äôs check assumptions diagnostic plots.can see suspected backed Residuals plot. data aren‚Äôt linear appears sort odd -pattern . Given lack linearity just isn‚Äôt worth worrying plots model misspecified: straight line just doesn‚Äôt represent data .Just reference, practice looking diagnostic plots, ignore lack linearity can say thatNormality pretty good Q-Q plotHomogeneity variance isn‚Äôt good appears noticeable drop variance go left right (consideration Location-Scale plot)don‚Äôt appear influential points (looking Cook‚Äôs distance plot)However, none relevant particular case since data aren‚Äôt linear straight line wrong model fit.situation?Well actually, bit tricky aren‚Äôt easy fixes . two broad solutions dealing misspecified model.common solution need predictor variables model. ‚Äôre trying explain/predict high school graduation using number frost days. Obviously many things affect proportion high school graduates just cold State (weird potential predictor think ) need statistical approach allows us look multiple predictor variables. ‚Äôll cover approach next two sessions.potential solution say high school graduation can fact predicted number frost days relationship isn‚Äôt linear. need specify relationship (curve basically) try fit data new, non-linear, curve. process called, unsurprisingly, non-linear regression don‚Äôt cover course. process best used already strong theoretical reason non-linear relationship two variables (sigmoidal dose-response curves pharmacology exponential relationships cell growth). case don‚Äôt preconceived notions wouldn‚Äôt really appropriate case.Neither solutions can tackled knowledge far course can definitely say based upon data set, isn‚Äôt linear relationship (significant otherwise) frosty days high school graduation rates.","code":"\n# plot the data and add the regression line\nUSAstate %>% \n  ggplot(aes(x = murder, y = life_exp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n# create a linear model\nlm_murder <- lm(life_exp ~ murder,\n           data = USAstate)\n\n# create the diagnostic plots\nlm_murder %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\nanova(lm_murder)## Analysis of Variance Table\n## \n## Response: life_exp\n##           Df Sum Sq Mean Sq F value   Pr(>F)    \n## murder     1 53.838  53.838  74.989 2.26e-11 ***\n## Residuals 48 34.461   0.718                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# plot the data\nUSAstate %>% \n  ggplot(aes(x = frost, y = hs_grad)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n# create a linear model\nlm_frost <- lm(hs_grad ~ frost,\n               data = USAstate)\n\n# create the diagnostic plots\nlm_frost %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)"},{"path":"linear-regression.html","id":"key-points-5","chapter":"17 Linear regression","heading":"17.10 Key points","text":"Linear regression tests linear relationship exists two variablesIf , can use one variable predict anotherA linear model intercept slope test slope differs zeroWe create linear models R lm() function use anova() assess slope coefficientWe can use linear regression four assumptions met:\ndata linear\nResiduals normally distributed\nResiduals correlated fitted values\nsingle point large influence linear model\ndata linearResiduals normally distributedResiduals correlated fitted valuesNo single point large influence linear modelWe can use resid_panel() get diagnostic plots R, help evaluate assumptions","code":""},{},{"path":"cs4-intro.html","id":"cs4-intro","chapter":"18 Introduction","heading":"18 Introduction","text":"","code":""},{"path":"cs4-intro.html","id":"objectives-7","chapter":"18 Introduction","heading":"18.1 Objectives","text":"Aim: introduce R commands carrying two-way ANOVA linear regression grouped data/ANCOVABy end practical participants able achieve following:Carry two-way ANOVA using R interpret outputAnalyse linear regression grouped data (ANCOVA)","code":""},{"path":"cs4-intro.html","id":"background-3","chapter":"18 Introduction","heading":"18.2 Background","text":"practical focuses implementation various statistical tests relating multiple predictor variables R. focus underlying theory tests (although demonstrators happy answer questions may ).test section explaining perform test, section explaining results output screen, exercise complete relating test .","code":""},{},{"path":"introduction-3.html","id":"introduction-3","chapter":"19 Introduction","heading":"19 Introduction","text":"","code":""},{"path":"introduction-3.html","id":"cs4-datasets","chapter":"19 Introduction","heading":"19.1 Datasets","text":"section uses various data sets. located data/raw/ folder working directory. Please see Datasets information.","code":""},{},{"path":"two-way-anova.html","id":"two-way-anova","chapter":"20 Two-way ANOVA","heading":"20 Two-way ANOVA","text":"","code":""},{"path":"two-way-anova.html","id":"objectives-8","chapter":"20 Two-way ANOVA","heading":"20.1 Objectives","text":"QuestionsWhen use two-way ANOVA appropriate?perform R?ObjectivesBe able perform two-way ANOVA RUnderstand concept interaction two predictor variablesBe able plot interactions R","code":""},{"path":"two-way-anova.html","id":"purpose-and-aim-6","chapter":"20 Two-way ANOVA","heading":"20.2 Purpose and aim","text":"two-way analysis variance used two categorical predictor variables (factors) single continuous response variable. example, looking body weight (continuous response variable kilograms) affected sex (categorical variable, Male Female) exercise type (categorical variable, Control Runner).analysing type data two things want know:either predictor variables effect response variable .e.¬†sex affect body weight? runner affect body weight?interaction two predictor variables? interaction mean effect exercise weight depends whether male female rather independent sex. example male means runners weigh non-runners, female means runners weight less non-runners say interaction.first consider visualise data carrying appropriate statistical test.","code":""},{"path":"two-way-anova.html","id":"section-commands-10","chapter":"20 Two-way ANOVA","heading":"20.3 Section commands","text":"New commands used section:","code":""},{"path":"two-way-anova.html","id":"data-and-hypotheses-9","chapter":"20 Two-way ANOVA","heading":"20.4 Data and hypotheses","text":"recreate example analysis used lecture. data stored .csv file called data/tidy/CS4-exercise.csv.","code":""},{"path":"two-way-anova.html","id":"summarise-and-visualise-10","chapter":"20 Two-way ANOVA","heading":"20.5 Summarise and visualise","text":"exercise data frame three variables; weight, sex exercise. weight continuous response variable, whereas sex exercise categorical predictor variables.First, read data:visualise:produce box plots showing response variable (weight) terms one predictor variables. values predictor variable case aren‚Äôt taken account.(Optional) Add titles, axis labels information see fit plots make presentable.Visualise predictor variables together:produces box plots (four) combinations predictor variables. plotting sex x-axis; weight y-axis filling box plot exercise regime.‚Äôve also changed default colouring scheme, using scale_fill_brewer(palette = \"Dark2\"). uses colour-blind friendly colour palette (Brewer colour pallete ).(Optional) Add titles, axis labels information see fit plot make presentable.example four box plots relatively easy compare look interactions variables, two groups per categorical variable, become harder spot going .compare categorical variables easily can just plot group means aids ability look interactions main effects predictor variable.Create interaction plot:plot weight y-axis, sex x-axis.colour data exercise regime group data exercise work mean values groupgeom_point() displays datastat_summary(fun = mean)calculates mean groupscale_colour_brewer() lets us define colour paletteThe choice categorical factor plotted horizontal axis plotted different lines completely arbitrary. Looking data ways shouldn‚Äôt add anything often ‚Äôll find prefer one plot another.Plot interaction plot way round:now good feeling data already provide guesses following three questions:appear interaction two categorical variables?:\nexercise effect weight?\nsex effect weight?\nexercise effect weight?sex effect weight?can now attempt answer three questions formally using ANOVA test. ask R explicitly test three things: interaction, effect exercise effect sex.","code":"\nexercise <- read_csv(\"data/tidy/CS4-exercise.csv\")\n# visualise the data, sex vs weight\nexercise %>% \n  ggplot(aes(x = sex, y = weight)) +\n  geom_boxplot()\n# visualise the data, exercise vs weight\nexercise %>% \n  ggplot(aes(x = exercise, y = weight)) +\n  geom_boxplot()\nexercise %>% \n  ggplot(aes(x = sex, y = weight, fill = exercise)) +\n  geom_boxplot() +\n  scale_fill_brewer(palette = \"Dark2\")\nexercise %>% \n  ggplot(aes(x = sex,\n             y = weight,\n             colour = exercise, group = exercise)) +\n  geom_point() +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_colour_brewer(palette = \"Dark2\")\nexercise %>% \n  ggplot(aes(x = exercise,\n             y = weight,\n             colour = sex, group = sex)) +\n  geom_point() +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_colour_brewer(palette = \"Dark2\")"},{"path":"two-way-anova.html","id":"assumptions-16","chapter":"20 Two-way ANOVA","heading":"20.6 Assumptions","text":"can formally test things first need define model check underlying assumptions. use following code define model:sex:exercise term R represents concept interaction two variables.two-way ANOVA type linear model need satisfy pretty much assumptions simple linear regression one-way ANOVA:data must systematic pattern itThe residuals must normally distributedThe residuals must homogeneity varianceThe fit depend overly much single point (point high leverage)., check assumptions visually producing four key diagnostic plots.Residual plot shows residuals predicted values. systematic pattern plot pretty good.Q-Q plot allows visual inspection normality. , looks OK (perfect OK).Location-Scale plot allows us investigate whether homogeneity variance. plot fine (perfect fine).Cook‚Äôs D plot shows individual point high influence model (values well 0.5)shorthand way writing:weight ~ sex + exercise + sex:exerciseIf use following syntax:weight ~ sex * exerciseThen R interprets exactly way writing three terms.\ncan see compare output following two commands:","code":"\n# define the linear model\nlm_exercise <- lm(weight ~ sex + exercise + sex:exercise,\n                  data = exercise)\nlm_exercise %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\nanova(lm(weight ~ sex + exercise + sex:exercise,\n         data = exercise))## Analysis of Variance Table\n## \n## Response: weight\n##              Df Sum Sq Mean Sq F value    Pr(>F)    \n## sex           1 607.20  607.20 43.1144 6.493e-06 ***\n## exercise      1 184.83  184.83 13.1240  0.002287 ** \n## sex:exercise  1  82.42   82.42  5.8521  0.027839 *  \n## Residuals    16 225.34   14.08                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(lm(weight ~ sex * exercise,\n         data = exercise))## Analysis of Variance Table\n## \n## Response: weight\n##              Df Sum Sq Mean Sq F value    Pr(>F)    \n## sex           1 607.20  607.20 43.1144 6.493e-06 ***\n## exercise      1 184.83  184.83 13.1240  0.002287 ** \n## sex:exercise  1  82.42   82.42  5.8521  0.027839 *  \n## Residuals    16 225.34   14.08                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"two-way-anova.html","id":"implement-the-test-2","chapter":"20 Two-way ANOVA","heading":"20.7 Implement the test","text":"assumptions appear met well enough, meaning can implement ANOVA. follows (probably easiest bit!):","code":"\n# perform the ANOVA\nanova(lm_exercise)"},{"path":"two-way-anova.html","id":"interpret-output-and-present-results","chapter":"20 Two-way ANOVA","heading":"20.8 Interpret output and present results","text":"Performing ANOVA gives us following output:row table different effects ‚Äôve asked R consider. last column important one contains p-values (although also need F-values degrees freedom reporting purposes). need look interaction row first.sex:exercise p-value 0.028 (smaller 0.05) can conclude interaction sex exercise significant.must stop.top two lines (corresponding effects sex exercise) meaningless now p-values reported utterly redundant (particular way care p-values small).model significant interaction logically impossible meaningfully interpret main effects.report follows:two-way ANOVA test showed significant interaction effects sex Exercise Weight (F = 5.8521, df = 1,16, p = 0.028). Exercise associated small loss weight males larger loss weight females.","code":"## Analysis of Variance Table\n## \n## Response: weight\n##              Df Sum Sq Mean Sq F value    Pr(>F)    \n## sex           1 607.20  607.20 43.1144 6.493e-06 ***\n## exercise      1 184.83  184.83 13.1240  0.002287 ** \n## sex:exercise  1  82.42   82.42  5.8521  0.027839 *  \n## Residuals    16 225.34   14.08                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"two-way-anova.html","id":"exercise-cells","chapter":"20 Two-way ANOVA","heading":"20.9 Exercise: Cells","text":"Exercise 20.1  Cell growthThese data/tidy/CS4-cells.csv data fictional experiment involves looking effect different concentrations substance growth rate two different cell types (annoyingly vague know ‚Äì suggestions context welcome !). two cell types three concentrations.cell type control experiment substance added (.e.¬†concentration none); low concentration substance high concentration substance. cells called B.\ncombination cell type substance concentration add substance individual cell petri dish 8 hours, count number cells dish (may well biologically weird/impossible ‚Äì suggestions welcome). experiment repeated three times.cell type control experiment substance added (.e.¬†concentration none); low concentration substance high concentration substance. cells called B.\ncombination cell type substance concentration add substance individual cell petri dish 8 hours, count number cells dish (may well biologically weird/impossible ‚Äì suggestions welcome). experiment repeated three times.Questions answer:Visualise data using boxplots interaction plots.appear interaction?Carry two-way ANOVA test.Check assumptions.can conclude? (Write sentence summarise).Let‚Äôs look interaction plots. ‚Äôre omitting actual data just plotting mean values:‚Äôre constructed box plots ‚Äôve also constructed two interaction plots. needed one interaction plot find can quite useful look data different angles. interaction plots suggest interaction lines plots aren‚Äôt parallel. Looking interaction plot concentration x-axis, appears non-difference cell types concentration none, difference cell types concentration low high.First need define model:Next, check assumptions:, actually look pretty good, although first glance might bit worried apparent heterogeneity variance. last group Residual plot appear spread 5 groups. echoed Location-Scale plot, red line kicks end. Whilst technically signify heterogeneity variance aren‚Äôt worried three data points per group. low number data points per group get one data point little bit extreme others (purely chance) large impact perception homogeneity variance. data points group certain observed heterogeneity variance true feature underlying parent population (therefore problem) rather just caused single random point (therefore problem).Let‚Äôs carry two-way ANOVA:definitely significant interaction concentration cell_type.","code":"\n# read in the data\ncells <- read_csv(\"data/tidy/CS4-cells.csv\")\n\n# let's have a peek at the data\ncells## # A tibble: 18 √ó 4\n##       id cell_type concentration cell_number\n##    <dbl> <chr>     <chr>               <dbl>\n##  1     1 A         none                    7\n##  2     2 A         none                    9\n##  3     3 A         none                    4\n##  4     4 B         none                    5\n##  5     5 B         none                    8\n##  6     6 B         none                    9\n##  7     7 A         low                    22\n##  8     8 A         low                    28\n##  9     9 A         low                    26\n## 10    10 B         low                    12\n## 11    11 B         low                    17\n## 12    12 B         low                    14\n## 13    13 A         high                   89\n## 14    14 A         high                   78\n## 15    15 A         high                   83\n## 16    16 B         high                   48\n## 17    17 B         high                   44\n## 18    18 B         high                   45\ncells %>% \n  ggplot(aes(x = concentration, y = cell_number)) +\n  geom_boxplot()\ncells %>% \n  ggplot(aes(x = cell_type, y = cell_number)) +\n  geom_boxplot()\n# by cell type\ncells %>% \n  ggplot(aes(x = concentration,\n             y = cell_number,\n             colour = cell_type, group = cell_type)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_colour_brewer(palette = \"Dark2\")\n# by concentration\ncells %>% \n  ggplot(aes(x = cell_type,\n             y = cell_number,\n             colour = concentration, group = concentration)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_colour_brewer(palette = \"Dark2\")\n# define the linear model, with interaction term\nlm_cells <- lm(cell_number ~ concentration * cell_type,\n          data = cells)\nlm_cells %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\n# perform the ANOVA\nanova(lm_cells)## Analysis of Variance Table\n## \n## Response: cell_number\n##                         Df  Sum Sq Mean Sq F value    Pr(>F)    \n## concentration            2 10932.1  5466.1 537.645 1.807e-12 ***\n## cell_type                1  1152.0  1152.0 113.311 1.816e-07 ***\n## concentration:cell_type  2  1158.3   579.2  56.967 7.485e-07 ***\n## Residuals               12   122.0    10.2                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"two-way-anova.html","id":"load-the-data","chapter":"20 Two-way ANOVA","heading":"20.9.1 Load the data","text":"","code":"\n# read in the data\ncells <- read_csv(\"data/tidy/CS4-cells.csv\")\n\n# let's have a peek at the data\ncells## # A tibble: 18 √ó 4\n##       id cell_type concentration cell_number\n##    <dbl> <chr>     <chr>               <dbl>\n##  1     1 A         none                    7\n##  2     2 A         none                    9\n##  3     3 A         none                    4\n##  4     4 B         none                    5\n##  5     5 B         none                    8\n##  6     6 B         none                    9\n##  7     7 A         low                    22\n##  8     8 A         low                    28\n##  9     9 A         low                    26\n## 10    10 B         low                    12\n## 11    11 B         low                    17\n## 12    12 B         low                    14\n## 13    13 A         high                   89\n## 14    14 A         high                   78\n## 15    15 A         high                   83\n## 16    16 B         high                   48\n## 17    17 B         high                   44\n## 18    18 B         high                   45"},{"path":"two-way-anova.html","id":"visualise-the-data","chapter":"20 Two-way ANOVA","heading":"20.9.2 Visualise the data","text":"Let‚Äôs look interaction plots. ‚Äôre omitting actual data just plotting mean values:‚Äôre constructed box plots ‚Äôve also constructed two interaction plots. needed one interaction plot find can quite useful look data different angles. interaction plots suggest interaction lines plots aren‚Äôt parallel. Looking interaction plot concentration x-axis, appears non-difference cell types concentration none, difference cell types concentration low high.","code":"\ncells %>% \n  ggplot(aes(x = concentration, y = cell_number)) +\n  geom_boxplot()\ncells %>% \n  ggplot(aes(x = cell_type, y = cell_number)) +\n  geom_boxplot()\n# by cell type\ncells %>% \n  ggplot(aes(x = concentration,\n             y = cell_number,\n             colour = cell_type, group = cell_type)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_colour_brewer(palette = \"Dark2\")\n# by concentration\ncells %>% \n  ggplot(aes(x = cell_type,\n             y = cell_number,\n             colour = concentration, group = concentration)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_colour_brewer(palette = \"Dark2\")"},{"path":"two-way-anova.html","id":"assumptions-17","chapter":"20 Two-way ANOVA","heading":"20.9.3 Assumptions","text":"First need define model:Next, check assumptions:, actually look pretty good, although first glance might bit worried apparent heterogeneity variance. last group Residual plot appear spread 5 groups. echoed Location-Scale plot, red line kicks end. Whilst technically signify heterogeneity variance aren‚Äôt worried three data points per group. low number data points per group get one data point little bit extreme others (purely chance) large impact perception homogeneity variance. data points group certain observed heterogeneity variance true feature underlying parent population (therefore problem) rather just caused single random point (therefore problem).","code":"\n# define the linear model, with interaction term\nlm_cells <- lm(cell_number ~ concentration * cell_type,\n          data = cells)\nlm_cells %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)"},{"path":"two-way-anova.html","id":"implement-the-test-3","chapter":"20 Two-way ANOVA","heading":"20.9.4 Implement the test","text":"Let‚Äôs carry two-way ANOVA:","code":"\n# perform the ANOVA\nanova(lm_cells)## Analysis of Variance Table\n## \n## Response: cell_number\n##                         Df  Sum Sq Mean Sq F value    Pr(>F)    \n## concentration            2 10932.1  5466.1 537.645 1.807e-12 ***\n## cell_type                1  1152.0  1152.0 113.311 1.816e-07 ***\n## concentration:cell_type  2  1158.3   579.2  56.967 7.485e-07 ***\n## Residuals               12   122.0    10.2                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"two-way-anova.html","id":"interpret-the-output-and-report-the-results","chapter":"20 Two-way ANOVA","heading":"20.9.5 Interpret the output and report the results","text":"definitely significant interaction concentration cell_type.","code":""},{"path":"two-way-anova.html","id":"exercise-tulips","chapter":"20 Two-way ANOVA","heading":"20.10 Exercise: Tulips","text":"Exercise 20.2  Blooms growing conditionsThe data/tidy/CS4-tulip.csv dataset contains information experiment determine best conditions growing tulips (well someone care sorts things!). average number flower heads (blooms) recorded 27 different plots. plot experienced one three different watering regimes one three different shade regimes.Investigate number blooms affected different growing conditions.data set watering regime (water) shading regime (shade) encoded numerical values. However, numbers actually categories, representing amount water/shade., don‚Äôt want treat numbers factors. can convert columns using as_factor() function. ‚Äôd like keep referring columns factors, update existing data set.dataset three variables; blooms (response variable) water shade (two potential predictor variables).always ‚Äôll visualise data first:, interaction plots suggest might interaction . Digging little deeper descriptive perspective, looks though water regime 1 behaving differently water regimes 2 3 different shade conditions.First need define model:Next, check assumptions:actually OK. Point number 8 messing homogeneity variance assumption little bit, since ‚Äôs one point won‚Äôt worry . two-way ANOVA analysis still cards.Let‚Äôs carry two-way ANOVA.appear significant interaction water shade expected.","code":"\n# read in the data\ntulip <- read_csv(\"data/tidy/CS4-tulip.csv\")## Rows: 27 Columns: 3## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## dbl (3): water, shade, blooms## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n# have a quick look at the data\ntulip## # A tibble: 27 √ó 3\n##    water shade blooms\n##    <dbl> <dbl>  <dbl>\n##  1     1     1    0  \n##  2     1     2    0  \n##  3     1     3  111. \n##  4     2     1  183. \n##  5     2     2   59.2\n##  6     2     3   76.8\n##  7     3     1  225. \n##  8     3     2   83.8\n##  9     3     3  135. \n## 10     1     1   80.1\n## # ‚Ä¶ with 17 more rows\n# convert watering and shade regimes to factor\ntulip <- tulip %>% \n  mutate(water = as_factor(water),\n         shade = as_factor(shade))\n# by watering regime\ntulip %>% \n  ggplot(aes(x = water, y = blooms)) +\n  geom_boxplot()\n# by shading regime\ntulip %>% \n  ggplot(aes(x = shade, y = blooms)) +\n  geom_boxplot()\n# interaction plot by watering regime\ntulip %>% \n  ggplot(aes(x = shade,\n             y = blooms,\n             colour = water, group = water)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_colour_brewer(palette = \"Dark2\")\n# interaction plot by shade regime\ntulip %>% \n  ggplot(aes(x = water,\n             y = blooms,\n             colour = shade, group = shade)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_colour_brewer(palette = \"Dark2\")\n# define the linear model\nlm_tulip <- lm(blooms ~ water * shade,\n               data = tulip)\nlm_tulip %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\n# perform the ANOVA\nanova(lm_tulip)## Analysis of Variance Table\n## \n## Response: blooms\n##             Df Sum Sq Mean Sq F value    Pr(>F)    \n## water        2 103626   51813 22.0542 1.442e-05 ***\n## shade        2  36376   18188  7.7417   0.00375 ** \n## water:shade  4  41058   10265  4.3691   0.01211 *  \n## Residuals   18  42288    2349                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"two-way-anova.html","id":"load-the-data-1","chapter":"20 Two-way ANOVA","heading":"20.10.1 Load the data","text":"data set watering regime (water) shading regime (shade) encoded numerical values. However, numbers actually categories, representing amount water/shade., don‚Äôt want treat numbers factors. can convert columns using as_factor() function. ‚Äôd like keep referring columns factors, update existing data set.dataset three variables; blooms (response variable) water shade (two potential predictor variables).","code":"\n# read in the data\ntulip <- read_csv(\"data/tidy/CS4-tulip.csv\")## Rows: 27 Columns: 3## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## dbl (3): water, shade, blooms## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n# have a quick look at the data\ntulip## # A tibble: 27 √ó 3\n##    water shade blooms\n##    <dbl> <dbl>  <dbl>\n##  1     1     1    0  \n##  2     1     2    0  \n##  3     1     3  111. \n##  4     2     1  183. \n##  5     2     2   59.2\n##  6     2     3   76.8\n##  7     3     1  225. \n##  8     3     2   83.8\n##  9     3     3  135. \n## 10     1     1   80.1\n## # ‚Ä¶ with 17 more rows\n# convert watering and shade regimes to factor\ntulip <- tulip %>% \n  mutate(water = as_factor(water),\n         shade = as_factor(shade))"},{"path":"two-way-anova.html","id":"visualise-the-data-1","chapter":"20 Two-way ANOVA","heading":"20.10.2 Visualise the data","text":"always ‚Äôll visualise data first:, interaction plots suggest might interaction . Digging little deeper descriptive perspective, looks though water regime 1 behaving differently water regimes 2 3 different shade conditions.","code":"\n# by watering regime\ntulip %>% \n  ggplot(aes(x = water, y = blooms)) +\n  geom_boxplot()\n# by shading regime\ntulip %>% \n  ggplot(aes(x = shade, y = blooms)) +\n  geom_boxplot()\n# interaction plot by watering regime\ntulip %>% \n  ggplot(aes(x = shade,\n             y = blooms,\n             colour = water, group = water)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_colour_brewer(palette = \"Dark2\")\n# interaction plot by shade regime\ntulip %>% \n  ggplot(aes(x = water,\n             y = blooms,\n             colour = shade, group = shade)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_colour_brewer(palette = \"Dark2\")"},{"path":"two-way-anova.html","id":"assumptions-18","chapter":"20 Two-way ANOVA","heading":"20.10.3 Assumptions","text":"First need define model:Next, check assumptions:actually OK. Point number 8 messing homogeneity variance assumption little bit, since ‚Äôs one point won‚Äôt worry . two-way ANOVA analysis still cards.","code":"\n# define the linear model\nlm_tulip <- lm(blooms ~ water * shade,\n               data = tulip)\nlm_tulip %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)"},{"path":"two-way-anova.html","id":"implement-the-test-4","chapter":"20 Two-way ANOVA","heading":"20.10.4 Implement the test","text":"Let‚Äôs carry two-way ANOVA.","code":"\n# perform the ANOVA\nanova(lm_tulip)## Analysis of Variance Table\n## \n## Response: blooms\n##             Df Sum Sq Mean Sq F value    Pr(>F)    \n## water        2 103626   51813 22.0542 1.442e-05 ***\n## shade        2  36376   18188  7.7417   0.00375 ** \n## water:shade  4  41058   10265  4.3691   0.01211 *  \n## Residuals   18  42288    2349                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"two-way-anova.html","id":"interpret-the-output-and-report-results","chapter":"20 Two-way ANOVA","heading":"20.10.5 Interpret the output and report results","text":"appear significant interaction water shade expected.","code":""},{"path":"two-way-anova.html","id":"key-points-6","chapter":"20 Two-way ANOVA","heading":"20.11 Key points","text":"two-way ANOVA used two categorical variables single continuous variableWe can visually check interactions categorical variables using interaction plotsThe two-way ANOVA type linear model assumes following:\ndata systematic pattern\nresiduals normally distributed\nresiduals homogeneity variance\nfit depend single point (single point high leverage)\ndata systematic patternthe residuals normally distributedthe residuals homogeneity variancethe fit depend single point (single point high leverage)","code":""},{},{"path":"linear-regression-with-grouped-data.html","id":"linear-regression-with-grouped-data","chapter":"21 Linear regression with grouped data","heading":"21 Linear regression with grouped data","text":"","code":""},{"path":"linear-regression-with-grouped-data.html","id":"objectives-9","chapter":"21 Linear regression with grouped data","heading":"21.1 Objectives","text":"QuestionsHow perform linear regression grouped data?ObjectivesBe able perform linear regression grouped data RCalculate linear regression individual groups visualise dataUnderstand able create equations regression lineBe able deal interactions context","code":""},{"path":"linear-regression-with-grouped-data.html","id":"purpose-and-aim-7","chapter":"21 Linear regression with grouped data","heading":"21.2 Purpose and aim","text":"linear regression analysis grouped data used one categorical predictor variable (factor), one continuous predictor variable. response variable must still continuous however.example experiment looks light intensity woodland, light intensity (continuous: lux) affected height measurement taken, recorded depth measured top canopy (continuous: metres) type woodland (categorical: Conifer Broad leaf).analysing type data want know:difference groups?continuous predictor variable affect continuous response variable (canopy depth affect measured light intensity?)interaction two predictor variables? interaction display difference slopes regression lines group, example perhaps conifer dataset significantly steeper line broad leaf woodland dataset.case, interaction means regression lines slope.\nEssentially analysis identical two-way ANOVA (R doesn‚Äôt really notice difference).plot data visually inspect .test interaction doesn‚Äôt exist :\ncan test see either predictor variable effect (.e.¬†regression lines different intercepts? common gradient significantly different zero?)\ncan test see either predictor variable effect (.e.¬†regression lines different intercepts? common gradient significantly different zero?)first consider visualise data carrying appropriate statistical test.","code":""},{"path":"linear-regression-with-grouped-data.html","id":"section-commands-11","chapter":"21 Linear regression with grouped data","heading":"21.3 Section commands","text":"New commands used section:","code":""},{"path":"linear-regression-with-grouped-data.html","id":"data-and-hypotheses-10","chapter":"21 Linear regression with grouped data","heading":"21.4 Data and hypotheses","text":"data stored data/tidy/CS4-treelight.csv.Read data inspect :treelight data frame four variables; id, light, depth species. light continuous response variable, depth continuous predictor variable species categorical predictor variable.","code":"\n# read in the data\ntreelight <- read_csv(\"data/tidy/CS4-treelight.csv\")\n\n# inspect the data\ntreelight## # A tibble: 23 √ó 4\n##       id light depth species\n##    <dbl> <dbl> <dbl> <chr>  \n##  1     1 4106.  1    Conifer\n##  2     2 4934.  1.75 Conifer\n##  3     3 4417.  2.5  Conifer\n##  4     4 4529.  3.25 Conifer\n##  5     5 3443.  4    Conifer\n##  6     6 4640.  4.75 Conifer\n##  7     7 3082.  5.5  Conifer\n##  8     8 2368.  6.25 Conifer\n##  9     9 2777.  7    Conifer\n## 10    10 2419.  7.75 Conifer\n## # ‚Ä¶ with 13 more rows"},{"path":"linear-regression-with-grouped-data.html","id":"summarise-and-visualise-11","chapter":"21 Linear regression with grouped data","heading":"21.5 Summarise and visualise","text":"looks like slight negative correlation depth light intensity, light intensity reducing canopy depth increases. useful plot regression lines plot. can follows, updating code:Looking plot, doesn‚Äôt appear significant interaction woodland type (Broadleaf Conifer) depth light measurements taken (depth) amount light intensity getting canopy gradients two lines appear similar. appear noticeable slope lines lines look though different intercepts. suggests isn‚Äôt interaction depth species significant effect Light independently.","code":"\n# plot the data\ntreelight %>% \n  ggplot(aes(x = depth, y = light, colour = species)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Depth (m)\",\n       y = \"Light intensity (lux)\")\n# plot the data\ntreelight %>% \n  ggplot(aes(x = depth, y = light, colour = species)) +\n  geom_point() +\n  # add regression lines\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Depth (m)\",\n       y = \"Light intensity (lux)\")"},{"path":"linear-regression-with-grouped-data.html","id":"implement-the-test-5","chapter":"21 Linear regression with grouped data","heading":"21.6 Implement the test","text":"case ‚Äôre going implement test checking assumptions (know, let‚Äôs live little!). ‚Äôll find soon‚Ä¶can test possible interaction formally:Remember depth * species shorthand way writing full set depth + species + depth:species terms R .e. main effects interaction effect.","code":"\nanova(lm(light ~ depth * species,\n         data = treelight))"},{"path":"linear-regression-with-grouped-data.html","id":"interpret-output-and-present-results-1","chapter":"21 Linear regression with grouped data","heading":"21.7 Interpret output and present results","text":"gives following output:two-way ANOVA row table different effects ‚Äôve asked R consider. last column important one contains p-values. need look interaction row first.\ndepth:species p-value 0.393 (bigger 0.05) can conclude interaction depth species isn‚Äôt significant. can now consider whether predictor variables independently effect. depth species small p-values (2.86x10-9 4.13x10 -11) can conclude significant effect light.means two regression lines non-zero slope, different intercepts. now like know values .","code":"## Analysis of Variance Table\n## \n## Response: light\n##               Df   Sum Sq  Mean Sq  F value    Pr(>F)    \n## depth          1 30812910 30812910 107.8154 2.861e-09 ***\n## species        1 51029543 51029543 178.5541 4.128e-11 ***\n## depth:species  1   218138   218138   0.7633    0.3932    \n## Residuals     19  5430069   285793                       \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"linear-regression-with-grouped-data.html","id":"finding-intercept-values","chapter":"21 Linear regression with grouped data","heading":"21.7.1 Finding intercept values","text":"Unfortunately, R doesn‚Äôt make obvious easy us deciphering required getting right.\nsimple straight line linear regression conifer dataset , output relatively straightforward.can interpret meaning intercept regression line 5014 coefficient depth variable (number front equation) -292.2., equation regression line given :\\[\\begin{equation}\nLight = 5014 + -292.2 \\cdot Depth\n\\end{equation}\\]came fitting simple linear model using conifer dataset, meaning every extra 1 m depth forest canopy lose 292.2 lux light.looked full dataset, found interaction wasn‚Äôt important. means model two distinct intercepts single slope (‚Äôs get linear regression without interaction), need ask R calculate specific combination. command simply:Notice + symbol argument, opposed * symbol used earlier. means explicitly including interaction term fit, consequently forcing R calculate equation lines gradient.Ideally like R give us two equations, one forest type, four parameters total.\nUnfortunately, R parsimonious doesn‚Äôt . Instead R gives three coefficients, require bit interpretation.first two numbers R returns (underneath Intercept depth) exact intercept slope coefficients one lines (case correspond data Broadleaf woodlands).coefficients belonging line, R uses first two coefficients baseline values expresses coefficients relative ones. R also doesn‚Äôt tell explicitly group using baseline reference group! (mention R can helpful times üòâ?), decipher output?First, need work group used baseline.group comes first alphabetically, BroadleafThe way check look see group mentioned table. Conifer mentioned (SpeciesConifer heading) baseline group Broadleaf.means intercept value depth coefficient correspond Broadleaf group result know equation one lines :Broadleaf:\\[\\begin{equation}\nLight = 7962 + -262.2 \\cdot Depth\n\\end{equation}\\]example know gradient lines (explicitly asked R include interaction), need find intercept value Conifer group. Unfortunately, final value given underneath SpeciesConifer give intercept Conifer, instead tells difference Conifer group intercept baseline intercept .e.¬†equation regression line conifer woodland given :\\[\\begin{equation}\nLight = (7962 + -3113) + -262.2 \\cdot Depth\n\\end{equation}\\]\\[\\begin{equation}\nLight = 4829 + -262.2 \\cdot Depth\n\\end{equation}\\]","code":"\n# filter the Conifer data and fit a linear model\ntreelight %>% \n  filter(species == \"Conifer\") %>% \n  lm(light ~ depth, data = .)## \n## Call:\n## lm(formula = light ~ depth, data = .)\n## \n## Coefficients:\n## (Intercept)        depth  \n##      5014.0       -292.2\nlm(light ~ depth + species,\n   data = treelight)## \n## Call:\n## lm(formula = light ~ depth + species, data = treelight)\n## \n## Coefficients:\n##    (Intercept)           depth  speciesConifer  \n##         7962.0          -262.2         -3113.0"},{"path":"linear-regression-with-grouped-data.html","id":"adding-custom-regression-lines","chapter":"21 Linear regression with grouped data","heading":"21.7.2 Adding custom regression lines","text":"example determined interaction term species:depth significant. good visualise model without interaction term.relatively straightforward understand output model bit better.First , load broom library. part tidyverse, don‚Äôt install . loaded default, hence us loading . broom changes format many common base R outputs tidy format, can work output analyses easily.function use called augment(). take model object dataset adds information observation dataset.output shows us lots data. original light values light column ‚Äôs species depth. added information fitted (predicted) values based light ~ depth + species model defined.fitted predicted values .fitted column, corresponding residuals .resid column. Remember, data = predicted values + error, add .fitted + resid end original data .Using information can now plot regression lines species:Lastly, want plot data regression lines together, change code follows:","code":"\n# define the model without interaction term\nlm_additive <- lm(light ~ species + depth,\n                  data = treelight)\n\n# load the broom package\nlibrary(broom)\n\n# augment the model\nlm_additive %>% augment()## # A tibble: 23 √ó 9\n##    light species depth .fitted .resid   .hat .sigma .cooksd .std.resid\n##    <dbl> <chr>   <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>      <dbl>\n##  1 4106. Conifer  1      4587.  -481. 0.191    531. 0.0799      -1.01 \n##  2 4934. Conifer  1.75   4390.   544. 0.156    528. 0.0766       1.11 \n##  3 4417. Conifer  2.5    4194.   223. 0.128    542. 0.00985      0.449\n##  4 4529. Conifer  3.25   3997.   532. 0.105    530. 0.0440       1.06 \n##  5 3443. Conifer  4      3800.  -358. 0.0896   538. 0.0163      -0.706\n##  6 4640. Conifer  4.75   3604.  1037. 0.0801   486. 0.120        2.03 \n##  7 3082. Conifer  5.5    3407.  -325. 0.0769   540. 0.0113      -0.637\n##  8 2368. Conifer  6.25   3210.  -842. 0.0801   507. 0.0793      -1.65 \n##  9 2777. Conifer  7      3014.  -237. 0.0896   542. 0.00719     -0.468\n## 10 2419. Conifer  7.75   2817.  -398. 0.105    537. 0.0247      -0.792\n## # ‚Ä¶ with 13 more rows\n# plot the regression lines by species\nlm_additive %>%\n  augment() %>% \n  ggplot(aes(x = depth, y = .fitted, colour = species)) +\n  geom_line() +\n  scale_color_brewer(palette = \"Dark2\")\n# plot the regression lines\nlm_additive %>%\n  augment() %>% \n  ggplot(aes(x = depth, y = .fitted, colour = species)) +\n  # add the original data points\n  geom_point(data = treelight,\n             aes(x = depth, y = light, colour = species)) +\n  geom_line() +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Depth (m)\",\n       y = \"Light intensity (lux)\")"},{"path":"linear-regression-with-grouped-data.html","id":"assumptions-19","chapter":"21 Linear regression with grouped data","heading":"21.8 Assumptions","text":"case first wanted check interaction significant, prior checking assumptions. checked assumptions first, done one full model (interaction), done ANOVA everything OK. found interaction significant, meaning ‚Äôd re-check assumptions new model. order bit less important . main thing check assumptions report !Anyway, hopefully ‚Äôve got gist checking assumptions linear models now: diagnostic plots!Residuals plot looks OK, systematic pattern.Q-Q plot isn‚Äôt perfect, ‚Äôm happy normality assumption.Location-Scale plot OK, slight suggestion heterogeneity variance, nothing worried .Cook‚Äôs D plot shows points OKWoohoo!","code":"\nlm_additive %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)"},{"path":"linear-regression-with-grouped-data.html","id":"dealing-with-interaction","chapter":"21 Linear regression with grouped data","heading":"21.9 Dealing with interaction","text":"significant interaction two predictor variables (example, light intensity dropped significantly faster conifer woods broad leaf woods, addition lower overall, looking two equations linear regression, time gradients vary well.\ncase interaction important need output linear regression explicitly includes interaction term:written using short-hand:really absolutely difference end result.\nEither way gives us following output:Broadleaf line used baseline regression can read values intercept slope directly:Broadleaf:\n\\[\\begin{equation}\nLight = 7798.57 + -221.13 \\cdot Depth\n\\end{equation}\\]Note different previous section, allowing interaction fitted values change.conifer line different intercept value different gradient value. value underneath speciesConifer gives us difference intercept conifer line broad leaf line. new, additional term depth:speciesConifer tells us coefficient depth varies conifer line .e.¬†gradient different. Putting two together gives us following equation regression line conifer woodland:Conifer:\n\\[\\begin{equation}\nLight = (7798.57 + -2784.58) + (-221.13 + -71.04) \\cdot Depth\n\\end{equation}\\]\\[\\begin{equation}\nLight = 5014 + -292.2 \\cdot Depth\n\\end{equation}\\]also happen exactly regression lines get calculating linear regression group‚Äôs data separately.","code":"\nlm(light ~ depth + species + depth:species,\n   data = treelight)\nlm(light ~ depth * species,\n   data = treelight)## \n## Call:\n## lm(formula = light ~ depth * species, data = treelight)\n## \n## Coefficients:\n##          (Intercept)                 depth        speciesConifer  \n##              7798.57               -221.13              -2784.58  \n## depth:speciesConifer  \n##               -71.04"},{"path":"linear-regression-with-grouped-data.html","id":"exercise-clover-and-yarrow","chapter":"21 Linear regression with grouped data","heading":"21.10 Exercise: Clover and yarrow","text":"Exercise 21.1  Clover yarrow field trialsThe data/tidy/CS4-clover.csv dataset contains information field trials three different farms (, B C). farm recorded yield clover ten fields along density yarrow stalks field.Investigate clover yield affected yarrow stalk density. evidence competition two species?difference farms?dataset three variables; yield (response variable), yarrow (continuous predictor variable) farm (categorical predictor variables). always ‚Äôll visualise data first:Looking plot stands, ‚Äôs pretty clear yarrow density significant effect yield, ‚Äôs pretty hard see plot whether effect farm, whether interaction. order work ‚Äôll want add regression lines farm separately.regression lines close together, looks much isn‚Äôt interaction, also isn‚Äôt effect farm. Let‚Äôs carry analysis:confirms suspicions looking plot. isn‚Äôt interaction yarrow farm. yarrow density statistically significant effect yield isn‚Äôt difference different farms yields clover.Let‚Äôs check assumptions:borderline case.Normality fine (Q-Q plot)aren‚Äôt highly influential points (Cook‚Äôs D plot)strong suggestion heterogeneity variance (Location-Scale plot). points relatively close regression lines, much great spread points low yarrow density (corresponds high yield values, predicted values correspond ).Finally, slight suggestion data might linear, might curve slightly (Residual plot).two options; arguably OK real life.can claim assumptions well enough met just report analysis ‚Äôve just done.can decide analysis appropriate look options.\ncan try transform data taking logs yield. might fix problems: taking logs response variable effect improving heterogeneity variance Residuals plot spread right vs.¬†left (like ). also appropriate think true relationship response predictor variables exponential rather linear (might ). capabilities try option.\ntry permutation based approach (beyond remit course, actually bit tricky situation). wouldn‚Äôt address non-linearity deal variance assumption.\ncome specific functional, mechanistic relationship yarrow density clover yield based upon aspects biology. example might threshold effect yarrow densities particular value, clover yields unaffected, soon yarrow values get threshold clover yield decreases (maybe even linearly). require much better understanding clover-yarrow dynamics (personally know little).\ncan try transform data taking logs yield. might fix problems: taking logs response variable effect improving heterogeneity variance Residuals plot spread right vs.¬†left (like ). also appropriate think true relationship response predictor variables exponential rather linear (might ). capabilities try option.try permutation based approach (beyond remit course, actually bit tricky situation). wouldn‚Äôt address non-linearity deal variance assumption.come specific functional, mechanistic relationship yarrow density clover yield based upon aspects biology. example might threshold effect yarrow densities particular value, clover yields unaffected, soon yarrow values get threshold clover yield decreases (maybe even linearly). require much better understanding clover-yarrow dynamics (personally know little).Let‚Äôs quick little transformation data, repeat analysis see assumptions better met time (just hell ):, looks plausible. ‚Äôs noticeable outlier Farm B (data point bottom plot) otherwise see : probably isn‚Äôt interaction; likely effect yarrow log(yield); probably isn‚Äôt difference farms.Let‚Äôs analysis:Woop. good far. conclusions terms significant isn‚Äôt. Now just need check assumptions:Well, actually better set diagnostic plots. Whilst one data point (example Q-Q plot) clear outlier, ignore point plots look better.now know yarrow significant predictor yield ‚Äôre happy assumptions met.","code":"\nclover <- read_csv(\"data/tidy/CS4-clover.csv\")\n# plot the data\nclover %>% \n  ggplot(aes(x = yarrow, y = yield,\n             colour = farm)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Dark2\")\n# plot the data\nclover %>% \n  ggplot(aes(x = yarrow, y = yield,\n             colour = farm, group = farm)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_brewer(palette = \"Dark2\")\nlm_clover <- lm(yield ~ yarrow * farm,\n                data = clover)\n\nanova(lm_clover)## Analysis of Variance Table\n## \n## Response: yield\n##             Df Sum Sq Mean Sq F value    Pr(>F)    \n## yarrow       1 8538.3  8538.3 28.3143 1.847e-05 ***\n## farm         2    3.8     1.9  0.0063    0.9937    \n## yarrow:farm  2  374.7   187.4  0.6213    0.5457    \n## Residuals   24 7237.3   301.6                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nlm_clover %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\n# plot log-transformed data\nclover %>% \n  ggplot(aes(x = yarrow, y = log(yield), colour = farm)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_brewer(palette = \"Dark2\")\n# define linear model\nlm_log_clover <- lm(log(yield) ~ yarrow * farm,\n                    data = clover)\n\nanova(lm_log_clover)## Analysis of Variance Table\n## \n## Response: log(yield)\n##             Df  Sum Sq Mean Sq F value   Pr(>F)    \n## yarrow       1 10.6815 10.6815 27.3233 2.34e-05 ***\n## farm         2  0.0862  0.0431  0.1103   0.8960    \n## yarrow:farm  2  0.8397  0.4199  1.0740   0.3575    \n## Residuals   24  9.3823  0.3909                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nlm_log_clover %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)"},{"path":"linear-regression-with-grouped-data.html","id":"key-points-7","chapter":"21 Linear regression with grouped data","heading":"21.11 Key points","text":"linear regression analysis grouped data used one categorical one continuous predictor variable, together one continuous response variableWe can visualise data plotting regression line together raw dataWhen performing ANOVA, need check interaction termsAgain, check underlying assumptions using diagnostic plotsWe can create equation regression line group data using lm() output","code":""},{},{"path":"cs5-intro.html","id":"cs5-intro","chapter":"22 Introduction","heading":"22 Introduction","text":"","code":""},{"path":"cs5-intro.html","id":"objectives-10","chapter":"22 Introduction","heading":"22.1 Objectives","text":"Aim: introduce R commands constructing linear models multiple continuous categorical variables performing backwards stepwise eliminationBy end practical participants able achieve following:Construct linear model three continuous categorical variables\nUnderstand include exclude interaction terms\nUnderstand interpret output\nUnderstand include exclude interaction termsUnderstand interpret outputPerform backwards stepwise elimination produce minimal model","code":""},{"path":"cs5-intro.html","id":"background-4","chapter":"22 Introduction","heading":"22.2 Background","text":"practical divided two main sections. first section explores concept linear model framework revisits work previous practicals. linear model concept expanded systems three predictor variables.second section focuses model selection technique called backwards stepwise elimination. technique allows comparisons made nested models uninformative predictor variables can dropped minimal model remains.Within section worked example exercise.","code":""},{},{"path":"introduction-4.html","id":"introduction-4","chapter":"23 Introduction","heading":"23 Introduction","text":"","code":""},{"path":"introduction-4.html","id":"cs5-datasets","chapter":"23 Introduction","heading":"23.1 Datasets","text":"section uses various data sets. located data/raw/ folder working directory. Please see Datasets information.","code":""},{},{"path":"linear-models.html","id":"linear-models","chapter":"24 Linear models","heading":"24 Linear models","text":"","code":""},{"path":"linear-models.html","id":"objectives-11","chapter":"24 Linear models","heading":"24.1 Objectives","text":"QuestionsHow use linear model framework three predictor variables?ObjectivesBe able expand linear model framework R three predictor variablesDefine equation regression line categorical variableBe able construct analyse possible combination predictor variables data","code":""},{"path":"linear-models.html","id":"purpose-and-aim-8","chapter":"24 Linear models","heading":"24.2 Purpose and aim","text":"Revisiting linear model framework expanding systems three predictor variables.","code":""},{"path":"linear-models.html","id":"section-commands-12","chapter":"24 Linear models","heading":"24.3 Section commands","text":"Commands used section","code":""},{"path":"linear-models.html","id":"data-and-hypotheses-11","chapter":"24 Linear models","heading":"24.4 Data and hypotheses","text":"first section uses following dataset:\ndata/tidy/CS5-H2S.csv. dataset comprising 16 observations three variables (one dependent two predictor). records air pollution caused H2S produced two types waste treatment plants. types treatment plant, obtain eight measurements H2S production (ppm). also obtain information daily temperature.","code":""},{"path":"linear-models.html","id":"summarise-and-visualise-12","chapter":"24 Linear models","heading":"24.5 Summarise and visualise","text":"Let‚Äôs first load data:four columns:id unique ID columntreatment_plant contains name waste treatment plantdaily_temp contains average daily temperature degrees Celsiushydrogen_sulfide contains H2S production (ppm)Next, visualise data:looks though variable treatment_plant effect H2S emissions (one cloud points higher ). also suggestion daily temperature might affect emissions (data sets look like gradient regression line respective cloud might zero) also appears might interaction treatment_plant daily_temperature gradient two regression lines parallel.","code":"\n#load the data\nairpoll <- read_csv(\"data/tidy/CS5-H2S.csv\")\n\n# look at the data\nairpoll## # A tibble: 16 √ó 4\n##       id treatment_plant daily_temp hydrogen_sulfide\n##    <dbl> <chr>                <dbl>            <dbl>\n##  1     1 A                       21             5.22\n##  2     2 A                       22             4.39\n##  3     3 A                       22             5.24\n##  4     4 A                       24             5.04\n##  5     5 A                       27             4.6 \n##  6     6 A                       28             5.04\n##  7     7 A                       29             5.08\n##  8     8 A                       30             3.97\n##  9     9 B                       21             6.06\n## 10    10 B                       21             6.51\n## 11    11 B                       22             6.33\n## 12    12 B                       23             6.01\n## 13    13 B                       28             6.09\n## 14    14 B                       28             6.93\n## 15    15 B                       28             7.6 \n## 16    16 B                       29             7.65\n# plot the data\nairpoll %>% \n  ggplot(aes(x = daily_temp,\n             y = hydrogen_sulfide,\n             colour = treatment_plant,\n             group = treatment_plant)) +\n  geom_point() +\n  # add regression lines\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_brewer(palette = \"Dark2\")"},{"path":"linear-models.html","id":"implemention","chapter":"24 Linear models","heading":"24.6 Implemention","text":"Construct analyse full linear model.construct model main effects interaction term. Remember hydrogen_sulfide ~ treatment_plant * daily_temp short hand version hydrogen_sulfide ~ treatment_plant + daily_temp + treatment_plant:daily_temp.gives us coefficients model:best interpreted using linear model notation:\\[\\begin{equation}\nH_2S = 6.20495 - 0.05448 \\cdot daily\\_temp + \\\\ \\binom{0}{-2.73075}\\binom{treatment\\_plantA}{treatment\\_plantB} + \\\\\n\\binom{0}{0.18141 \\cdot daily\\_temp}\\binom{treatment\\_plantA}{treatment\\_plantB}\n\\end{equation}\\]effectively shorthand writing equation two straight lines (one categorical variable):\\[\\begin{equation}\ntreatment\\_plantA = 6.20495 - 0.05448 \\cdot daily\\_temp\n\\end{equation}\\]\\[\\begin{equation}\ntreatment\\_plantB = 3.4742 + 0.12693 \\cdot daily\\_temp\n\\end{equation}\\]Performing ANOVA full linear model gives following output:can see interaction term appears marginally significant, implying effect temperature hydrogen sulfide production different two different treatment plants.check assumptions lm_full using diagnostic plots:","code":"\n# define the linear model with all terms and interactions\nlm_full <- lm(hydrogen_sulfide ~ treatment_plant * daily_temp,\n              data = airpoll)\n\n# view the model\nlm_full## \n## Call:\n## lm(formula = hydrogen_sulfide ~ treatment_plant * daily_temp, \n##     data = airpoll)\n## \n## Coefficients:\n##                 (Intercept)             treatment_plantB  \n##                     6.20495                     -2.73075  \n##                  daily_temp  treatment_plantB:daily_temp  \n##                    -0.05448                      0.18141## # A tibble: 4 √ó 2\n##   term                        estimate\n##   <chr>                          <dbl>\n## 1 (Intercept)                   6.20  \n## 2 treatment_plantB             -2.73  \n## 3 daily_temp                   -0.0545\n## 4 treatment_plantB:daily_temp   0.181\nanova(lm_full)## Analysis of Variance Table\n## \n## Response: hydrogen_sulfide\n##                            Df  Sum Sq Mean Sq F value    Pr(>F)    \n## treatment_plant             1 13.3225 13.3225 54.1557 8.746e-06 ***\n## daily_temp                  1  0.2316  0.2316  0.9415   0.35104    \n## treatment_plant:daily_temp  1  1.4470  1.4470  5.8822   0.03201 *  \n## Residuals                  12  2.9520  0.2460                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nlm_full %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)"},{"path":"linear-models.html","id":"exploring-models","chapter":"24 Linear models","heading":"24.7 Exploring models","text":"Rather stop however, use concept linear model full potential show can construct analyse possible combination predictor variables dataset. Namely consider following four extra models:","code":""},{"path":"linear-models.html","id":"additive-model","chapter":"24 Linear models","heading":"24.7.1 Additive model","text":"Construct analyse additive linear model.first line creates linear model seeks explain hydrogen_sulfide values purely terms categorical treatment_plant variable continuous daily_temp variable.second line produces following output:gives us coefficients additive model:best interpreted using linear model notation:\\[\\begin{equation}\nH_2S = 3.9 + 0.036 \\cdot daily\\_temp + \\\\\n\\binom{0}{1.8} \\binom{treatment\\_plantA}{treatment\\_plantB}\n\\end{equation}\\]effectively shorthand writing equation two straight lines (one categorical variable):\\[\\begin{equation}\nH_2S(treatment\\_plantA) = 3.9 + 0.036 \\cdot daily\\_temp\n\\end{equation}\\]\\[\\begin{equation}\nH_2S(treatment\\_plantB) = 5.7 + 0.036 \\cdot daily\\_temp\n\\end{equation}\\]important note much coefficients changed (natural assume change model given ‚Äôve altered predictor variables included). striking signs coefficients changed! example, full model saw coefficient treatment_plantB negative (implying general treatment_plantB produced lower H2S values treatment_plantA default) whereas now positive indicating exactly opposite effect. Given difference two models inclusion interaction term saw significant analysis full model, perhaps, surprising dropping term lead different results.just imagine never included first place! looked additive model come completely different conclusions baseline pollution levels plant.3rd line produces following output:can see temperature term significant, whereas treatment_plant term significant indeed.Exercise 24.1  Check assumptions additive model. differ significantly full model?","code":"\n# define the linear model\nlm_add <- lm(hydrogen_sulfide ~ treatment_plant + daily_temp,\n             data = airpoll)\n\n# view the linear model\nlm_add\n\n# perform an ANOVA on the model\nanova(lm_add)## \n## Call:\n## lm(formula = hydrogen_sulfide ~ treatment_plant + daily_temp, \n##     data = airpoll)\n## \n## Coefficients:\n##      (Intercept)  treatment_plantB        daily_temp  \n##          3.90164           1.83861           0.03629## # A tibble: 3 √ó 2\n##   term             estimate\n##   <chr>               <dbl>\n## 1 (Intercept)        3.90  \n## 2 treatment_plantB   1.84  \n## 3 daily_temp         0.0363## Analysis of Variance Table\n## \n## Response: hydrogen_sulfide\n##                 Df  Sum Sq Mean Sq F value    Pr(>F)    \n## treatment_plant  1 13.3225 13.3225 39.3702 2.858e-05 ***\n## daily_temp       1  0.2316  0.2316  0.6845     0.423    \n## Residuals       13  4.3991  0.3384                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"linear-models.html","id":"revisiting-anova","chapter":"24 Linear models","heading":"24.7.2 Revisiting ANOVA","text":"Construct analyse effect treatment_plant:third line gives us model coefficients:case tells us means groups. (Intercept) mean treatment_plantA H2S data (4.8225) whilst treatment_plantB tells us mean treatment plant B H2S data 1.8250 intercept value .e. mean treatment_plantB 4.8225 + 1.8250 = 6.6475.last line gives us normal ANOVA table testing whether means two groups differ significantly .Exercise 24.2  Check assumptions plant model. differ significantly previous models?","code":"\n# visualise the data\nairpoll %>% \n  ggplot(aes(x = treatment_plant, y = hydrogen_sulfide)) +\n  geom_boxplot() +\n  # add the data points and ensure they are jittered\n  # so they do not overlap\n  geom_jitter(width = 0.1)\n# define the linear model\nlm_plant <- lm(hydrogen_sulfide ~ treatment_plant,\n               data = airpoll)\n\n# view the linear model\nlm_plant\n\n# perform an ANOVA on the model\nanova(lm_plant)## \n## Call:\n## lm(formula = hydrogen_sulfide ~ treatment_plant, data = airpoll)\n## \n## Coefficients:\n##      (Intercept)  treatment_plantB  \n##            4.823             1.825## Analysis of Variance Table\n## \n## Response: hydrogen_sulfide\n##                 Df  Sum Sq Mean Sq F value    Pr(>F)    \n## treatment_plant  1 13.3225 13.3225  40.278 1.809e-05 ***\n## Residuals       14  4.6307  0.3308                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"linear-models.html","id":"revisiting-regression","chapter":"24 Linear models","heading":"24.7.3 Revisiting regression","text":"Construct simple linear regression model, H2S depends average daily temperature:model gives us coefficients equation regression lineIn case tells us intercept (Intercept) gradient (daily_temp) regression line.last line gives us ANOVA analysis:Temperature clearly significant effect.Exercise 24.3  , check assumptions temperature model. differ significantly previous models?","code":"\n# plot the data\nairpoll %>% \n  ggplot(aes(x = daily_temp, y = hydrogen_sulfide)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n# define the linear model\nlm_temp <- lm(hydrogen_sulfide ~ daily_temp,\n              data = airpoll)\n\n# view the model\nlm_temp\n\n# perform an ANOVA on the model\nanova(lm_temp)## \n## Call:\n## lm(formula = hydrogen_sulfide ~ daily_temp, data = airpoll)\n## \n## Coefficients:\n## (Intercept)   daily_temp  \n##     5.21465      0.02066## Analysis of Variance Table\n## \n## Response: hydrogen_sulfide\n##            Df  Sum Sq Mean Sq F value Pr(>F)\n## daily_temp  1  0.0753  0.0753   0.059 0.8117\n## Residuals  14 17.8779  1.2770"},{"path":"linear-models.html","id":"the-null-model","chapter":"24 Linear models","heading":"24.7.4 The null model","text":"Construct analyse null model:lm_null fit null model data (effectively just finding mean H2S values dataset)null model gives us mean H2S values (.e.¬†coefficient null model)null model rarely analysed sake instead used reference point sophisticated model selection techniques.","code":"\n# visualise the data\nairpoll %>% \n  ggplot(aes(y = hydrogen_sulfide)) +\n  geom_boxplot()\n# define the null model\nlm_null <- lm(hydrogen_sulfide ~ 1,\n              data = airpoll)\n\n# view the model\nlm_null## \n## Call:\n## lm(formula = hydrogen_sulfide ~ 1, data = airpoll)\n## \n## Coefficients:\n## (Intercept)  \n##       5.735"},{"path":"linear-models.html","id":"exercise-trees","chapter":"24 Linear models","heading":"24.8 Exercise: trees","text":"Exercise 24.4  Trees: example continuous variablesUse internal dataset trees. data frame 31 observations 3 continuous variables. variables height Height, diameter Girth timber volume Volume 31 felled black cherry trees.Investigate relationship Volume (dependent variable) Height Girth (predictor variables).variables continuous isn‚Äôt way producing 2D plot three variables visualisation purposes using R‚Äôs standard plotting functions.construct four linear models\nAssume volume depends Height, Girth interaction Girth Height\nAssume Volume depends Height Girth isn‚Äôt interaction .\nAssume Volume depends Girth (plot result, regression line).\nAssume Volume depends Height (plot result, regression line).\nAssume volume depends Height, Girth interaction Girth HeightAssume Volume depends Height Girth isn‚Äôt interaction .Assume Volume depends Girth (plot result, regression line).Assume Volume depends Height (plot result, regression line).linear model write algebraic equation linear model produces relates volume two continuous predictor variables.Check assumptions model. concerns?NB: two continuous predictors, interaction term simply two values multiplied together (Girth:Height means Girth x Height)Use equations calculate predicted volume tree diameter 20 inches height 67 feet case.Let‚Äôs construct four linear models turn.r commands :can use output get following equation:Volume = 69.40 + -1.30 \\(\\cdot\\) Height + -5.86 \\(\\cdot\\) Girth + 0.13 \\(\\cdot\\) Height \\(\\cdot\\) GirthIf stick numbers (Girth = 20 Height = 67) get following equation:Volume = 69.40 + -1.30 \\(\\cdot\\) 67 + -5.86 \\(\\cdot\\) 20 + 0.13 \\(\\cdot\\) 67 \\(\\cdot\\) 20Volume = 45.81Here note interaction term just requires us multiple three numbers together (haven‚Äôt looked continuous predictors examples exercise included check see whole process making sense).look diagnostic plots model using following commands get:assumptions OK.suggestion heterogeneity variance (variance lower small large fitted (.e.¬†predicted Volume) values), can attributed small number data points edges, ‚Äôm overly concerned.Similarly, suggestion snaking Q-Q plot (suggesting lack normality) mainly due inclusion one data point overall plot looks acceptable.highly influential pointsThe r commands :can use output get following equation:Volume = -57.99 + 0.34 \\(\\cdot\\) Height + 4.71 \\(\\cdot\\) GirthIf stick numbers (Girth = 20 Height = 67) get following equation:Volume = -57.99 + 0.34 \\(\\cdot\\) 67 + 4.71 \\(\\cdot\\) 20Volume = 58.91If look diagnostic plots model using following commands get following:model isn‚Äôt great.worrying lack linearity exhibited Residuals plot suggesting linear model isn‚Äôt appropriate.Assumptions Normality seem OKEquality variance harder interpret. Given lack linearity data isn‚Äôt really sensible interpret Location-Scale plot stands (since plot generated assuming ‚Äôve fitted straight line data), sake practising interpretation ‚Äôll go. definitely suggestions heterogeneity variance cluster points fitted values around 20 noticeably lower variance rest dataset.One point influential weren‚Äôt issues linearity model remove point repeat analysis. stands isn‚Äôt much point.r commands :can use output get following equation:Volume = `-87.12 + 1.54 \\(\\cdot\\) HeightIf stick numbers (Girth = 20 Height = 67) get following equation:Volume = -87.12 + 1.54 \\(\\cdot\\) 67Volume = 16.28If look diagnostic plots model using following commands get following:model also isn‚Äôt great.main issue clear heterogeneity variance. trees bigger volumes data much spread trees smaller volumes (can seen clearly Location-Scale plot).Apart , assumption Normality seems OKAnd aren‚Äôt hugely influential points modelThe r commands :can use output get following equation:Volume = -36.94 + 5.07 \\(\\cdot\\) GirthIf stick numbers (Girth = 20 Height = 67) get following equation:Volume = -36.94 + 5.07 \\(\\cdot\\) 20Volume = 64.37If look diagnostic plots model using following commands get following:diagnostic plots look rather similar ones generated additive model issue lack linearity, heterogeneity variance one data points influential.","code":"\n# define the model\nlm_tree_full <- lm(Volume ~ Height * Girth,\n                   data = trees)\n\n# view the model\nlm_tree_full## \n## Call:\n## lm(formula = Volume ~ Height * Girth, data = trees)\n## \n## Coefficients:\n##  (Intercept)        Height         Girth  Height:Girth  \n##      69.3963       -1.2971       -5.8558        0.1347\nlm_tree_full %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\n# define the model\nlm_tree_add <- lm(Volume ~ Height + Girth,\n                  data = trees)\n\n# view the model\nlm_tree_add## \n## Call:\n## lm(formula = Volume ~ Height + Girth, data = trees)\n## \n## Coefficients:\n## (Intercept)       Height        Girth  \n##    -57.9877       0.3393       4.7082\nlm_tree_add %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\n# define the model\nlm_height <- lm(Volume ~ Height,\n              data = trees)\n\n# view the model\nlm_height## \n## Call:\n## lm(formula = Volume ~ Height, data = trees)\n## \n## Coefficients:\n## (Intercept)       Height  \n##     -87.124        1.543\nlm_height %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\n# define the model\nlm_girth <- lm(Volume ~ Girth,\n               data = trees)\n\n# view the model\nlm_girth## \n## Call:\n## lm(formula = Volume ~ Girth, data = trees)\n## \n## Coefficients:\n## (Intercept)        Girth  \n##     -36.943        5.066\nlm_girth %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)"},{"path":"linear-models.html","id":"full-model","chapter":"24 Linear models","heading":"24.8.1 Full model","text":"r commands :can use output get following equation:Volume = 69.40 + -1.30 \\(\\cdot\\) Height + -5.86 \\(\\cdot\\) Girth + 0.13 \\(\\cdot\\) Height \\(\\cdot\\) GirthIf stick numbers (Girth = 20 Height = 67) get following equation:Volume = 69.40 + -1.30 \\(\\cdot\\) 67 + -5.86 \\(\\cdot\\) 20 + 0.13 \\(\\cdot\\) 67 \\(\\cdot\\) 20Volume = 45.81Here note interaction term just requires us multiple three numbers together (haven‚Äôt looked continuous predictors examples exercise included check see whole process making sense).look diagnostic plots model using following commands get:assumptions OK.suggestion heterogeneity variance (variance lower small large fitted (.e.¬†predicted Volume) values), can attributed small number data points edges, ‚Äôm overly concerned.Similarly, suggestion snaking Q-Q plot (suggesting lack normality) mainly due inclusion one data point overall plot looks acceptable.highly influential points","code":"\n# define the model\nlm_tree_full <- lm(Volume ~ Height * Girth,\n                   data = trees)\n\n# view the model\nlm_tree_full## \n## Call:\n## lm(formula = Volume ~ Height * Girth, data = trees)\n## \n## Coefficients:\n##  (Intercept)        Height         Girth  Height:Girth  \n##      69.3963       -1.2971       -5.8558        0.1347\nlm_tree_full %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)"},{"path":"linear-models.html","id":"additive-model-1","chapter":"24 Linear models","heading":"24.8.2 Additive model","text":"r commands :can use output get following equation:Volume = -57.99 + 0.34 \\(\\cdot\\) Height + 4.71 \\(\\cdot\\) GirthIf stick numbers (Girth = 20 Height = 67) get following equation:Volume = -57.99 + 0.34 \\(\\cdot\\) 67 + 4.71 \\(\\cdot\\) 20Volume = 58.91If look diagnostic plots model using following commands get following:model isn‚Äôt great.worrying lack linearity exhibited Residuals plot suggesting linear model isn‚Äôt appropriate.Assumptions Normality seem OKEquality variance harder interpret. Given lack linearity data isn‚Äôt really sensible interpret Location-Scale plot stands (since plot generated assuming ‚Äôve fitted straight line data), sake practising interpretation ‚Äôll go. definitely suggestions heterogeneity variance cluster points fitted values around 20 noticeably lower variance rest dataset.One point influential weren‚Äôt issues linearity model remove point repeat analysis. stands isn‚Äôt much point.","code":"\n# define the model\nlm_tree_add <- lm(Volume ~ Height + Girth,\n                  data = trees)\n\n# view the model\nlm_tree_add## \n## Call:\n## lm(formula = Volume ~ Height + Girth, data = trees)\n## \n## Coefficients:\n## (Intercept)       Height        Girth  \n##    -57.9877       0.3393       4.7082\nlm_tree_add %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)"},{"path":"linear-models.html","id":"height-only-model","chapter":"24 Linear models","heading":"24.8.3 Height-only model","text":"r commands :can use output get following equation:Volume = `-87.12 + 1.54 \\(\\cdot\\) HeightIf stick numbers (Girth = 20 Height = 67) get following equation:Volume = -87.12 + 1.54 \\(\\cdot\\) 67Volume = 16.28If look diagnostic plots model using following commands get following:model also isn‚Äôt great.main issue clear heterogeneity variance. trees bigger volumes data much spread trees smaller volumes (can seen clearly Location-Scale plot).Apart , assumption Normality seems OKAnd aren‚Äôt hugely influential points model","code":"\n# define the model\nlm_height <- lm(Volume ~ Height,\n              data = trees)\n\n# view the model\nlm_height## \n## Call:\n## lm(formula = Volume ~ Height, data = trees)\n## \n## Coefficients:\n## (Intercept)       Height  \n##     -87.124        1.543\nlm_height %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)"},{"path":"linear-models.html","id":"girth-only-model","chapter":"24 Linear models","heading":"24.8.4 Girth-only model","text":"r commands :can use output get following equation:Volume = -36.94 + 5.07 \\(\\cdot\\) GirthIf stick numbers (Girth = 20 Height = 67) get following equation:Volume = -36.94 + 5.07 \\(\\cdot\\) 20Volume = 64.37If look diagnostic plots model using following commands get following:diagnostic plots look rather similar ones generated additive model issue lack linearity, heterogeneity variance one data points influential.","code":"\n# define the model\nlm_girth <- lm(Volume ~ Girth,\n               data = trees)\n\n# view the model\nlm_girth## \n## Call:\n## lm(formula = Volume ~ Girth, data = trees)\n## \n## Coefficients:\n## (Intercept)        Girth  \n##     -36.943        5.066\nlm_girth %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)"},{"path":"linear-models.html","id":"key-points-8","chapter":"24 Linear models","heading":"24.9 Key points","text":"can define linear model lm(), adding extra variablesUsing coefficients model can construct linear model equationThe underlying assumptions linear model three predictor variables two-way ANOVA","code":""},{},{"path":"model-comparisons.html","id":"model-comparisons","chapter":"25 Model comparisons","heading":"25 Model comparisons","text":"","code":""},{"path":"model-comparisons.html","id":"objectives-12","chapter":"25 Model comparisons","heading":"25.1 Objectives","text":"QuestionsHow compare linear models?decide one ‚Äúbest‚Äù model?ObjectivesBe able compare models using Akaike Information Criterion (AIC)Use AIC context Backwards Stepwise Elimination R","code":""},{"path":"model-comparisons.html","id":"purpose-and-aim-9","chapter":"25 Model comparisons","heading":"25.2 Purpose and aim","text":"previous example used single dataset fitted five linear models depending predictor variables used. Whilst fun (seriously, else right now?) seems ‚Äúbetter way.‚Äù Well, thankfully ! fact several methods can used compare different models order help identify ‚Äúbest‚Äù model. specifically, can determine full model (uses available predictor variables interactions) necessary appropriately describe dependent variable, whether can throw away terms (e.g.¬†interaction term) don‚Äôt offer useful predictive power.use Akaike Information Criterion order compare different models.","code":""},{"path":"model-comparisons.html","id":"section-commands-13","chapter":"25 Model comparisons","heading":"25.3 Section commands","text":"New commands section:","code":""},{"path":"model-comparisons.html","id":"data-and-hypotheses-12","chapter":"25 Model comparisons","heading":"25.4 Data and hypotheses","text":"section uses data/tidy/CS5-Ladybird.csv data set. data set comprises 20 observations three variables (one dependent two predictor). records clutch size (eggs) species ladybird alongside two potential predictor variables; mass female (weight), colour male (male) categorical variable.","code":""},{"path":"model-comparisons.html","id":"backwards-stepwise-elimination","chapter":"25 Model comparisons","heading":"25.5 Backwards Stepwise Elimination","text":"First, load data store object called ladybird. visualise data.","code":"\nladybird <- read_csv(\"data/tidy/CS5-Ladybird.csv\")\n# visualise the data\nladybird %>% \n  ggplot(aes(x = weight, y = eggs,\n             colour = male)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Dark2\")"},{"path":"model-comparisons.html","id":"comparing-models-with-aic-step-1","chapter":"25 Model comparisons","heading":"25.5.1 Comparing models with AIC (step 1)","text":"First, construct full linear model:Now construct reduced model (.e.¬†next simplest model) doesn‚Äôt interactions:compare two models simply use command extractAIC() model.line first number tells many parameters model second number tells AIC score model. can see full model 4 parameters (intercept, coefficient continuous variable weight, coefficient categorical variable male coefficient interaction term weight:male) AIC score 41.3 (1dp). reduced model lower AIC score 40.4 (1dp) 3 parameters (since ‚Äôve dropped interaction term). different ways interpreting AIC scores widely used interpretation says :difference two AIC scores greater 2 model smallest AIC score supported model higher AIC scoreif difference two models‚Äô AIC scores less 2 models equally well supportedThis choice language (supported vs significant) deliberate areas statistics AIC scores used differently way going use (ask want bit philosophical ramble ). However, situation use AIC scores decide whether reduced model least good full model. since difference AIC scores less 2, can say dropping interaction term left us model simpler (fewer terms) least good (AIC score) full model. reduced model eggs ~ weight + male designated current working minimal model.","code":"\n# define the full model\nlm_full <- lm(eggs ~ weight * male,\n              data = ladybird)\n\n# view the model summary\nsummary(lm_full)\n# define the model\nlm_red <- lm(eggs ~ weight + male,\n             data = ladybird)\n\n# view the model summary\nsummary(lm_red)\nextractAIC(lm_full)## [1]  4.00000 41.28452\nextractAIC(lm_red)## [1]  3.00000 40.43819"},{"path":"model-comparisons.html","id":"comparing-models-with-aic-step-2","chapter":"25 Model comparisons","heading":"25.5.2 Comparing models with AIC (step 2)","text":"Next, see remaining terms can dropped. look models dropped male weight (.e.¬†eggs ~ weight eggs ~ male) compare AIC values AIC current minimal model (eggs ~ weight + male). AIC values least one new reduced models lower (least 2 greater) AIC current minimal model, can drop relevant term get new minimal model. find situation can drop one term drop term gives us model lowest AIC.Drop variable weight examine AIC:Drop variable male examine AIC:Considering outputs together comparing AIC current minimal model (40.4) can see dropping male decreased AIC 38.8, whereas dropping weight actually increased AIC 60.0 thus worsened model quality.Hence can drop male new minimal model eggs ~ weight.","code":"\n# define the model\nlm_male <- lm(eggs ~ male,\n              data = ladybird)\n\n# extract the AIC\nextractAIC(lm_male)## [1]  2.00000 59.95172\n# define the model\nlm_weight <- lm(eggs ~ weight,\n                data = ladybird)\n\n# extract the AIC\nextractAIC(lm_weight)## [1]  2.00000 38.76847"},{"path":"model-comparisons.html","id":"comparing-models-with-aic-step-3","chapter":"25 Model comparisons","heading":"25.5.3 Comparing models with AIC (step 3)","text":"final comparison drop variable weight compare simple model null model (eggs ~ 1), assumes brood size constant across parameters.Drop variable weight see effect:AIC null model quite bit larger current minimal model eggs ~ weight conclude weight important. minimal model eggs ~ weight., summary, conclude :Female size useful predictor clutch size, male type important.stage can analyse minimal linear (lm.weight) model using anova() function, consider diagnostic plots using plot(lm.weight) command.","code":"\n# define the model\nlm_null <- lm(eggs ~ 1,\n              data = ladybird)\n\n# extract the AIC\nextractAIC(lm_null)## [1]  1.00000 58.46029"},{"path":"model-comparisons.html","id":"notes-on-backwards-stepwise-elimination","chapter":"25 Model comparisons","heading":"25.6 Notes on Backwards Stepwise Elimination","text":"method finding minimal model starting full model removing variables called backward stepwise elimination. Although regularly practised data analysis, increasing criticism approach, calls avoided entirely.made work procedure ? Given prevalence academic papers, useful aware procedures know issues . situations, using AIC model comparisons justified come across regularly. Additionally, may situations feel good reasons drop parameter model ‚Äì using technique can justify doesn‚Äôt affect model fit. Taken together, using backwards stepwise elimination model comparison still useful technique.Performing backwards stepwise elimination manually can quite tedious. Thankfully R acknowledges single inbuilt function called step() can perform necessary steps using AIC.perform full backwards stepwise elimination process find minimal model . output familiar ask demonstrator questions.Yes, told earlier, ‚Äôs fun ? (also useful understand steps behind technique suppose‚Ä¶)","code":"## Start:  AIC=41.28\n## eggs ~ weight * male\n## \n##               Df Sum of Sq    RSS    AIC\n## - weight:male  1    6.2724 111.90 40.438\n## <none>                     105.63 41.285\n## \n## Step:  AIC=40.44\n## eggs ~ weight + male\n## \n##          Df Sum of Sq    RSS    AIC\n## - male    1     1.863 113.77 38.768\n## <none>                111.90 40.438\n## - weight  1   216.196 328.10 59.952\n## \n## Step:  AIC=38.77\n## eggs ~ weight\n## \n##          Df Sum of Sq    RSS    AIC\n## <none>                113.77 38.768\n## - weight  1    222.78 336.55 58.460## \n## Call:\n## lm(formula = eggs ~ weight, data = ladybird)\n## \n## Coefficients:\n## (Intercept)       weight  \n##       4.320        1.873"},{"path":"model-comparisons.html","id":"exercise-bse","chapter":"25 Model comparisons","heading":"25.7 Exercise: BSE","text":"Exercise 25.1  BSE trees airpollUse internal dataset trees airpoll dataset earlier.Perform backwards stepwise elimination datasets discover minimal model using AIC.NB: interaction term significant main factor part interaction term dropped model.‚Äôre feeling attempt backwards stepwise elimination process internal CO2 dataset. data frame 1 dependent variable (uptake) 4 predictor variables (Plant, Type, Treatment, conc). Unfortunately, dataset contain enough data construct full linear model using 4 predictor variables (interactions), ignore Plant variable take uptake ~ Type + Treatment + conc + Type:Treatment + Type:conc +  Treatment:conc + Type:Treatment:conc full model.relatively straightforward using step() function.need first construct full linear model simply pass linear model object step function R rest.construct full linear model Height, Girth interaction run step() function:BSE approach gets far first step (trying drop interaction term). see immediately dropping interaction term makes model worse process stops. next line (underneath Call:) see best model still full model get see coefficients term.construct full linear model treatment_plant, daily_temp interaction run step function:, BSE approach gets far first step (trying drop interaction term). see immediately dropping interaction term makes model worse process stops. next line (underneath Call:) see best model still full model get see coefficients term.time manage three steps. first successful manage drop three-way interaction Type:Treatment:conc. next step end dropping Treatment:conc interaction. final step realise can‚Äôt drop terms ‚Äôre done. minimal model 5 terms coefficients model given bottom output.","code":"\n# define the full model\nlm_trees <- lm(Volume ~ Girth * Height,\n               data = trees)\n\n# perform BSE\nstep(lm_trees)## Start:  AIC=65.49\n## Volume ~ Girth * Height\n## \n##                Df Sum of Sq    RSS    AIC\n## <none>                      198.08 65.495\n## - Girth:Height  1    223.84 421.92 86.936## \n## Call:\n## lm(formula = Volume ~ Girth * Height, data = trees)\n## \n## Coefficients:\n##  (Intercept)         Girth        Height  Girth:Height  \n##      69.3963       -5.8558       -1.2971        0.1347## Rows: 16 Columns: 4## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## chr (1): treatment_plant\n## dbl (3): id, daily_temp, hydrogen_sulfide## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n# define the full model\nlm_airpoll <- lm(hydrogen_sulfide ~ treatment_plant * daily_temp,\n                 data = airpoll)\n\n# perform BSE\nstep(lm_airpoll)## Start:  AIC=-19.04\n## hydrogen_sulfide ~ treatment_plant * daily_temp\n## \n##                              Df Sum of Sq    RSS     AIC\n## <none>                                    2.9520 -19.041\n## - treatment_plant:daily_temp  1     1.447 4.3991 -14.659## \n## Call:\n## lm(formula = hydrogen_sulfide ~ treatment_plant * daily_temp, \n##     data = airpoll)\n## \n## Coefficients:\n##                 (Intercept)             treatment_plantB  \n##                     6.20495                     -2.73075  \n##                  daily_temp  treatment_plantB:daily_temp  \n##                    -0.05448                      0.18141\n# define the model, ignore the Plant variable\nlm_co2 <- lm(uptake ~ Type + Treatment + conc\n             + Type:Treatment + Type:conc + Treatment:conc\n             + Type:Treatment:conc,\n             data = CO2)\nstep(lm_co2)## Start:  AIC=302.6\n## uptake ~ Type + Treatment + conc + Type:Treatment + Type:conc + \n##     Treatment:conc + Type:Treatment:conc\n## \n##                       Df Sum of Sq    RSS    AIC\n## - Type:Treatment:conc  1    55.535 2602.7 302.41\n## <none>                             2547.2 302.60\n## \n## Step:  AIC=302.41\n## uptake ~ Type + Treatment + conc + Type:Treatment + Type:conc + \n##     Treatment:conc\n## \n##                  Df Sum of Sq    RSS    AIC\n## - Treatment:conc  1    31.871 2634.6 301.44\n## <none>                        2602.7 302.41\n## - Type:conc       1   207.998 2810.7 306.87\n## - Type:Treatment  1   225.730 2828.5 307.40\n## \n## Step:  AIC=301.44\n## uptake ~ Type + Treatment + conc + Type:Treatment + Type:conc\n## \n##                  Df Sum of Sq    RSS    AIC\n## <none>                        2634.6 301.44\n## - Type:conc       1    208.00 2842.6 305.82\n## - Type:Treatment  1    225.73 2860.3 306.34## \n## Call:\n## lm(formula = uptake ~ Type + Treatment + conc + Type:Treatment + \n##     Type:conc, data = CO2)\n## \n## Coefficients:\n##                      (Intercept)                   TypeMississippi  \n##                         25.29351                          -4.72692  \n##                 Treatmentchilled                              conc  \n##                         -3.58095                           0.02308  \n## TypeMississippi:Treatmentchilled              TypeMississippi:conc  \n##                         -6.55714                          -0.01070"},{"path":"model-comparisons.html","id":"trees-dataset","chapter":"25 Model comparisons","heading":"25.7.1 trees dataset","text":"construct full linear model Height, Girth interaction run step() function:BSE approach gets far first step (trying drop interaction term). see immediately dropping interaction term makes model worse process stops. next line (underneath Call:) see best model still full model get see coefficients term.","code":"\n# define the full model\nlm_trees <- lm(Volume ~ Girth * Height,\n               data = trees)\n\n# perform BSE\nstep(lm_trees)## Start:  AIC=65.49\n## Volume ~ Girth * Height\n## \n##                Df Sum of Sq    RSS    AIC\n## <none>                      198.08 65.495\n## - Girth:Height  1    223.84 421.92 86.936## \n## Call:\n## lm(formula = Volume ~ Girth * Height, data = trees)\n## \n## Coefficients:\n##  (Intercept)         Girth        Height  Girth:Height  \n##      69.3963       -5.8558       -1.2971        0.1347"},{"path":"model-comparisons.html","id":"airpoll-dataset","chapter":"25 Model comparisons","heading":"25.7.2 airpoll dataset","text":"construct full linear model treatment_plant, daily_temp interaction run step function:, BSE approach gets far first step (trying drop interaction term). see immediately dropping interaction term makes model worse process stops. next line (underneath Call:) see best model still full model get see coefficients term.","code":"## Rows: 16 Columns: 4## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \",\"\n## chr (1): treatment_plant\n## dbl (3): id, daily_temp, hydrogen_sulfide## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n# define the full model\nlm_airpoll <- lm(hydrogen_sulfide ~ treatment_plant * daily_temp,\n                 data = airpoll)\n\n# perform BSE\nstep(lm_airpoll)## Start:  AIC=-19.04\n## hydrogen_sulfide ~ treatment_plant * daily_temp\n## \n##                              Df Sum of Sq    RSS     AIC\n## <none>                                    2.9520 -19.041\n## - treatment_plant:daily_temp  1     1.447 4.3991 -14.659## \n## Call:\n## lm(formula = hydrogen_sulfide ~ treatment_plant * daily_temp, \n##     data = airpoll)\n## \n## Coefficients:\n##                 (Intercept)             treatment_plantB  \n##                     6.20495                     -2.73075  \n##                  daily_temp  treatment_plantB:daily_temp  \n##                    -0.05448                      0.18141"},{"path":"model-comparisons.html","id":"co2-dataset","chapter":"25 Model comparisons","heading":"25.7.3 CO2 dataset","text":"time manage three steps. first successful manage drop three-way interaction Type:Treatment:conc. next step end dropping Treatment:conc interaction. final step realise can‚Äôt drop terms ‚Äôre done. minimal model 5 terms coefficients model given bottom output.","code":"\n# define the model, ignore the Plant variable\nlm_co2 <- lm(uptake ~ Type + Treatment + conc\n             + Type:Treatment + Type:conc + Treatment:conc\n             + Type:Treatment:conc,\n             data = CO2)\nstep(lm_co2)## Start:  AIC=302.6\n## uptake ~ Type + Treatment + conc + Type:Treatment + Type:conc + \n##     Treatment:conc + Type:Treatment:conc\n## \n##                       Df Sum of Sq    RSS    AIC\n## - Type:Treatment:conc  1    55.535 2602.7 302.41\n## <none>                             2547.2 302.60\n## \n## Step:  AIC=302.41\n## uptake ~ Type + Treatment + conc + Type:Treatment + Type:conc + \n##     Treatment:conc\n## \n##                  Df Sum of Sq    RSS    AIC\n## - Treatment:conc  1    31.871 2634.6 301.44\n## <none>                        2602.7 302.41\n## - Type:conc       1   207.998 2810.7 306.87\n## - Type:Treatment  1   225.730 2828.5 307.40\n## \n## Step:  AIC=301.44\n## uptake ~ Type + Treatment + conc + Type:Treatment + Type:conc\n## \n##                  Df Sum of Sq    RSS    AIC\n## <none>                        2634.6 301.44\n## - Type:conc       1    208.00 2842.6 305.82\n## - Type:Treatment  1    225.73 2860.3 306.34## \n## Call:\n## lm(formula = uptake ~ Type + Treatment + conc + Type:Treatment + \n##     Type:conc, data = CO2)\n## \n## Coefficients:\n##                      (Intercept)                   TypeMississippi  \n##                         25.29351                          -4.72692  \n##                 Treatmentchilled                              conc  \n##                         -3.58095                           0.02308  \n## TypeMississippi:Treatmentchilled              TypeMississippi:conc  \n##                         -6.55714                          -0.01070"},{"path":"model-comparisons.html","id":"key-points-9","chapter":"25 Model comparisons","heading":"25.8 Key points","text":"can use Backwards Stepwise Elimination (BSE) full model see certain terms add predictive power model notThe AIC allows us compare different models - difference AIC 2 two models, smallest AIC score supportedWe can use step() function let R perform automatic BSE","code":""},{},{"path":"cs6-intro.html","id":"cs6-intro","chapter":"26 Introduction","heading":"26 Introduction","text":"","code":""},{"path":"cs6-intro.html","id":"objectives-13","chapter":"26 Introduction","heading":"26.1 Objectives","text":"introduce R commands conducting power analyses practising systematic statistical analysisBy end practical participants able :apply priori power analysis techniques t-tests linear models order determine appropriate sample sizes given effect size, power significance levelssystematically analyse produce readable statistical reports previously unseen simple datasets containing single continuous, binary proportion response variable multiple categorical continuous predictor variables","code":""},{"path":"cs6-intro.html","id":"background-5","chapter":"26 Introduction","heading":"26.2 Background","text":"practical consists two separate sections:first looks power analysis t-tests linear models.second section consists single worked example plus series example datasets, can analysed written using R techniques explored previous practicals. can considered opportunity review techniques ‚Äôve looked course. Details R commands given instead expected use previous materials need look various commands. intentional aims reflect use course resources future!","code":""},{},{"path":"introduction-5.html","id":"introduction-5","chapter":"27 Introduction","heading":"27 Introduction","text":"","code":""},{"path":"introduction-5.html","id":"cs6-datasets","chapter":"27 Introduction","heading":"27.1 Datasets","text":"section uses various data sets. located data/raw/ folder working directory. Please see Datasets information.","code":""},{},{"path":"power-analysis.html","id":"power-analysis","chapter":"28 Power analysis","heading":"28 Power analysis","text":"","code":""},{"path":"power-analysis.html","id":"objectives-14","chapter":"28 Power analysis","heading":"28.1 Objectives","text":"QuestionsWhat power analysis?can use power analysis design better experiments?ObjectivesBe able perform power analysis RUnderstand importance effect sizeUse power, significance level effect size optimise experimental design","code":""},{"path":"power-analysis.html","id":"background-6","chapter":"28 Power analysis","heading":"28.2 Background","text":"hypothesis tests can wrong two ways:can appear found significant result really isn‚Äôt anything : false positive (Type error), orwe can fail spot significant result really something interesting going : false negative (Type II error).probability getting false positive analysis precisely significance level use analysis. , order reduce likelihood getting false positive simply reduce significance level test (0.05 0.01 say). Easy .Unfortunately, unintended consequences (doesn‚Äôt everything?). turns reducing significance level means increase chance getting false negatives. make sense; ‚Äôre increasing barrier entry terms acceptance ‚Äôll also accidentally miss good stuff.Power capacity test detect significant different results. affected three things:effect size: .e.¬†big difference want able detect, alternatively consider meaningful effect/difference ?sample sizethe significance levelIn ideal world want carrying highly powerful tests using low significance levels, reduce chance getting false positive maximise chances finding true effect.Power analysis allows us design experiments just . Given:desired power (0.8 80% considered pretty good)significance level (0.05 5% trusty yet arbitrary steed )effect size like detectWe can calculate amount data need collect experiments. (Woohoo! looks statistics actually give us answer last rather perpetual shades--grey ‚Äúmaybes‚Äù).reality easily usable power analysis functions operate assumption data collect meet assumptions chosen statistical test perfectly. , example, want design experiment investigating effectiveness single drug compared placebo (simple t-test) want know many patients group order test work, standard power analysis techniques still assume data end collecting meet assumptions t-test carry (sorry raised hopes ever slightly üòâ).","code":""},{"path":"power-analysis.html","id":"effect-size","chapter":"28 Power analysis","heading":"28.2.1 Effect size","text":"shall see commands carrying power analyses simple implement apart concept effect size. tricky issue people get grips two reasons:Effect size related biological significance rather statistical significanceThe way specify effect sizesWith respect first point common conversation goes bit like :: ‚Äú‚Äôve told carry power analysis, eh? Lucky . sort effect size looking ?‚Äù: ‚Äúidea ‚Äôre talking . want know drug better placebo. many patients need?‚Äù: ‚Äúdepends big difference think drug compared placebo.‚Äù: ‚Äúhaven‚Äôt carried experiment yet, absolutely idea big effect !‚Äù: (honest relatively well-informed conversation: much closer things actually go)key point effect sizes power analyses need specify effect size interested observing, one biologically relevant see. may well actually 0.1% difference effectiveness drug placebo designing experiment detect require markedly individuals experiment trying detect 50% difference effectiveness. reality three places can get sense effect sizes :pilot studyPrevious literature theoryJacob CohenJacob Cohen American statistician developed large set measures effect sizes (use today). came rough set numerical measures ‚Äúsmall,‚Äù ‚Äúmedium‚Äù ‚Äúlarge‚Äù effect sizes still use today. come caveats though; Jacob psychologist assessment large effect may somewhat different . form useful starting point however.lot different ways specifying effects sizes, can split three distinct families estimates:Correlation estimates: use R2 measure variance explained model (linear models, anova etc. large R2 value indicate lot variance explained model expect see lot difference groups, tight cluster points around line best fit. argument goes need fewer data points observe relationship confidence. Trying find relationship low R2 value trickier therefore require data points equivalent power.Difference means: look far apart means two groups , measured units standard deviations (t-tests). effect size 2 case interpreted two groups means two standard deviations away (quite big difference), whereas effect size 0.2 harder detect require data pick .Difference count data: freely admit idea intuitively explain (shock, horror). Mathematically based chi-squared statistic ‚Äôs good can tell ‚Äôm afraid. , however, pretty easy calculate.reference Cohen‚Äôs suggested values effect sizes different tests. ‚Äôll probably surprised small .look carry power analyses estimate effect sizes section.","code":""},{"path":"power-analysis.html","id":"packages","chapter":"28 Power analysis","heading":"28.3 Packages","text":"using pwr package section, contains functions allow us perform power calculations. Please install now.can running following code console:Next, load running:","code":"\ninstall.packages(\"pwr\")\nlibrary(pwr)"},{"path":"power-analysis.html","id":"section-commands-14","chapter":"28 Power analysis","heading":"28.4 Section commands","text":"New commands used section:","code":""},{"path":"power-analysis.html","id":"t-tests","chapter":"28 Power analysis","heading":"28.5 t-tests","text":"Let‚Äôs assume want design experiment determine whether difference mean price male female students pay cafe. many male female students need observe order detect ‚Äúmedium‚Äù effect size 80% power significance level 0.05?first need think test use analyse data. two groups continuous response. Clearly t-test.","code":""},{"path":"power-analysis.html","id":"get-the-effect-size","chapter":"28 Power analysis","heading":"28.5.1 Get the effect size","text":"Now need work ‚Äúmedium‚Äù effect size . absence information appeal Cohen‚Äôs conventional values:function just returns default conventional values effect sizes determined Jacob Cohen back day. just saves us scrolling back page look table provided. takes two arguments:test one \n‚Äút,‚Äù t-tests,\n‚Äúanova‚Äù anova,\n‚Äúf2‚Äù linear models\n‚Äúchisq‚Äù chi-squared test\n‚Äút,‚Äù t-tests,‚Äúanova‚Äù anova,‚Äúf2‚Äù linear models‚Äúchisq‚Äù chi-squared testsize, just one ‚Äúsmall,‚Äù ‚Äúmedium‚Äù ‚Äúlarge.‚Äùbit want bottom line; apparently want effect size 0.5.\nsort study effect size measured terms Cohen‚Äôs d statistic. simply measure different means two groups expressed terms number standard deviations apart . , case ‚Äôre looking detect two means 0.5 standard deviations away . minute ‚Äôll look means real data.","code":"\ncohen.ES(test = \"t\", size = \"medium\")## \n##      Conventional effect size from Cohen (1982) \n## \n##            test = t\n##            size = medium\n##     effect.size = 0.5"},{"path":"power-analysis.html","id":"carry-out-a-power-analysis","chapter":"28 Power analysis","heading":"28.5.2 Carry out a power analysis","text":"follows:first line ‚Äôre looking n = 63.76 tells need 64 (rounding ) students group (128 total) order carry study sufficient power. lines self-explanatory (well stage; need tell function just returning values ‚Äôve just typed bigger problems worry ).pwr.t.test() function six arguments. Two specify sort t-test ‚Äôll carrying \n* type; describes type t-test eventually carrying (one two.sample, one.sample paired), \n* alternative; describes type alternative hypothesis want test (one two.sided, less greater)four arguments used power analysis:d; effect size, single number calculated using Cohen‚Äôs d statistic.sig.level; significance levelpower; powern; number observations per sample.function works allowing specify three four arguments function works fourth. example used test standard fashion specifying power, significance desired effect size getting function tell us necessary sample size.can use function answer different question:know advance can observe 30 students per group, effect size able observe 80% power 5% significance level?Let‚Äôs see :time want see effect size look second line can see experiment many people expected detect difference means d = 0.74 standard deviations. good bad? Well, depends natural variation data; data really noisy large variation large standard deviation mean 0.74 standard deviations might actually quite big difference groups. hand data doesn‚Äôt vary much, 0.74 standard deviations might actually really small number test pick even quite small differences mean.previous two examples little bit context-free terms effect size. Let‚Äôs look can use pilot study real data calculate effect sizes perform power analysis inform future study.Let‚Äôs look fishlength data saw first practical relating lengths fish two separate rivers. saved data/tidy/CS6-fishlength.csv.summary statistics can see 39 observations Aripo river 29 observations Guanapo river. box plot see groups appear different means t-test analysis can see difference significant.Can use information design efficient experiment? One confident powerful enough pick difference means big observed study fewer observations?Let‚Äôs first work exactly effect size previous study really estimating Cohen‚Äôs d using data.cohens_d() function calculates effect size using formula test. effsize column contains information want, case 0.94 .can know actually answer question see many fish really need catch future:can see future experiments really need use 19 fish group (18.77 first line rounded , fish harmed experiment‚Ä¶) wanted confident detecting difference observed previous study.approach can also used pilot study showed smaller effect size wasn‚Äôt observed significant (indeed arguably, pilot study shouldn‚Äôt really concern significance really used way assessing potential effect sizes can used follow-study).","code":"\npwr.t.test(d = 0.5, sig.level = 0.05, power = 0.8,\n           type = \"two.sample\", alternative = \"two.sided\")## \n##      Two-sample t test power calculation \n## \n##               n = 63.76561\n##               d = 0.5\n##       sig.level = 0.05\n##           power = 0.8\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group\npwr.t.test(n = 30, sig.level = 0.05, power = 0.8,\n           type = \"two.sample\", alternative = \"two.sided\")## \n##      Two-sample t test power calculation \n## \n##               n = 30\n##               d = 0.7356292\n##       sig.level = 0.05\n##           power = 0.8\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group\n# read in the data\nfishlength <- read_csv(\"data/tidy/CS6-fishlength.csv\")\n\n# look at the data\nfishlength## # A tibble: 68 √ó 3\n##       id river   length\n##    <dbl> <chr>    <dbl>\n##  1     1 Guanapo   19.1\n##  2     2 Guanapo   23.3\n##  3     3 Guanapo   18.2\n##  4     4 Guanapo   16.4\n##  5     5 Guanapo   19.7\n##  6     6 Guanapo   16.6\n##  7     7 Guanapo   17.5\n##  8     8 Guanapo   19.9\n##  9     9 Guanapo   19.1\n## 10    10 Guanapo   18.8\n## # ‚Ä¶ with 58 more rows\n# summarise the data\nfishlength %>% \n  select(-id) %>% \n  group_by(river) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 2 √ó 11\n##   river   variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 Aripo   length      39  17.5  26.4   20.1   2.2  20.3  1.78 0.285 0.577\n## 2 Guanapo length      29  11.2  23.3   18.8   2.2  18.3  2.58 0.48  0.983\n# visualise the data\nfishlength %>% \n  ggplot(aes(x = river, y = length)) +\n  geom_boxplot()\n# perform t-test\nfishlength %>% \n  t_test(length ~ river)## # A tibble: 1 √ó 8\n##   .y.    group1 group2     n1    n2 statistic    df       p\n## * <chr>  <chr>  <chr>   <int> <int>     <dbl> <dbl>   <dbl>\n## 1 length Aripo  Guanapo    39    29      3.64  46.9 0.00067\nfishlength %>%\n  cohens_d(length ~ river, var.equal = TRUE)## # A tibble: 1 √ó 7\n##   .y.    group1 group2  effsize    n1    n2 magnitude\n## * <chr>  <chr>  <chr>     <dbl> <int> <int> <ord>    \n## 1 length Aripo  Guanapo   0.942    39    29 large\npwr.t.test(d = 0.94, power = 0.8, sig.level = 0.05,\n           type = \"two.sample\", alternative = \"two.sided\")## \n##      Two-sample t test power calculation \n## \n##               n = 18.77618\n##               d = 0.94\n##       sig.level = 0.05\n##           power = 0.8\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group"},{"path":"power-analysis.html","id":"exercise-one-sample","chapter":"28 Power analysis","heading":"28.6 Exercise: one-sample","text":"Exercise 28.1  Performing power analysis one-sample data setLoad data/tidy/CS6-onesample.csv (data looked earlier practical containing information fish lengths Guanapo river).Assume pilot study analyse data using one-sample t-test see evidence mean length fish differs 19 cm.Use results analysis estimate effect size.Work big sample size required detect effect big power 0.8 significance 0.05.sample size change wanted 0.9 power significance 0.01?First, read data:Let‚Äôs run one-sample t-test :appear statistically significant result ; mean length fish appears different 20 cm.Let‚Äôs calculate effect size using data. gives us following output effect size terms Cohen‚Äôs d metric.effect size -0.66 moderate effect size. pretty good means might able detect effect fewer samples.Although effect size negative, matter terms power calculations whether ‚Äôs negative positive., let‚Äôs power analysis actually calculate minimum sample size required:need 21 (round n value) observations experimental protocol order able detect effect size big (small?) 5% significance level 80% power. Let‚Äôs see happen wanted even stringent:‚Äôd need 38 observations! need bit work wanted work level significance power. small differences fish length biologically meaningful?","code":"\nfish_data <- read_csv(\"data/tidy/CS6-onesample.csv\")\nfish_data %>% \n  t_test(length ~ 1,\n         mu = 20,\n         alternative = \"two.sided\")## # A tibble: 1 √ó 7\n##   .y.    group1 group2         n statistic    df       p\n## * <chr>  <chr>  <chr>      <int>     <dbl> <dbl>   <dbl>\n## 1 length 1      null model    29     -3.55    28 0.00139\nfish_data %>% \n  cohens_d(length ~ 1, mu = 20)## # A tibble: 1 √ó 6\n##   .y.    group1 group2     effsize     n magnitude\n## * <chr>  <chr>  <chr>        <dbl> <int> <ord>    \n## 1 length 1      null model  -0.659    29 moderate\npwr.t.test(d = -0.6590669, sig.level = 0.05, power = 0.8,\n           type = \"one.sample\")## \n##      One-sample t test power calculation \n## \n##               n = 20.07483\n##               d = 0.6590669\n##       sig.level = 0.05\n##           power = 0.8\n##     alternative = two.sided\npwr.t.test(d = -0.6590669, sig.level = 0.01, power = 0.9,\n           type = \"one.sample\")## \n##      One-sample t test power calculation \n## \n##               n = 37.62974\n##               d = 0.6590669\n##       sig.level = 0.01\n##           power = 0.9\n##     alternative = two.sided"},{"path":"power-analysis.html","id":"exercise-two-sample-paired","chapter":"28 Power analysis","heading":"28.7 Exercise: two-sample paired","text":"Exercise 28.2  Power analysis paired two-sample data setLoad data/tidy/CS6-twopaired.csv (data used earlier practical relates cortisol levels measured 20 participants morning evening).first carry power analysis work big effect size experiment able detect power 0.8 significance level 0.05. Don‚Äôt look data just yet!Now calculate actual observed effect size study.repeat study future, many observations necessary detect observed effect 80% power significance level 0.01?First, read data:paired dataset 20 pairs observations, sort effect size detect significance level 0.05 power 0.8?Remember get effect size measured Cohen‚Äôs d metric. experimental design able detect d value 0.66 (2dp) medium large effect size.Now let‚Äôs look actual data work effect size actually :value, -1.16, massive effect size. ‚Äôs quite likely actually participants study actually need given large effect. Let calculate many individuals actually need:needed 13 pairs participants study given size effect trying detect.","code":"\ncortisol <- read_csv(\"data/tidy/CS6-twopaired.csv\")\npwr.t.test(n = 20, sig.level = 0.05, power = 0.8,\n           type = \"paired\")## \n##      Paired t test power calculation \n## \n##               n = 20\n##               d = 0.6604413\n##       sig.level = 0.05\n##           power = 0.8\n##     alternative = two.sided\n## \n## NOTE: n is number of *pairs*\ncortisol %>% \n  cohens_d(cortisol ~ time, paired = TRUE)## # A tibble: 1 √ó 7\n##   .y.      group1  group2  effsize    n1    n2 magnitude\n## * <chr>    <chr>   <chr>     <dbl> <int> <int> <ord>    \n## 1 cortisol evening morning   -1.16    20    20 large\npwr.t.test(d = -1.159019, sig.level = 0.01, power = 0.8,\n           type = \"paired\")## \n##      Paired t test power calculation \n## \n##               n = 12.10628\n##               d = 1.159019\n##       sig.level = 0.01\n##           power = 0.8\n##     alternative = two.sided\n## \n## NOTE: n is number of *pairs*"},{"path":"power-analysis.html","id":"linear-model-power-calculations","chapter":"28 Power analysis","heading":"28.8 Linear model power calculations","text":"Thankfully ideas ‚Äôre already covered t-test section see us good stead going forward can stop writing everything excruciating detail (things know).linear models ‚Äôll just use pwr.f2.test() power calculation won‚Äôt need function effect sizes (‚Äôs just based R2 just able read screen).Read data/tidy/CS6-lobsters.csv. dataset used earlier practical describes effect lobster weight three different food sources.Type :box plot shows us might well differences groupsthe ANOVA analysis though shows isn‚Äôt sufficient evidence support claim given insignificant p-value observe.question can ask :really difference different food sources big appears , big sample need order able detect statistically?First let‚Äôs calculate observed effect size study. linear models effect size called Cohen‚Äôs f2. can calculate easily using R2 value model fit shoving following formula:\\[\\begin{equation}\nf^2 = \\frac{R^2}{1-R^2}\n\\end{equation}\\]find R2 anova lm_lobster can calculate Cohen‚Äôs f^2:now ‚Äôve got Cohen‚Äôs f2. ‚Äôs one thing need power calculation linear model (gets bit arbitrary); degrees freedom. can either read bottom line anova(lm_lobster) output can extract follows:two different degrees freedom: numerator degrees freedom (DFn) denominator degrees freedom (DFd).numerator degrees freedom 2. number want. simply number parameters model minus 1. model three parameters three groups, 3 - 1 = 2 (see maths isn‚Äôt bad). number called denominator degrees freedom, case 15. actually number want power analysis calculate ‚Äôs proxy number observations used model, ‚Äôll see minute., now want run power analysis linear model, using following information:power = 0.8significance = 0.05effect size = 0.219numerator DF = 2We can feed pwr.f2.test() function, useu represent numerator DF valuef2 represent Cohen‚Äôs f2 effect size valueAs numbers just ‚Äôve put function . new number v. denominator degrees freedom required analysis sufficient power. Thankfully number related number observations use straightforward manner:\\(number\\:\\:observations = u + v + 1\\)case ideally 48 observations (45 + 2 + 1, remembering round ) experiment.\ntwo questions might now ask (‚Äôre still following ‚Äì ‚Äôre quite possibly definitely need coffee now):many observations go group?\nideally equally distributed (case 16 per group).\nideally equally distributed (case 16 per group).complicated, isn‚Äôt just single function just , just tells many observations need?\ngood question ‚Äì answer sorry ‚Äì sometimes life just hard.\ngood question ‚Äì answer sorry ‚Äì sometimes life just hard.challenging part using power analyses linear models working numerator degrees freedom . easiest way thinking say ‚Äôs number parameters model, excluding intercept. look back wrote linear model equations, able see many non-zero parameters expected. simple cases table help , complex linear models need write linear model equation count parameters (sorry!).","code":"\n# read in the data\nlobsters <- read_csv(\"data/tidy/CS6-lobsters.csv\")\n\n# visualise the data\nlobsters %>% \n  ggplot(aes(x = diet, y = weight)) +\n  geom_boxplot()\n# define the linear model\nlm_lobster <- lm(weight ~ diet,\n                 data = lobsters)\n\n# perform ANOVA on model\nanova(lm_lobster)## Analysis of Variance Table\n## \n## Response: weight\n##           Df Sum Sq Mean Sq F value Pr(>F)\n## diet       2 1567.2  783.61  1.6432 0.2263\n## Residuals 15 7153.1  476.87\n# get the effect size for ANOVA\nR2 <- eta_squared(anova(lm_lobster))\n\nR2##      diet \n## 0.1797219\n# calculate Cohen's f2\nR2 / (1 - R2)##      diet \n## 0.2190987\n# get degrees of freedom\nlm_lobster %>% \n  anova() %>% \n  anova_summary()##   Effect DFn DFd     F     p p<.05  ges\n## 1   diet   2  15 1.643 0.226       0.18\npwr.f2.test(u = 2, f2 = 0.219,\n            sig.level = 0.05, power = 0.8)## \n##      Multiple regression power calculation \n## \n##               u = 2\n##               v = 44.12292\n##              f2 = 0.219\n##       sig.level = 0.05\n##           power = 0.8"},{"path":"power-analysis.html","id":"exercise-mussel-muscles","chapter":"28 Power analysis","heading":"28.9 Exercise: Mussel muscles","text":"Exercise 28.3  Calculating effect sizeThe file data/raw/CS6-shelllength.csv contains information pilot study looking whether standardised length anterior adductor muscle scar mussel Mytilus trossulus differs across five locations around world (well might interest someone‚Ä¶).Find effect size study perform power calculation (0.8 0.05 significance level) determine many mussel muscles need recorded order confident effect really exists.Let‚Äôs load data:Let‚Äôs just quick look data see ‚Äôre dealing :effectively looking one-way ANOVA five groups. useful know later.Now fit linear model:get \\(R^2\\) value 0.4559 can use calculate Cohen‚Äôs \\(f^2\\) value using formula notes:Now, model 5 parameters (5 groups) numerator degrees freedom 4 \\((5 - 1 = 4)\\). means can now carry power analysis:tells us denominator degrees freedom 15 (14.62 rounded ), means need 20 observations total across five groups detect effect size (Remember: number observations = numerator d.f. + denominator d.f. + 1)","code":"\nmusseldat <- read.csv(\"data/raw/CS6-shelllength.csv\")\nboxplot(length ~ location,\n        data = musseldat)\n# define the model\nlm.mussel <- lm(length ~ location,\n                data = musseldat)\n\n# summarise the model\nsummary(lm.mussel)## \n## Call:\n## lm(formula = length ~ location, data = musseldat)\n## \n## Residuals:\n##       Min        1Q    Median        3Q       Max \n## -0.025400 -0.007956  0.000100  0.007000  0.031757 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)         0.078012   0.004454  17.517  < 2e-16 ***\n## locationNewport    -0.003213   0.006298  -0.510  0.61331    \n## locationPetersburg  0.025430   0.006519   3.901  0.00043 ***\n## locationTillamook   0.002187   0.005975   0.366  0.71656    \n## locationTvarminne   0.017687   0.006803   2.600  0.01370 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.0126 on 34 degrees of freedom\n## Multiple R-squared:  0.4559, Adjusted R-squared:  0.3918 \n## F-statistic: 7.121 on 4 and 34 DF,  p-value: 0.0002812\nf2 <- 0.4559 / (1 - 0.4559)\nf2## [1] 0.8378974\npwr.f2.test(u = 4, f2 = 0.8378974,\n            sig.level = 0.05 , power = 0.8)## \n##      Multiple regression power calculation \n## \n##               u = 4\n##               v = 14.62182\n##              f2 = 0.8378974\n##       sig.level = 0.05\n##           power = 0.8"},{"path":"power-analysis.html","id":"exercise-epilepsy","chapter":"28 Power analysis","heading":"28.10 Exercise: epilepsy","text":"Exercise 28.4  Power effectThe file /data/raw/CS6-epilepsy1.csv contains information ages rates seizures 236 patients undertaking clinical trial.Analyse data using linear model calculate effect size.relationship large age seizure rate big study needed observe effect 90% power.Let‚Äôs load data:Let‚Äôs just quick look data see ‚Äôre dealing :effectively looking simple linear regression .Now fit linear model:get \\(R^2\\) value 0.0009134 (tiny!) can use calculate Cohen‚Äôs \\(f^2\\) value using formula notes:effect size absolutely tiny. really wanted design experiment pick effect size small expect ‚Äôll need 1000s participants.Now, model 2 parameters (intercept slope) numerator degrees freedom (u) 1 (2 - 1 = 1!). means can now carry power analysis:tells us denominator degrees freedom (v) 11494 (11493.05 rounded ), means need 11496 participants detect effect size (Remember: number observations = numerator d.f. (u) + denominator d.f. (v) + 1).","code":"\nepilepsydat <- read.csv(\"data/raw/CS6-epilepsy1.csv\")\nplot(seizure ~ age,\n     data = epilepsydat)\n# define the model\nlm.epilepsy <- lm(seizure ~ age,\n                  data = epilepsydat)\n\n# summarise the model\nsummary(lm.epilepsy)## \n## Call:\n## lm(formula = seizure ~ age, data = epilepsydat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.77513 -0.19585 -0.04333  0.22288  1.24168 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.814935   0.124857   6.527 4.12e-10 ***\n## age         -0.001990   0.004303  -0.463    0.644    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.413 on 234 degrees of freedom\n## Multiple R-squared:  0.0009134,  Adjusted R-squared:  -0.003356 \n## F-statistic: 0.2139 on 1 and 234 DF,  p-value: 0.6441\nf2 <- 0.0009134 / (1 - 0.0009134)\nf2## [1] 0.0009142351\npwr.f2.test(u = 1, f2 = 0.0009142351,\n            sig.level = 0.05 , power = 0.9)## \n##      Multiple regression power calculation \n## \n##               u = 1\n##               v = 11493.05\n##              f2 = 0.0009142351\n##       sig.level = 0.05\n##           power = 0.9"},{"path":"power-analysis.html","id":"exercise-drug-versus-placebo","chapter":"28 Power analysis","heading":"28.11 Exercise: drug versus placebo","text":"Exercise 28.5  Study sizeWe wish test effectiveness new drug placebo. thought gender age patients may effect response.Write linear model equation might describe relationship variables including possible two-way interactions.big study need detect medium effect size (according Cohen) power 90%, significance level 0.05?system single response variable, three categorical predictor variables. One gender, two possible levels (M, F). One treatment, two possible levels (Drug, placebo) one continuous (age). linear model possible two-way interactions look something like :response ~ treatment + gender + age + treatment:gender + treatment:age + age:genderIn order power calculation set , ‚Äôll need four things:effect size. ‚Äôre told ‚Äôs medium effect size according Cohen can use default values.desired power. ‚Äôre told ‚Äôs 90%significance level work . ‚Äôre told going 0.05.numerator degrees freedom. tricky bit. can adding degrees freedom term separately.effect size easy get:Alternatively, looked online (may give us different values, values relevant specific discipline).numerator degrees freedom best calculated working degrees freedom six terms separately adding .three simple ideas need:degrees freedom categorical variable just number groups - 1The degrees freedom continuous variable always 1the degrees freedom interaction simple product degrees main effects involved interaction.means:df gender 1 (2 groups - 1)df treatment 1 (2 groups -1)df age 1 (continuous predictor)df gender:treatment 1 (1 x 1)df gender:age 1 (1 x 1)df age:treatment 1 (1 x 1)Rather boring 1 honest. Anyway, given denominator degrees freedom just sum , can see \\(u = 6\\).now information carry power analysis:get denominator df 116, means need least 123 participants study (Remember: number observations = numerator d.f. (u) + denominator d.f. (v) + 1). Given four unique combinations gender treatment, practically sensible round 124 participants equal number (31) combination gender treatment. also sensible aim similar distribution age ranges group well.","code":"\ncohen.ES(test = \"f2\", size = \"medium\")## \n##      Conventional effect size from Cohen (1982) \n## \n##            test = f2\n##            size = medium\n##     effect.size = 0.15\npwr.f2.test(u = 6, f2 = 0.15,\n            sig.level = 0.05, power = 0.9)## \n##      Multiple regression power calculation \n## \n##               u = 6\n##               v = 115.5826\n##              f2 = 0.15\n##       sig.level = 0.05\n##           power = 0.9"},{"path":"power-analysis.html","id":"key-points-10","chapter":"28 Power analysis","heading":"28.12 Key points","text":"Power capacity test detect significant results affected \neffect size\nsample size\nsignificance level\neffect sizesample sizethe significance levelPower analysis optimises trade-power, significance level desired effect size like detectPoint 3","code":""},{},{"path":"statistical-reporting.html","id":"statistical-reporting","chapter":"29 Statistical reporting","heading":"29 Statistical reporting","text":"","code":""},{"path":"statistical-reporting.html","id":"objectives-15","chapter":"29 Statistical reporting","heading":"29.1 Objectives","text":"QuestionsHow consistently report statistical results?ObjectivesBe able follow consistent analysis methodology RReport statistical results clear structured way","code":""},{"path":"statistical-reporting.html","id":"purpose-and-aim-10","chapter":"29 Statistical reporting","heading":"29.2 Purpose and aim","text":"practise statistical methods covered previous sessions produce consistent statistical reports.","code":""},{"path":"statistical-reporting.html","id":"data-and-hypotheses-13","chapter":"29 Statistical reporting","heading":"29.3 Data and hypotheses","text":"first section uses data file data/raw/CS6-NPYield.csv. dataset comprising 24 observations three variables (one dependent two predictor). records yield peas (pounds/plot) different plots. plot record whether nitrogen /phosphate fertiliser added.","code":""},{"path":"statistical-reporting.html","id":"analysis-methodology","chapter":"29 Statistical reporting","heading":"29.4 Analysis methodology","text":"Carry analysis suggested following steps confirm analysis matches text section.","code":""},{"path":"statistical-reporting.html","id":"step-1---identify-the-variables-and-the-question-to-be-explored","chapter":"29 Statistical reporting","heading":"29.4.1 Step 1 - Identify the variables and the question to be explored","text":"Load dataset look raw data\nConfirm NPYield dataset contains 24 observations 3 variables\nConfirm NPYield dataset contains 24 observations 3 variablesCheck names types variables\nyield continuous variable\nNit Pho categorical (factor) predictor variables\nyield continuous variableNit Pho categorical (factor) predictor variablesThe obvious question ask variables Nit Pho affect variable yield?","code":""},{"path":"statistical-reporting.html","id":"step-2-describe-the-data","chapter":"29 Statistical reporting","heading":"29.4.2 Step 2 ‚Äì Describe the data","text":"Plot data\nPlot effects individual predictor variables first. box plots yield Nit yield Pho appropriate.\nPlot effects interactions pairs predictor variables. interaction plot appropriate since two predictor variables categorical\nPlot effects individual predictor variables first. box plots yield Nit yield Pho appropriate.Plot effects interactions pairs predictor variables. interaction plot appropriate since two predictor variables categoricalCarry descriptive statistics may find useful\nCalculate means, variances, ranges etc. ‚Äôre going bother given simplicity dataset.\nCalculate means, variances, ranges etc. ‚Äôre going bother given simplicity dataset.Consider whether appear significant effects variables\nappears effect nitrogen\nappear effect phosphate\nmay interaction effect clear\nappears effect nitrogenThere appear effect phosphateThere may interaction effect clear","code":""},{"path":"statistical-reporting.html","id":"step-3---perform-tests-and-or-fit-models","chapter":"29 Statistical reporting","heading":"29.4.3 Step 3 - Perform tests and or fit models","text":"Select test appropriate data \n, single continuous response variable two categorical variables two options: two-way ANOVA test linear model (‚Äôd always go linear model framework). ‚Äôll first fit model interactions lm(yield ~ Nit * Pho) look model reduction see terms valid.\n, single continuous response variable two categorical variables two options: two-way ANOVA test linear model (‚Äôd always go linear model framework). ‚Äôll first fit model interactions lm(yield ~ Nit * Pho) look model reduction see terms valid.Assess results model fit\nIdentify significant effects. full model significant performing backwards stepwise elimination find minimal model yield ~ Nit.\nDetermine coefficients best fitting model. find formula given :\n\\[\\begin{equation}\n yield = 52.07 + \\binom{0}{5.62} \\binom{Nit:N}{Nit:Y}\n \\end{equation}\\]Identify significant effects. full model significant performing backwards stepwise elimination find minimal model yield ~ Nit.Determine coefficients best fitting model. find formula given :","code":""},{"path":"statistical-reporting.html","id":"step-4---check-assumptions-of-testsmodels","chapter":"29 Statistical reporting","heading":"29.4.4 Step 4 - Check assumptions of tests/models","text":"Plot Diagnostics plots /carry appropriate tests\njust plot diagnostic plots. look diagnostics full model minimal model.\njust plot diagnostic plots. look diagnostics full model minimal model.","code":""},{"path":"statistical-reporting.html","id":"writing-up-the-analysis","chapter":"29 Statistical reporting","heading":"29.5 Writing up the analysis","text":"four broad sections statistical report:IntroductionMethods & ResultsDiscussionAppendix","code":""},{"path":"statistical-reporting.html","id":"introduction-6","chapter":"29 Statistical reporting","heading":"29.5.1 Introduction","text":"aim :Describe data represent information collectedState question investigated.Always try keep language non-technicalFor example:IntroductionThe aim analysis investigate relationship yield peas addition fertilisers. yield peas pounds/plot recorded 24 plots used response variable. Two predictor variables thought potentially effect: addition fixed amount nitrogen based fertiliser addition fixed amount phosphate based fertiliser.","code":""},{"path":"statistical-reporting.html","id":"methods-and-results","chapter":"29 Statistical reporting","heading":"29.5.2 Methods and results","text":"section aim :Describe data using descriptive statistics plotsDescribe/state procedures undertaken\ne.g.¬†‚ÄúFit linear model interactions variables‚Äù\ne.g.¬†‚ÄúFit linear model interactions variables‚ÄùPresent figures state key results\ne.g.¬†Show lines best fit scatter graph state whether F-statistics significant alongside relevant p-values.\ne.g.¬†Show lines best fit scatter graph state whether F-statistics significant alongside relevant p-values.Produce diagnostic plots /results assumption testsThis section contain enough detail enable someone else replicate results. R output needs included .example using ggplot2 patchwork, makes lot easier produce nice graphs. possible similar things using base R syntax, ‚Äôll probably find composing plot panels best done external programme.example:Methods resultsThe response variable yield plotted two categorical variables Pho Nit independently, means different categorical combinations calculated plotted. shown Figure 1:Figure 1a appears suggest isn‚Äôt effect phosphate yield. Figure 1b indicates might effect nitrogen yield. Figure 1c suggests might interaction effect nitrogen phosphate yield, although might due presence outlier (Phosphate & Nitrogen) group.full linear model containing variables interaction fitted data (yield ~ Nit + Pho + Nit:Pho) model assumptions checked using full residual analysis (see Figure 2). assumptions equal variance normality appear met, suggesting linear model analysis may adequate data.ANOVA analysis full model compared null model results gives non-significant result (F3,20 = 2.21, p = 0.12) suggesting insufficient evidence yield affected variables.Backwards stepwise elimination used find minimal model. interaction Nitrogen Phosphate Phosphate terms yield found significant minimal model found :\\[\\begin{equation}\nyield ~ Nit\n\\end{equation}\\]variable Nit significant predictor yield (F1,22 = 6.06, p = 0.02).\\[\\begin{equation}\nyield = 52.07 + \\binom{0}{5.62} \\binom{Nit:N}{Nit:Y}\n\\end{equation}\\]box plot final model result () alongside diagnostic plots minimal model (b, c) shown Figure 3. assumptions equal variance normality still appear met, suggesting linear model analysis appropriate data.","code":""},{"path":"statistical-reporting.html","id":"discussion-2","chapter":"29 Statistical reporting","heading":"29.5.3 Discussion","text":"section aim :Summarise results context question, .e.¬†find?Discuss results model assumption tests. met? anything slightly dodgy?Discuss data limitations, e.g.¬†number data points, presence outliers etc.example:DiscussionThe analysis shows linear model may adequate data addition Nitrogen based fertiliser significant predictor pea yield, whereas Phosphate based fertiliser statistically significant effect. can seen addition Nitrogen appears increase yield.\ninteresting note whilst reduced model shown statistically significant, full model significant compared null model. number data points relatively small full interaction analysis (6 points per 2-way classification), repeating analysis larger number observations might beneficial.","code":""},{"path":"statistical-reporting.html","id":"appendix","chapter":"29 Statistical reporting","heading":"29.5.4 Appendix","text":"section always necessary / included option :Add R output , .e.¬†Full ANOVA tables printed outputCould include copy R script ‚Äôve done something clever terms data manipulation. can aid reproducibility results ‚Äôve obtained","code":""},{"path":"statistical-reporting.html","id":"exercise-cars-2-variables","chapter":"29 Statistical reporting","heading":"29.6 Exercise: cars 2 variables","text":"Exercise 29.1  Investigate relationship continuous response variable mpg categorical predictor variables cyl gear.data can found data/raw/CS6-cars2var.csv. dataset:mpg miles per galloncyl number cylinders (categorical variable)gear number gears (categorical variable)wt weight car (continuous variable)load datasets R, check variable interpreted correctly. Specifically check categorical variables loaded factors numbers. Specifically, may force variable interpreted factor using line code : cars3$gear <- factor(cars3$gear)","code":""},{"path":"statistical-reporting.html","id":"exercise-cars-3-variables","chapter":"29 Statistical reporting","heading":"29.7 Exercise: cars 3 variables","text":"Exercise 29.2  dataset now also includes continuous predictor variable wt. Investigate relationships four variables.data can found data/raw/CS6-cars3var.csv. dataset:mpg miles per galloncyl number cylinders (categorical variable)gear number gears (categorical variable)wt weight car (continuous variable)dataset three predictor variables (, B, C) one response variables (Y) six initial plots:3 individual plots: Y vs.¬†, Y vs.¬†B, Y vs.¬†C3 two-variable plots: Y vs.¬†&B, Y vs.¬†&C, Y vs.¬†B&CWe know look two types two-variable plots:categorical x categorical (interaction.plot)categorical x continuous (see linear model practical)won‚Äôt usually plot continuous x continuous two-variable plots (since require 3D plotting), ‚Äôre interested look plot3d() command rgl library.","code":""},{"path":"statistical-reporting.html","id":"key-points-11","chapter":"29 Statistical reporting","heading":"29.8 Key points","text":"useful analysis methodology :\nidentify variables question explored\ndescribe data (plots, summaries)\nperform tests fit models\ncheck assumptions tests/models\nidentify variables question exploreddescribe data (plots, summaries)perform tests fit modelscheck assumptions tests/modelsWriting statistical analysis often follows structure:\nIntroduction (description data non-technical way; collection methods; question answer)\nMethods & Results (description data; procedures; present figures key results; results assumptions checks)\nDiscussion (summarise results; discuss results assumptions; limitations data)\nAppendix (optional, can include R output)\nIntroduction (description data non-technical way; collection methods; question answer)Methods & Results (description data; procedures; present figures key results; results assumptions checks)Discussion (summarise results; discuss results assumptions; limitations data)Appendix (optional, can include R output)","code":""}]
